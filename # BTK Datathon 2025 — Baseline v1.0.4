{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112016,"databundleVersionId":13341508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================\n# BTK Datathon 2025 â€” Baseline v1.2 (micro)\n# Safe, incremental upgrades on your 1360 build\n# =============================\n\nimport os, sys, gc, math, json, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.isotonic import IsotonicRegression\n\nimport lightgbm as lgb\nwarnings.filterwarnings(\"ignore\")\n\n# -----------------------------\n# Config (keeps the working backbone)\n# -----------------------------\nCFG = {\n    \"seed\": 42,\n    \"seeds\": [42],                 # single seed (your best runs)\n    \"cv_type\": \"time\",             # DO NOT change (GroupKFold degraded LB earlier)\n    \"use_log_target\": False,       # DO NOT change (log target hurt LB)\n    \"add_user_history\": True,      # DO NOT turn off (turning off hurt LB)\n    \"add_sequence_extras\": True,\n\n    # Safe, proven LGBM core\n    \"lgb_params\": {\n        \"objective\": \"regression\",\n        \"metric\": \"mse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 63,\n        \"min_data_in_leaf\": 60,\n        \"feature_fraction\": 0.85,\n        \"bagging_fraction\": 0.85,\n        \"bagging_freq\": 1,\n        \"lambda_l2\": 2.0,\n        \"max_depth\": -1,\n        \"verbosity\": -1,\n        \"force_row_wise\": True,\n        \"extra_trees\": False,\n        \"seed\": 42, \"bagging_seed\": 42, \"feature_fraction_seed\": 42,\n        \"max_bin\": 255,\n    },\n\n    \"n_splits\": 3,\n    \"n_splits_group\": 5,\n    \"early_stopping_rounds\": 300,\n    \"num_boost_round\": 6000,\n\n    # Post-processing (remember: hard caps hurt; keep cap high)\n    \"clip\": {\"floor\": 0.0, \"cap\": 2000.0},\n\n    # --- New micro-upgrades (toggle easily) ---\n    \"add_micro_feats\": True,       # 4 small chronology ratios/timings\n    \"add_cyc_hour\": True,          # sin/cos hour\n    \"use_isotonic\": True,          # monotonic calibration on OOF (low risk)\n    \"submission_name\": \"submission_v1_2_micro.csv\",\n}\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\nset_seed(CFG[\"seed\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:45:25.682292Z","iopub.execute_input":"2025-08-30T05:45:25.683558Z","iopub.status.idle":"2025-08-30T05:45:34.027489Z","shell.execute_reply.started":"2025-08-30T05:45:25.683521Z","shell.execute_reply":"2025-08-30T05:45:34.026320Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -----------------------------\n# IO\n# -----------------------------\nCANDIDATE_DIRS = [Path(\"/kaggle/input/datathon-2025\")]\ndef find_csv(filename: str) -> Path:\n    for d in CANDIDATE_DIRS:\n        p = d / filename\n        if p.exists():\n            return p\n    raise FileNotFoundError(f\"Could not find {filename} in {CANDIDATE_DIRS}\")\n\ntrain_path = find_csv(\"train.csv\")\ntest_path  = find_csv(\"test.csv\")\nsub_path   = find_csv(\"sample_submission.csv\")\nprint(train_path, test_path, sub_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:45:47.716501Z","iopub.execute_input":"2025-08-30T05:45:47.717722Z","iopub.status.idle":"2025-08-30T05:45:47.728712Z","shell.execute_reply.started":"2025-08-30T05:45:47.717688Z","shell.execute_reply":"2025-08-30T05:45:47.727552Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datathon-2025/train.csv /kaggle/input/datathon-2025/test.csv /kaggle/input/datathon-2025/sample_submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -----------------------------\n# Read\n# -----------------------------\ndef detect_time_col(cols):\n    cand = [c for c in cols if c.lower() in (\"event_time\", \"event_timestamp\", \"timestamp\", \"time\", \"event_datetime\")]\n    if cand: return cand[0]\n    cand = [c for c in cols if \"time\" in c.lower() or \"date\" in c.lower()]\n    return cand[0] if cand else None\n\ndef read_df(path):\n    df = pd.read_csv(path)\n    tcol = detect_time_col(df.columns)\n    if tcol is None:\n        raise ValueError(\"Couldn't detect a time column. Please update detect_time_col().\")\n    df[tcol] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True)\n    if tcol != \"event_time\":\n        df = df.rename(columns={tcol: \"event_time\"})\n    return df\n\ntrain = read_df(train_path)\ntest  = read_df(test_path)\nsub   = pd.read_csv(sub_path)\n\nTARGET_COL = \"session_value\"\nassert TARGET_COL in train.columns and TARGET_COL not in test.columns\n\nprint(\"Shapes:\", train.shape, test.shape, sub.shape)\n\ndisplay(train.head(3))\ndisplay(test.head(3))\ndisplay(sub.head(3))\n\n# Dedup (unchanged)\nbefore = len(train)\ntrain = train.drop_duplicates().reset_index(drop=True)\nafter = len(train)\nprint(f\"Dropped {before - after} duplicate rows from train.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:45:50.250033Z","iopub.execute_input":"2025-08-30T05:45:50.250378Z","iopub.status.idle":"2025-08-30T05:45:51.489672Z","shell.execute_reply.started":"2025-08-30T05:45:50.250354Z","shell.execute_reply":"2025-08-30T05:45:51.488530Z"}},"outputs":[{"name":"stdout","text":"Shapes: (141219, 7) (62951, 6) (30789, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                 event_time event_type   product_id category_id      user_id  \\\n0 2025-06-19 10:23:07+00:00   ADD_CART  PROD_011223   CAT_00054  USER_097562   \n1 2025-06-07 21:34:45+00:00   ADD_CART  PROD_005519   CAT_00144  USER_006535   \n2 2025-06-21 21:29:09+00:00   ADD_CART  PROD_000577   CAT_00273  USER_047199   \n\n     user_session  session_value  \n0  SESSION_158779          90.29  \n1  SESSION_029987          16.39  \n2  SESSION_022134          64.27  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_time</th>\n      <th>event_type</th>\n      <th>product_id</th>\n      <th>category_id</th>\n      <th>user_id</th>\n      <th>user_session</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-06-19 10:23:07+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_011223</td>\n      <td>CAT_00054</td>\n      <td>USER_097562</td>\n      <td>SESSION_158779</td>\n      <td>90.29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-06-07 21:34:45+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_005519</td>\n      <td>CAT_00144</td>\n      <td>USER_006535</td>\n      <td>SESSION_029987</td>\n      <td>16.39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-06-21 21:29:09+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_000577</td>\n      <td>CAT_00273</td>\n      <td>USER_047199</td>\n      <td>SESSION_022134</td>\n      <td>64.27</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                 event_time event_type   product_id category_id      user_id  \\\n0 2025-06-28 10:09:58+00:00   ADD_CART  PROD_015000   CAT_00019  USER_109759   \n1 2025-06-25 11:57:50+00:00   ADD_CART  PROD_023887   CAT_00010  USER_010614   \n2 2025-06-30 14:34:20+00:00   ADD_CART  PROD_022673   CAT_00090  USER_041338   \n\n     user_session  \n0  SESSION_164059  \n1  SESSION_109583  \n2  SESSION_171382  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_time</th>\n      <th>event_type</th>\n      <th>product_id</th>\n      <th>category_id</th>\n      <th>user_id</th>\n      <th>user_session</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-06-28 10:09:58+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_015000</td>\n      <td>CAT_00019</td>\n      <td>USER_109759</td>\n      <td>SESSION_164059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-06-25 11:57:50+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_023887</td>\n      <td>CAT_00010</td>\n      <td>USER_010614</td>\n      <td>SESSION_109583</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-06-30 14:34:20+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_022673</td>\n      <td>CAT_00090</td>\n      <td>USER_041338</td>\n      <td>SESSION_171382</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     user_session  session_value\n0  SESSION_164059            0.0\n1  SESSION_109583            0.0\n2  SESSION_171382            0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_session</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SESSION_164059</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SESSION_109583</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SESSION_171382</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Dropped 670 duplicate rows from train.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -----------------------------\n# Typing / categories\n# -----------------------------\nID_USER = \"user_id\"\nID_SESSION = \"user_session\"\nPRODUCT_COL = \"product_id\"\nCATEGORY_COL = \"category_id\"\nEVENT_COL = \"event_type\"\n\nfor df in (train, test):\n    for c in [ID_USER, ID_SESSION]:\n        if c in df.columns:\n            df[c] = df[c].astype(str)\n    for c in [PRODUCT_COL, CATEGORY_COL, EVENT_COL]:\n        if c in df.columns:\n            df[c] = df[c].astype(\"category\")\n\nif EVENT_COL in train.columns:\n    all_types = sorted(list(set(train[EVENT_COL].dropna().unique()).union(set(test[EVENT_COL].dropna().unique()))))\n    train[EVENT_COL] = train[EVENT_COL].cat.set_categories(all_types)\n    test[EVENT_COL]  = test[EVENT_COL].cat.set_categories(all_types)\nprint(\"Event types:\", train[EVENT_COL].cat.categories.tolist() if EVENT_COL in train.columns else \"N/A\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:46:11.872974Z","iopub.execute_input":"2025-08-30T05:46:11.873300Z","iopub.status.idle":"2025-08-30T05:46:12.004894Z","shell.execute_reply.started":"2025-08-30T05:46:11.873278Z","shell.execute_reply":"2025-08-30T05:46:12.003892Z"}},"outputs":[{"name":"stdout","text":"Event types: ['ADD_CART', 'BUY', 'REMOVE_CART', 'VIEW']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# -----------------------------\n# Session table (adds first_buy_time & time_to_buy_sec)\n# -----------------------------\ndef build_session_table(events: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n    df = events.copy()\n    df = df.sort_values([ID_SESSION, \"event_time\"]).reset_index(drop=True)\n\n    # per-event flags\n    df[\"is_buy\"]  = (df[EVENT_COL] == \"BUY\").astype(int)\n    df[\"is_add\"]  = (df[EVENT_COL] == \"ADD_CART\").astype(int)\n    df[\"is_rem\"]  = (df[EVENT_COL] == \"REMOVE_CART\").astype(int)\n    df[\"is_view\"] = (df[EVENT_COL] == \"VIEW\").astype(int)\n\n    df[\"ev_idx\"] = df.groupby(ID_SESSION).cumcount()\n\n    first_event = df.groupby(ID_SESSION)[EVENT_COL].first().rename(\"first_event_type\")\n    last_event  = df.groupby(ID_SESSION)[EVENT_COL].last().rename(\"last_event_type\")\n\n    t_start = df.groupby(ID_SESSION)[\"event_time\"].min().rename(\"session_start\")\n    t_end   = df.groupby(ID_SESSION)[\"event_time\"].max().rename(\"session_end\")\n    duration = (t_end - t_start).dt.total_seconds().rename(\"duration_sec\")\n\n    agg_counts = df.groupby(ID_SESSION).agg(\n        n_events     = (EVENT_COL, \"size\"),\n        n_products   = (PRODUCT_COL, pd.Series.nunique),\n        n_categories = (CATEGORY_COL, pd.Series.nunique),\n        n_event_types= (EVENT_COL, pd.Series.nunique),\n        cnt_buy      = (\"is_buy\", \"sum\"),\n        cnt_add      = (\"is_add\", \"sum\"),\n        cnt_rem      = (\"is_rem\", \"sum\"),\n        cnt_view     = (\"is_view\", \"sum\"),\n    )\n\n    has_buy = (agg_counts[\"cnt_buy\"] > 0).astype(int).rename(\"has_buy\")\n    first_buy_idx = (\n        df[df[\"is_buy\"] == 1]\n        .groupby(ID_SESSION)[\"ev_idx\"].min()\n        .reindex(agg_counts.index).fillna(-1).astype(int).rename(\"idx_first_buy\")\n    )\n\n    # NEW: first BUY time + time_to_buy_sec (fallback to duration if no BUY)\n    first_buy_time = (\n        df[df[\"is_buy\"] == 1]\n        .groupby(ID_SESSION)[\"event_time\"].min()\n        .reindex(agg_counts.index)\n        .rename(\"first_buy_time\")\n    )\n    time_to_buy_sec = (first_buy_time - t_start).dt.total_seconds().rename(\"time_to_buy_sec\")\n    time_to_buy_sec = time_to_buy_sec.fillna(duration)\n\n    events_after_buy = (agg_counts[\"n_events\"] - (first_buy_idx + 1)).clip(lower=0).rename(\"events_after_first_buy\")\n\n    tmp = df.merge(first_buy_idx.rename(\"fb\"), left_on=ID_SESSION, right_index=True, how=\"left\")\n    before_fb = tmp[\"ev_idx\"] <= tmp[\"fb\"]\n    cnt_add_before_buy = tmp.loc[before_fb, \"is_add\"].groupby(tmp[ID_SESSION]).sum().reindex(agg_counts.index).fillna(0).astype(int).rename(\"cnt_add_before_buy\")\n    cnt_rem_before_buy = tmp.loc[before_fb, \"is_rem\"].groupby(tmp[ID_SESSION]).sum().reindex(agg_counts.index).fillna(0).astype(int).rename(\"cnt_rem_before_buy\")\n\n    def count_transitions(g):\n        x = g[EVENT_COL].astype(str).values\n        if len(x) <= 1: return 0\n        return int((x[1:] != x[:-1]).sum())\n    n_transitions = df.groupby(ID_SESSION).apply(count_transitions).rename(\"n_transitions\")\n\n    start_hour = t_start.dt.hour.rename(\"start_hour\")\n    start_dow  = t_start.dt.dayofweek.rename(\"start_dow\")\n    start_day  = t_start.dt.day.rename(\"start_day\")\n\n    user_map = df.groupby(ID_SESSION)[ID_USER].first().rename(ID_USER)\n\n    sess = pd.concat(\n        [\n            t_start, t_end, duration, agg_counts,\n            has_buy, first_buy_idx, first_buy_time, time_to_buy_sec,\n            events_after_buy,\n            cnt_add_before_buy, cnt_rem_before_buy,\n            n_transitions, first_event, last_event,\n            start_hour, start_dow, start_day,\n            user_map,\n        ],\n        axis=1\n    ).reset_index()\n\n    if is_train:\n        t = df.groupby(ID_SESSION)[TARGET_COL].first().reset_index()\n        sess = sess.merge(t, on=ID_SESSION, how=\"left\")\n        chk = df.groupby(ID_SESSION)[TARGET_COL].nunique().max()\n        if chk != 1:\n            print(\"WARNING: session_value is not constant within sessions.\")\n\n    for c in [\"first_event_type\", \"last_event_type\"]:\n        if c in sess.columns:\n            sess[c] = sess[c].astype(\"category\")\n    sess[ID_USER] = sess[ID_USER].astype(str)\n    return sess\n\ntrain_sess = build_session_table(train, is_train=True)\ntest_sess  = build_session_table(test,  is_train=False)\nprint(\"Session tables built.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:46:32.202505Z","iopub.execute_input":"2025-08-30T05:46:32.202876Z","iopub.status.idle":"2025-08-30T05:47:18.391596Z","shell.execute_reply.started":"2025-08-30T05:46:32.202851Z","shell.execute_reply":"2025-08-30T05:47:18.390673Z"}},"outputs":[{"name":"stdout","text":"Session tables built.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# -----------------------------\n# User history (keep ON)\n# -----------------------------\ndef add_user_history(train_sess: pd.DataFrame, test_sess: pd.DataFrame):\n    comb = pd.concat(\n        [train_sess.assign(_is_train=1), test_sess.assign(_is_train=0, **{TARGET_COL: np.nan})],\n        axis=0, ignore_index=True\n    ).sort_values([ID_USER, \"session_start\"]).reset_index(drop=True)\n\n    comb[\"user_prev_n_sessions\"] = comb.groupby(ID_USER).cumcount()\n\n    g = comb.groupby(ID_USER, sort=False)\n    pos = g.cumcount() + 1\n    prev_cnt = pos - 1\n    cum_sum_buy = g[\"has_buy\"].cumsum()\n    prev_sum_buy = cum_sum_buy - comb[\"has_buy\"]\n    comb[\"user_prev_buy_rate\"] = np.divide(\n        prev_sum_buy.astype(float), prev_cnt,\n        out=np.zeros_like(prev_sum_buy, dtype=float), where=prev_cnt > 0\n    )\n\n    comb[\"sv_notna\"]  = comb[TARGET_COL].notna().astype(int)\n    comb[\"sv_filled\"] = comb[TARGET_COL].fillna(0.0)\n    comb[\"cum_sum_sv\"] = g[\"sv_filled\"].cumsum()\n    comb[\"cum_cnt_sv\"] = g[\"sv_notna\"].cumsum()\n    prev_sum_sv = comb[\"cum_sum_sv\"] - comb[\"sv_filled\"]\n    prev_cnt_sv = comb[\"cum_cnt_sv\"] - comb[\"sv_notna\"]\n    prev_mean_sv = np.divide(\n        prev_sum_sv, prev_cnt_sv,\n        out=np.full(len(prev_sum_sv), np.nan, dtype=float), where=prev_cnt_sv > 0\n    )\n    global_mean_sv = float(train_sess[TARGET_COL].mean())\n    comb[\"user_prev_mean_sv\"] = np.where(np.isnan(prev_mean_sv), global_mean_sv, prev_mean_sv)\n\n    comb = comb.drop(columns=[\"sv_notna\",\"sv_filled\",\"cum_sum_sv\",\"cum_cnt_sv\"])\n\n    train_hist = comb[comb[\"_is_train\"] == 1].drop(columns=[\"_is_train\"])\n    test_hist  = comb[comb[\"_is_train\"] == 0].drop(columns=[\"_is_train\"])\n\n    for df in (train_hist, test_hist):\n        df[\"user_prev_n_sessions\"] = df[\"user_prev_n_sessions\"].astype(int)\n        df[\"user_prev_buy_rate\"]   = df[\"user_prev_buy_rate\"].astype(float)\n        df[\"user_prev_mean_sv\"]    = df[\"user_prev_mean_sv\"].astype(float)\n    return train_hist, test_hist\n\nif CFG[\"add_user_history\"]:\n    train_sess, test_sess = add_user_history(train_sess, test_sess)\n    print(\"Added user history features.\")\n    display(train_sess.filter(like=\"user_prev\").head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:47:44.147321Z","iopub.execute_input":"2025-08-30T05:47:44.147761Z","iopub.status.idle":"2025-08-30T05:47:44.665153Z","shell.execute_reply.started":"2025-08-30T05:47:44.147721Z","shell.execute_reply":"2025-08-30T05:47:44.664101Z"}},"outputs":[{"name":"stdout","text":"Added user history features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   user_prev_n_sessions  user_prev_buy_rate  user_prev_mean_sv\n0                     0                 0.0           42.19813\n1                     0                 0.0           42.19813\n2                     0                 0.0           42.19813","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_prev_n_sessions</th>\n      <th>user_prev_buy_rate</th>\n      <th>user_prev_mean_sv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>42.19813</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>42.19813</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>42.19813</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# -----------------------------\n# Micro features (small, targeted; safe)\n# -----------------------------\ndef add_micro_features(sess: pd.DataFrame, add_cyc_hour=True) -> pd.DataFrame:\n    s = sess.copy()\n    ne = s[\"n_events\"].clip(lower=1)\n    fb = s[\"idx_first_buy\"]\n\n    # If no BUY, set as \"end\" (1.0); if BUY exists, relative position in [0,1]\n    denom = np.maximum(ne - 1, 1)\n    s[\"buy_pos_ratio\"] = np.where(fb >= 0, fb / denom, 1.0)\n\n    s[\"prebuy_add_share\"] = np.where(s[\"cnt_add\"] > 0, s[\"cnt_add_before_buy\"] / s[\"cnt_add\"], 0.0)\n    s[\"prebuy_rem_share\"] = np.where(s[\"cnt_rem\"] > 0, s[\"cnt_rem_before_buy\"] / s[\"cnt_rem\"], 0.0)\n\n    if add_cyc_hour and \"start_hour\" in s:\n        s[\"hour_sin\"] = np.sin(2*np.pi*s[\"start_hour\"] / 24.0)\n        s[\"hour_cos\"] = np.cos(2*np.pi*s[\"start_hour\"] / 24.0)\n    return s\n\nif CFG[\"add_micro_feats\"]:\n    train_sess = add_micro_features(train_sess, add_cyc_hour=CFG[\"add_cyc_hour\"])\n    test_sess  = add_micro_features(test_sess,  add_cyc_hour=CFG[\"add_cyc_hour\"])\n    print(\"Added micro chronology features (and cyc hour if enabled).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:47:47.352912Z","iopub.execute_input":"2025-08-30T05:47:47.353671Z","iopub.status.idle":"2025-08-30T05:47:47.451757Z","shell.execute_reply.started":"2025-08-30T05:47:47.353641Z","shell.execute_reply":"2025-08-30T05:47:47.450738Z"}},"outputs":[{"name":"stdout","text":"Added micro chronology features (and cyc hour if enabled).\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# -----------------------------\n# (Optional) big enrichment set â€” KEEP OFF by default (hurt LB previously)\n# -----------------------------\ndef enrich_small_features(df: pd.DataFrame) -> pd.DataFrame:\n    eps = 1e-6\n    for c in [\"n_events\", \"n_products\", \"n_categories\", \"duration_sec\",\n              \"cnt_buy\", \"cnt_add\", \"cnt_rem\", \"cnt_view\"]:\n        if c in df: df[f\"log1p_{c}\"] = np.log1p(df[c].astype(float))\n    if \"n_events\" in df:\n        denom_ev = df[\"n_events\"].clip(lower=1).astype(float)\n        if \"cnt_add\" in df: df[\"rate_add_per_event\"] = df[\"cnt_add\"] / denom_ev\n        if \"cnt_rem\" in df: df[\"rate_rem_per_event\"] = df[\"cnt_rem\"] / denom_ev\n        if \"cnt_buy\" in df: df[\"rate_buy_per_event\"] = df[\"cnt_buy\"] / denom_ev\n    if \"duration_sec\" in df and \"n_events\" in df:\n        df[\"events_per_min\"] = df[\"n_events\"] / (df[\"duration_sec\"] / 60.0 + eps)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:47:49.822601Z","iopub.execute_input":"2025-08-30T05:47:49.822944Z","iopub.status.idle":"2025-08-30T05:47:49.831103Z","shell.execute_reply.started":"2025-08-30T05:47:49.822921Z","shell.execute_reply":"2025-08-30T05:47:49.829943Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Keep OFF (we learned these didnâ€™t transfer to LB)\nuse_enrichments = False\nif use_enrichments:\n    train_sess = enrich_small_features(train_sess)\n    test_sess  = enrich_small_features(test_sess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:47:57.063901Z","iopub.execute_input":"2025-08-30T05:47:57.064259Z","iopub.status.idle":"2025-08-30T05:47:57.069357Z","shell.execute_reply.started":"2025-08-30T05:47:57.064206Z","shell.execute_reply":"2025-08-30T05:47:57.068300Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# -----------------------------\n# Features\n# -----------------------------\ncategorical_cols = [\"first_event_type\", \"last_event_type\"]\nnumeric_cols = [\n    \"n_events\",\"n_products\",\"n_categories\",\"n_event_types\",\n    \"cnt_buy\",\"cnt_add\",\"cnt_rem\",\"cnt_view\",\n    \"duration_sec\",\"has_buy\",\"idx_first_buy\",\"events_after_first_buy\",\n    \"cnt_add_before_buy\",\"cnt_rem_before_buy\",\"n_transitions\",\n    \"start_hour\",\"start_dow\",\"start_day\",\n    \"time_to_buy_sec\",\n]\nif CFG[\"add_user_history\"]:\n    numeric_cols += [\"user_prev_n_sessions\",\"user_prev_buy_rate\",\"user_prev_mean_sv\"]\n\n# micro feats\nif CFG[\"add_micro_feats\"]:\n    micro_cols = [\"buy_pos_ratio\",\"prebuy_add_share\",\"prebuy_rem_share\"]\n    numeric_cols += [c for c in micro_cols if c in train_sess.columns]\nif CFG[\"add_cyc_hour\"]:\n    cyc_cols = [\"hour_sin\",\"hour_cos\"]\n    numeric_cols += [c for c in cyc_cols if c in train_sess.columns]\n\ncategorical_cols = [c for c in categorical_cols if c in train_sess.columns]\nnumeric_cols     = [c for c in numeric_cols if c in train_sess.columns]\nFEATS = categorical_cols + numeric_cols\n\nprint(\"Num features:\", len(FEATS))\nprint(\"Categorical:\", categorical_cols)\nprint(\"Numeric:\", [c for c in FEATS if c not in categorical_cols])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:48:10.108520Z","iopub.execute_input":"2025-08-30T05:48:10.109436Z","iopub.status.idle":"2025-08-30T05:48:10.118599Z","shell.execute_reply.started":"2025-08-30T05:48:10.109405Z","shell.execute_reply":"2025-08-30T05:48:10.117243Z"}},"outputs":[{"name":"stdout","text":"Num features: 29\nCategorical: ['first_event_type', 'last_event_type']\nNumeric: ['n_events', 'n_products', 'n_categories', 'n_event_types', 'cnt_buy', 'cnt_add', 'cnt_rem', 'cnt_view', 'duration_sec', 'has_buy', 'idx_first_buy', 'events_after_first_buy', 'cnt_add_before_buy', 'cnt_rem_before_buy', 'n_transitions', 'start_hour', 'start_dow', 'start_day', 'time_to_buy_sec', 'user_prev_n_sessions', 'user_prev_buy_rate', 'user_prev_mean_sv', 'buy_pos_ratio', 'prebuy_add_share', 'prebuy_rem_share', 'hour_sin', 'hour_cos']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# -----------------------------\n# Target\n# -----------------------------\nif CFG[\"use_log_target\"]:\n    train_sess[\"target\"] = np.log1p(train_sess[TARGET_COL].clip(lower=0))\nelse:\n    train_sess[\"target\"] = train_sess[TARGET_COL].astype(float)\n\nfor c in categorical_cols:\n    train_sess[c] = train_sess[c].astype(\"category\")\n    test_sess[c]  = test_sess[c].astype(\"category\")\n\nX = train_sess[FEATS].copy()\ny = train_sess[\"target\"].values\nX_test = test_sess[FEATS].copy()\ny_s = pd.Series(train_sess[\"target\"].values, index=X.index)\n\nprint(X.shape, X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:48:23.543781Z","iopub.execute_input":"2025-08-30T05:48:23.544081Z","iopub.status.idle":"2025-08-30T05:48:23.568545Z","shell.execute_reply.started":"2025-08-30T05:48:23.544059Z","shell.execute_reply":"2025-08-30T05:48:23.567282Z"}},"outputs":[{"name":"stdout","text":"(70736, 29) (30789, 29)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# -----------------------------\n# CV helpers (KEEP the original time windows â€” changing split hurt LB)\n# -----------------------------\ndef make_time_folds(df: pd.DataFrame, n_splits=3, date_col=\"session_start\"):\n    df_sorted = df.sort_values(date_col).reset_index()\n    n = len(df_sorted)\n    fold_sizes = [n // n_splits] * n_splits\n    for i in range(n % n_splits):\n        fold_sizes[i] += 1\n    idxs, start = [], 0\n    for fs in fold_sizes:\n        end = start + fs\n        idxs.append(df_sorted.loc[start:end-1, \"index\"].values)\n        start = end\n    folds = []\n    for i in range(n_splits):\n        val_idx = idxs[i]\n        tr_idx = np.concatenate([idxs[j] for j in range(n_splits) if j != i])\n        folds.append((tr_idx, val_idx))\n    return folds\n\ndef make_group_folds(df: pd.DataFrame, n_splits=5, group_col=ID_USER):\n    idx = np.arange(len(df))\n    gkf = GroupKFold(n_splits=n_splits)\n    return [(tr, va) for tr, va in gkf.split(idx, groups=df[group_col].values)]\n\nfolds_preview = make_time_folds(train_sess, n_splits=CFG[\"n_splits\"], date_col=\"session_start\")\nfor i, (_, va) in enumerate(folds_preview):\n    d1 = train_sess.loc[va, \"session_start\"].min()\n    d2 = train_sess.loc[va, \"session_start\"].max()\n    print(f\"Time Fold {i}: val window {d1} â†’ {d2}, size={len(va)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:48:41.107737Z","iopub.execute_input":"2025-08-30T05:48:41.108077Z","iopub.status.idle":"2025-08-30T05:48:41.175146Z","shell.execute_reply.started":"2025-08-30T05:48:41.108054Z","shell.execute_reply":"2025-08-30T05:48:41.173900Z"}},"outputs":[{"name":"stdout","text":"Time Fold 0: val window 2025-06-01 00:00:24+00:00 â†’ 2025-06-07 03:06:40+00:00, size=23579\nTime Fold 1: val window 2025-06-07 03:06:51+00:00 â†’ 2025-06-14 09:07:02+00:00, size=23579\nTime Fold 2: val window 2025-06-14 09:07:35+00:00 â†’ 2025-06-21 23:58:05+00:00, size=23578\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# -----------------------------\n# Train / OOF / Test\n# -----------------------------\noof_all = np.zeros(len(X), dtype=float)\ntest_preds_seeds = []\nfeat_imps = []\n\nglobal_mean_sv = float(train_sess[TARGET_COL].mean())\nX_index = pd.Index(X.index)\n\nfor seed in CFG[\"seeds\"]:\n    lgb_params = CFG[\"lgb_params\"].copy()\n    lgb_params.update({\"seed\": seed, \"bagging_seed\": seed, \"feature_fraction_seed\": seed})\n    set_seed(seed)\n\n    if CFG[\"cv_type\"] == \"time\":\n        folds = make_time_folds(train_sess, n_splits=CFG[\"n_splits\"], date_col=\"session_start\")\n    else:\n        folds = make_group_folds(train_sess, n_splits=CFG[\"n_splits_group\"], group_col=ID_USER)\n\n    oof_seed = np.zeros(len(X), dtype=float)\n    best_iters = []\n\n    for i, (tr_idx, va_idx) in enumerate(folds):\n        tr_pos = X_index.get_indexer(tr_idx)\n        va_pos = X_index.get_indexer(va_idx)\n        if (tr_pos < 0).any() or (va_pos < 0).any():\n            raise ValueError(f\"Fold {i}: indices not present in X.index.\")\n        if (va_pos >= len(X)).any() or (tr_pos >= len(X)).any():\n            raise ValueError(f\"Fold {i}: positional index out of bounds.\")\n\n        X_tr, y_tr = X.iloc[tr_pos], y_s.iloc[tr_pos].values\n        X_va, y_va = X.iloc[va_pos], y_s.iloc[va_pos].values\n\n        # (Remember: when we used group CV + history, mismatch hurt LB; leave as-is for time CV)\n        if CFG[\"cv_type\"] != \"time\":\n            X_va = X_va.copy()\n            if \"user_prev_mean_sv\" in X_va: X_va[\"user_prev_mean_sv\"] = global_mean_sv\n            if \"user_prev_buy_rate\" in X_va: X_va[\"user_prev_buy_rate\"] = 0.0\n            if \"user_prev_n_sessions\" in X_va: X_va[\"user_prev_n_sessions\"] = 0\n\n        lgb_train = lgb.Dataset(X_tr, label=y_tr, categorical_feature=categorical_cols, free_raw_data=False)\n        lgb_valid = lgb.Dataset(X_va, label=y_va, categorical_feature=categorical_cols, free_raw_data=False)\n\n        model = lgb.train(\n            lgb_params, lgb_train,\n            num_boost_round=CFG[\"num_boost_round\"],\n            valid_sets=[lgb_train, lgb_valid],\n            valid_names=[\"train\", \"valid\"],\n            callbacks=[lgb.early_stopping(CFG[\"early_stopping_rounds\"], verbose=False)]\n        )\n        best_iters.append(model.best_iteration)\n\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration)\n        if CFG[\"use_log_target\"]:\n            pred_va = np.expm1(pred_va).clip(min=0)\n        pred_va = np.clip(pred_va, CFG[\"clip\"][\"floor\"], CFG[\"clip\"][\"cap\"])\n\n        oof_seed[va_pos] = pred_va\n\n        fi = pd.DataFrame({\n            \"feature\": FEATS,\n            \"gain\": model.feature_importance(importance_type=\"gain\"),\n            \"split\": model.feature_importance(importance_type=\"split\"),\n            \"fold\": i,\n            \"seed\": seed,\n        })\n        feat_imps.append(fi)\n\n        print(f\"[seed {seed}] Fold {i}: best_iter={model.best_iteration}, val_size={len(va_pos)}\")\n\n    oof_all += oof_seed / len(CFG[\"seeds\"])\n\n    full_iters = int(np.mean(best_iters))\n    full_ds = lgb.Dataset(X, label=y_s.values, categorical_feature=categorical_cols, free_raw_data=False)\n    full_model = lgb.train(lgb_params, full_ds, num_boost_round=full_iters)\n\n    pred_test = full_model.predict(X_test)\n    if CFG[\"use_log_target\"]:\n        pred_test = np.expm1(pred_test).clip(min=0)\n    pred_test = np.clip(pred_test, CFG[\"clip\"][\"floor\"], CFG[\"clip\"][\"cap\"])\n    test_preds_seeds.append(pred_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:50:49.618119Z","iopub.execute_input":"2025-08-30T05:50:49.618470Z","iopub.status.idle":"2025-08-30T05:52:20.230892Z","shell.execute_reply.started":"2025-08-30T05:50:49.618446Z","shell.execute_reply":"2025-08-30T05:52:20.230139Z"}},"outputs":[{"name":"stdout","text":"[seed 42] Fold 0: best_iter=3039, val_size=23579\n[seed 42] Fold 1: best_iter=236, val_size=23579\n[seed 42] Fold 2: best_iter=2360, val_size=23578\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# -----------------------------\n# Metrics & FI\n# -----------------------------\nif CFG[\"use_log_target\"]:\n    y_raw = np.expm1(y_s.values)\n    oof_mse = mean_squared_error(y_raw, oof_all)\nelse:\n    y_raw = y_s.values\n    oof_mse = mean_squared_error(y_raw, oof_all)\nprint(f\"OOF MSE (pre-calibration, post-processed): {oof_mse:,.4f}\")\n\nfeat_importance = (\n    pd.concat(feat_imps, ignore_index=True)\n      .groupby(\"feature\")[[\"gain\",\"split\"]].mean()\n      .sort_values(\"gain\", ascending=False).reset_index()\n)\ndisplay(feat_importance.head(30))\n\ntest_pred = np.mean(test_preds_seeds, axis=0)\nprint(\"Test pred summary (pre-calibration):\")\ndisplay(pd.Series(test_pred).describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:52:35.658434Z","iopub.execute_input":"2025-08-30T05:52:35.658736Z","iopub.status.idle":"2025-08-30T05:52:35.704353Z","shell.execute_reply.started":"2025-08-30T05:52:35.658715Z","shell.execute_reply":"2025-08-30T05:52:35.703431Z"}},"outputs":[{"name":"stdout","text":"OOF MSE (pre-calibration, post-processed): 377.4050\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                   feature          gain         split\n0                  cnt_buy  4.622549e+08   3525.666667\n1                  has_buy  1.064414e+08     55.666667\n2   events_after_first_buy  8.508780e+07   3859.000000\n3               n_products  3.589600e+07   4431.333333\n4                  cnt_add  3.015860e+07   3162.000000\n5             duration_sec  2.393111e+07  13414.666667\n6                 n_events  2.352933e+07   4247.000000\n7             n_categories  1.883599e+07   4194.333333\n8            buy_pos_ratio  1.047668e+07   2168.333333\n9          time_to_buy_sec  7.735130e+06   9073.333333\n10                hour_cos  7.370438e+06   6306.333333\n11                 cnt_rem  7.162436e+06   2359.666667\n12           idx_first_buy  6.799450e+06   2056.000000\n13    user_prev_n_sessions  6.639252e+06   4666.000000\n14                hour_sin  6.422379e+06   5423.333333\n15         last_event_type  6.183504e+06   1394.333333\n16       user_prev_mean_sv  6.133867e+06  13075.666667\n17               start_dow  5.620843e+06   6477.000000\n18               start_day  5.527915e+06   8486.000000\n19           n_transitions  4.430776e+06   2028.666667\n20              start_hour  4.388915e+06   7381.666667\n21      cnt_add_before_buy  3.942398e+06    863.333333\n22                cnt_view  3.774147e+06   2596.666667\n23           n_event_types  3.348833e+06   1273.333333\n24        first_event_type  3.161811e+06   1322.666667\n25      cnt_rem_before_buy  2.311278e+06   1127.666667\n26        prebuy_add_share  1.246150e+06    261.000000\n27      user_prev_buy_rate  8.526274e+05   1101.666667\n28        prebuy_rem_share  1.746555e+05    124.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>gain</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cnt_buy</td>\n      <td>4.622549e+08</td>\n      <td>3525.666667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>has_buy</td>\n      <td>1.064414e+08</td>\n      <td>55.666667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>events_after_first_buy</td>\n      <td>8.508780e+07</td>\n      <td>3859.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n_products</td>\n      <td>3.589600e+07</td>\n      <td>4431.333333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cnt_add</td>\n      <td>3.015860e+07</td>\n      <td>3162.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>duration_sec</td>\n      <td>2.393111e+07</td>\n      <td>13414.666667</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>n_events</td>\n      <td>2.352933e+07</td>\n      <td>4247.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>n_categories</td>\n      <td>1.883599e+07</td>\n      <td>4194.333333</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>buy_pos_ratio</td>\n      <td>1.047668e+07</td>\n      <td>2168.333333</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>time_to_buy_sec</td>\n      <td>7.735130e+06</td>\n      <td>9073.333333</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>hour_cos</td>\n      <td>7.370438e+06</td>\n      <td>6306.333333</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>cnt_rem</td>\n      <td>7.162436e+06</td>\n      <td>2359.666667</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>idx_first_buy</td>\n      <td>6.799450e+06</td>\n      <td>2056.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>user_prev_n_sessions</td>\n      <td>6.639252e+06</td>\n      <td>4666.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hour_sin</td>\n      <td>6.422379e+06</td>\n      <td>5423.333333</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>last_event_type</td>\n      <td>6.183504e+06</td>\n      <td>1394.333333</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>user_prev_mean_sv</td>\n      <td>6.133867e+06</td>\n      <td>13075.666667</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>start_dow</td>\n      <td>5.620843e+06</td>\n      <td>6477.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>start_day</td>\n      <td>5.527915e+06</td>\n      <td>8486.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>n_transitions</td>\n      <td>4.430776e+06</td>\n      <td>2028.666667</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>start_hour</td>\n      <td>4.388915e+06</td>\n      <td>7381.666667</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>cnt_add_before_buy</td>\n      <td>3.942398e+06</td>\n      <td>863.333333</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>cnt_view</td>\n      <td>3.774147e+06</td>\n      <td>2596.666667</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>n_event_types</td>\n      <td>3.348833e+06</td>\n      <td>1273.333333</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>first_event_type</td>\n      <td>3.161811e+06</td>\n      <td>1322.666667</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>cnt_rem_before_buy</td>\n      <td>2.311278e+06</td>\n      <td>1127.666667</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>prebuy_add_share</td>\n      <td>1.246150e+06</td>\n      <td>261.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>user_prev_buy_rate</td>\n      <td>8.526274e+05</td>\n      <td>1101.666667</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>prebuy_rem_share</td>\n      <td>1.746555e+05</td>\n      <td>124.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Test pred summary (pre-calibration):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"count    30789.000000\nmean        43.639692\nstd         44.421166\nmin          0.000000\n25%         23.727635\n50%         29.528812\n75%         42.716727\nmax        917.451389\ndtype: float64"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# -----------------------------\n# Isotonic calibration (OPTIONAL but ON by default)\n# -----------------------------\nif CFG[\"use_isotonic\"]:\n    iso = IsotonicRegression(out_of_bounds=\"clip\")\n    iso.fit(oof_all, y_raw)  # OOF preds are out-of-fold; acceptable for level-2 fit\n\n    oof_cal = iso.predict(oof_all)\n    oof_cal = np.clip(oof_cal, CFG[\"clip\"][\"floor\"], CFG[\"clip\"][\"cap\"])\n    oof_mse_cal = mean_squared_error(y_raw, oof_cal)\n    print(f\"OOF MSE after isotonic calibration: {oof_mse_cal:,.4f}  (Î”={oof_mse - oof_mse_cal:+.4f})\")\n\n    test_pred = iso.predict(test_pred)\n    test_pred = np.clip(test_pred, CFG[\"clip\"][\"floor\"], CFG[\"clip\"][\"cap\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:52:38.958874Z","iopub.execute_input":"2025-08-30T05:52:38.959200Z","iopub.status.idle":"2025-08-30T05:52:39.007851Z","shell.execute_reply.started":"2025-08-30T05:52:38.959178Z","shell.execute_reply":"2025-08-30T05:52:39.006910Z"}},"outputs":[{"name":"stdout","text":"OOF MSE after isotonic calibration: 327.6013  (Î”=+49.8037)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Expect submission format: [\"user_session\", \"session_value\"]\nsub_out = sub.copy()\nkey = \"user_session\"\n\n# Map predictions by user_session (test_sess has one row per session)\npred_map = dict(zip(test_sess[key], test_pred))\nsub_out[TARGET_COL] = sub_out[key].map(pred_map).fillna(CFG[\"clip\"][\"floor\"])\n\nsave_name = \"submission_baseline_v1_0_4.csv\"\nsub_out.to_csv(save_name, index=False)\nprint(\"Saved:\", save_name)\ndisplay(sub_out.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:52:48.257113Z","iopub.execute_input":"2025-08-30T05:52:48.257452Z","iopub.status.idle":"2025-08-30T05:52:48.380043Z","shell.execute_reply.started":"2025-08-30T05:52:48.257431Z","shell.execute_reply":"2025-08-30T05:52:48.379084Z"}},"outputs":[{"name":"stdout","text":"Saved: submission_baseline_v1_0_4.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     user_session  session_value\n0  SESSION_164059     159.566786\n1  SESSION_109583      39.774610\n2  SESSION_171382      39.774610\n3  SESSION_137110      33.730594\n4  SESSION_146503     188.818724","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_session</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SESSION_164059</td>\n      <td>159.566786</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SESSION_109583</td>\n      <td>39.774610</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SESSION_171382</td>\n      <td>39.774610</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SESSION_137110</td>\n      <td>33.730594</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SESSION_146503</td>\n      <td>188.818724</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# -----------------------------\n# Diagnostics\n# -----------------------------\nprint(\"OOF MSE (final used for reporting):\",\n      mean_squared_error(y_raw, oof_cal if CFG[\"use_isotonic\"] else oof_all))\ntmp = train_sess.assign(y=y_raw, oof=(oof_cal if CFG[\"use_isotonic\"] else oof_all))\nday_mse = tmp.groupby(tmp[\"session_start\"].dt.date).apply(lambda d: mean_squared_error(d[\"y\"], d[\"oof\"]))\ndisplay(day_mse.to_frame(\"mse\").reset_index().rename(columns={\"session_start\":\"date\"}).head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:52:50.767385Z","iopub.execute_input":"2025-08-30T05:52:50.767705Z","iopub.status.idle":"2025-08-30T05:52:50.848994Z","shell.execute_reply.started":"2025-08-30T05:52:50.767683Z","shell.execute_reply":"2025-08-30T05:52:50.847966Z"}},"outputs":[{"name":"stdout","text":"OOF MSE (final used for reporting): 327.6012634004619\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          date         mse\n0   2025-06-01  622.407139\n1   2025-06-02  543.222743\n2   2025-06-03  307.575340\n3   2025-06-04  297.711478\n4   2025-06-05  464.454279\n5   2025-06-06  234.407730\n6   2025-06-07  416.594966\n7   2025-06-08  333.333938\n8   2025-06-09  216.850485\n9   2025-06-10  236.525549\n10  2025-06-11  342.273511\n11  2025-06-12  237.899418\n12  2025-06-13  242.352849\n13  2025-06-14  307.704389\n14  2025-06-15  296.031612\n15  2025-06-16  237.872353\n16  2025-06-17  331.747696\n17  2025-06-18  278.952415\n18  2025-06-19  210.011579\n19  2025-06-20  277.844853","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-06-01</td>\n      <td>622.407139</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-06-02</td>\n      <td>543.222743</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-06-03</td>\n      <td>307.575340</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-06-04</td>\n      <td>297.711478</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-06-05</td>\n      <td>464.454279</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2025-06-06</td>\n      <td>234.407730</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2025-06-07</td>\n      <td>416.594966</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2025-06-08</td>\n      <td>333.333938</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2025-06-09</td>\n      <td>216.850485</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2025-06-10</td>\n      <td>236.525549</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2025-06-11</td>\n      <td>342.273511</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2025-06-12</td>\n      <td>237.899418</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2025-06-13</td>\n      <td>242.352849</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2025-06-14</td>\n      <td>307.704389</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2025-06-15</td>\n      <td>296.031612</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2025-06-16</td>\n      <td>237.872353</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2025-06-17</td>\n      <td>331.747696</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2025-06-18</td>\n      <td>278.952415</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2025-06-19</td>\n      <td>210.011579</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2025-06-20</td>\n      <td>277.844853</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}