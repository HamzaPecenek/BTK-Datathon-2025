{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112016,"databundleVersionId":13341508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================\n# BTK Datathon 2025 â€” Baseline v1.1\n# =============================\n\nimport os, sys, gc, math, json, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, GroupKFold\n\nimport lightgbm as lgb\n\nwarnings.filterwarnings(\"ignore\")\n\nCFG = {\n    \"seed\": 42,\n    \"seeds\": [42],                 # single seed (closer to the 1350 run)\n    \"cv_type\": \"time\",             # <<< back to time-based CV\n    \"use_log_target\": False,       # <<< raw target like v1\n    \"add_user_history\": True,      # <<< ON like v1\n    \"add_sequence_extras\": True,\n\n    \"lgb_params\": {\n        \"objective\": \"regression\",\n        \"metric\": \"mse\",\n        \"learning_rate\": 0.05,     # your good setting\n        \"num_leaves\": 63,          # v1-size trees\n        \"min_data_in_leaf\": 60,    # small extra regularization over v1=50\n        \"feature_fraction\": 0.85,\n        \"bagging_fraction\": 0.85,\n        \"bagging_freq\": 1,\n        \"lambda_l2\": 2.0,\n        \"max_depth\": -1,\n        \"verbosity\": -1,\n        \"force_row_wise\": True,\n        \"extra_trees\": False,\n        \"seed\": 42, \"bagging_seed\": 42, \"feature_fraction_seed\": 42,\n        \"max_bin\": 255\n    },\n\n    \"n_splits\": 3,\n    \"n_splits_group\": 5,\n    \"early_stopping_rounds\": 300,\n    \"num_boost_round\": 6000,\n\n    # No hard cap; floor at 0 to match your best runs\n    \"clip\": {\"floor\": 0.0, \"cap\": 2000.0},\n}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T05:39:11.642493Z","iopub.execute_input":"2025-08-30T05:39:11.642798Z","iopub.status.idle":"2025-08-30T05:39:24.239183Z","shell.execute_reply.started":"2025-08-30T05:39:11.642765Z","shell.execute_reply":"2025-08-30T05:39:24.238076Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n\nset_seed(CFG[\"seed\"])\n\n# Kaggle input path(s)\nCANDIDATE_DIRS = [Path(\"/kaggle/input/datathon-2025\")]\n\ndef find_csv(filename: str) -> Path:\n    for d in CANDIDATE_DIRS:\n        p = d / filename\n        if p.exists():\n            return p\n    raise FileNotFoundError(f\"Could not find {filename} in {CANDIDATE_DIRS}\")\n\ntrain_path = find_csv(\"train.csv\")\ntest_path  = find_csv(\"test.csv\")\nsub_path   = find_csv(\"sample_submission.csv\")\n\nprint(train_path, test_path, sub_path)\n\n# Auto-detect time column\ndef detect_time_col(cols):\n    cand = [c for c in cols if c.lower() in (\"event_time\", \"event_timestamp\", \"timestamp\", \"time\", \"event_datetime\")]\n    if cand:\n        return cand[0]\n    cand = [c for c in cols if \"time\" in c.lower() or \"date\" in c.lower()]\n    return cand[0] if cand else None\n\ndef read_df(path):\n    df = pd.read_csv(path)\n    tcol = detect_time_col(df.columns)\n    if tcol is None:\n        raise ValueError(\"Couldn't detect a time column. Please update detect_time_col().\")\n    df[tcol] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True)\n    if tcol != \"event_time\":\n        df = df.rename(columns={tcol: \"event_time\"})\n    return df\n\ntrain = read_df(train_path)\ntest  = read_df(test_path)\nsub   = pd.read_csv(sub_path)\n\nprint(\"Shapes:\", train.shape, test.shape, sub.shape)\nprint(\"\\nTrain columns:\\n\", list(train.columns))\nprint(\"\\nTest columns:\\n\", list(test.columns))\nprint(\"\\nSubmission columns:\\n\", list(sub.columns))\n\nTARGET_COL = \"session_value\"\nassert TARGET_COL in train.columns and TARGET_COL not in test.columns, \"Target column check failed.\"\n\ndisplay(train.head(3))\ndisplay(test.head(3))\ndisplay(sub.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:51:55.064708Z","iopub.execute_input":"2025-08-29T06:51:55.065624Z","iopub.status.idle":"2025-08-29T06:51:55.826238Z","shell.execute_reply.started":"2025-08-29T06:51:55.065590Z","shell.execute_reply":"2025-08-29T06:51:55.824963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"before = len(train)\ntrain = train.drop_duplicates().reset_index(drop=True)\nafter = len(train)\nprint(f\"Dropped {before - after} duplicate rows from train.\")\n\n# Normalize frequent columns\nID_USER = \"user_id\"\nID_SESSION = \"user_session\"\nPRODUCT_COL = \"product_id\"\nCATEGORY_COL = \"category_id\"\nEVENT_COL = \"event_type\"\n\nexpected_cols = [ID_USER, ID_SESSION, PRODUCT_COL, CATEGORY_COL, EVENT_COL, \"event_time\"]\nfor c in expected_cols:\n    if c not in train.columns:\n        print(f\"Warning: expected column '{c}' not found in train.\")\n\n# Coerce types\nfor df in (train, test):\n    for c in [ID_USER, ID_SESSION]:\n        if c in df.columns:\n            df[c] = df[c].astype(str)\n    for c in [PRODUCT_COL, CATEGORY_COL, EVENT_COL]:\n        if c in df.columns:\n            df[c] = df[c].astype(\"category\")\n\n# Align event type categories\nif EVENT_COL in train.columns:\n    all_types = sorted(list(set(train[EVENT_COL].dropna().unique()).union(set(test[EVENT_COL].dropna().unique()))))\n    train[EVENT_COL] = train[EVENT_COL].cat.set_categories(all_types)\n    test[EVENT_COL]  = test[EVENT_COL].cat.set_categories(all_types)\nprint(\"Event types:\", train[EVENT_COL].cat.categories.tolist() if EVENT_COL in train.columns else \"N/A\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:51:59.810862Z","iopub.execute_input":"2025-08-29T06:51:59.811153Z","iopub.status.idle":"2025-08-29T06:52:00.078200Z","shell.execute_reply.started":"2025-08-29T06:51:59.811135Z","shell.execute_reply":"2025-08-29T06:52:00.077339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Session-level feature builder\ndef build_session_table(events: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n    df = events.copy()\n    df = df.sort_values([\"user_session\", \"event_time\"]).reset_index(drop=True)\n\n    # Per-event flags\n    df[\"is_buy\"]  = (df[\"event_type\"] == \"BUY\").astype(int)\n    df[\"is_add\"]  = (df[\"event_type\"] == \"ADD_CART\").astype(int)\n    df[\"is_rem\"]  = (df[\"event_type\"] == \"REMOVE_CART\").astype(int)\n    df[\"is_view\"] = (df[\"event_type\"] == \"VIEW\").astype(int)\n\n    # Rank within session\n    df[\"ev_idx\"] = df.groupby(\"user_session\").cumcount()\n\n    # First/last event type\n    first_event = df.groupby(\"user_session\")[\"event_type\"].first().rename(\"first_event_type\")\n    last_event  = df.groupby(\"user_session\")[\"event_type\"].last().rename(\"last_event_type\")\n\n    # Start/end and duration\n    t_start = df.groupby(\"user_session\")[\"event_time\"].min().rename(\"session_start\")\n    t_end   = df.groupby(\"user_session\")[\"event_time\"].max().rename(\"session_end\")\n    duration = (t_end - t_start).dt.total_seconds().rename(\"duration_sec\")\n\n    # Counts & uniques\n    agg_counts = df.groupby(\"user_session\").agg(\n        n_events     = (\"event_type\", \"size\"),\n        n_products   = (\"product_id\", pd.Series.nunique),\n        n_categories = (\"category_id\", pd.Series.nunique),\n        n_event_types= (\"event_type\", pd.Series.nunique),\n        cnt_buy      = (\"is_buy\", \"sum\"),\n        cnt_add      = (\"is_add\", \"sum\"),\n        cnt_rem      = (\"is_rem\", \"sum\"),\n        cnt_view     = (\"is_view\", \"sum\"),\n    )\n\n    # Has buy\n    has_buy = (agg_counts[\"cnt_buy\"] > 0).astype(int).rename(\"has_buy\")\n\n    # First BUY index (or -1)\n    first_buy_idx = (\n        df[df[\"is_buy\"] == 1]\n        .groupby(\"user_session\")[\"ev_idx\"].min()\n        .reindex(agg_counts.index).fillna(-1).astype(int).rename(\"idx_first_buy\")\n    )\n\n    # Events after first BUY\n    events_after_buy = (agg_counts[\"n_events\"] - (first_buy_idx + 1)).clip(lower=0).rename(\"events_after_first_buy\")\n\n    # Adds/removes before first BUY\n    tmp = df.merge(first_buy_idx.rename(\"fb\"), left_on=\"user_session\", right_index=True, how=\"left\")\n    before_fb = tmp[\"ev_idx\"] <= tmp[\"fb\"]\n    cnt_add_before_buy = tmp.loc[before_fb, \"is_add\"].groupby(tmp[\"user_session\"]).sum().reindex(agg_counts.index).fillna(0).astype(int).rename(\"cnt_add_before_buy\")\n    cnt_rem_before_buy = tmp.loc[before_fb, \"is_rem\"].groupby(tmp[\"user_session\"]).sum().reindex(agg_counts.index).fillna(0).astype(int).rename(\"cnt_rem_before_buy\")\n\n    # Transitions\n    def count_transitions(g):\n        x = g[\"event_type\"].astype(str).values\n        if len(x) <= 1:\n            return 0\n        return int((x[1:] != x[:-1]).sum())\n    n_transitions = df.groupby(\"user_session\").apply(count_transitions).rename(\"n_transitions\")\n\n    # Time-of-day\n    start_hour = t_start.dt.hour.rename(\"start_hour\")\n    start_dow  = t_start.dt.dayofweek.rename(\"start_dow\")\n    start_day  = t_start.dt.day.rename(\"start_day\")\n\n    # Carry user_id\n    user_map = df.groupby(\"user_session\")[\"user_id\"].first().rename(\"user_id\")\n\n    # Assemble\n    sess = pd.concat(\n        [\n            t_start, t_end, duration, agg_counts,\n            has_buy, first_buy_idx, events_after_buy,\n            cnt_add_before_buy, cnt_rem_before_buy,\n            n_transitions, first_event, last_event,\n            start_hour, start_dow, start_day,\n            user_map,\n        ],\n        axis=1\n    ).reset_index()\n\n    if is_train:\n        t = df.groupby(\"user_session\")[\"session_value\"].first().reset_index()\n        sess = sess.merge(t, on=\"user_session\", how=\"left\")\n        chk = df.groupby(\"user_session\")[\"session_value\"].nunique().max()\n        if chk != 1:\n            print(\"WARNING: session_value is not constant within sessions.\")\n\n    # Cast categoricals\n    for c in [\"first_event_type\", \"last_event_type\"]:\n        if c in sess.columns:\n            sess[c] = sess[c].astype(\"category\")\n    sess[\"user_id\"] = sess[\"user_id\"].astype(str)\n    return sess\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:52:34.103356Z","iopub.execute_input":"2025-08-29T06:52:34.103643Z","iopub.status.idle":"2025-08-29T06:52:34.124058Z","shell.execute_reply.started":"2025-08-29T06:52:34.103625Z","shell.execute_reply":"2025-08-29T06:52:34.122633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sess = build_session_table(train, is_train=True)\ntest_sess  = build_session_table(test,  is_train=False)\nprint(\"Session tables rebuilt; `user_id` included.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:52:45.808267Z","iopub.execute_input":"2025-08-29T06:52:45.808633Z","iopub.status.idle":"2025-08-29T06:53:27.178786Z","shell.execute_reply.started":"2025-08-29T06:52:45.808592Z","shell.execute_reply":"2025-08-29T06:53:27.177574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_user_history(train_sess: pd.DataFrame, test_sess: pd.DataFrame):\n    comb = pd.concat(\n        [train_sess.assign(_is_train=1), test_sess.assign(_is_train=0, **{TARGET_COL: np.nan})],\n        axis=0, ignore_index=True\n    ).sort_values([\"user_id\", \"session_start\"]).reset_index(drop=True)\n\n    # 1) Prior # sessions\n    comb[\"user_prev_n_sessions\"] = comb.groupby(\"user_id\").cumcount()\n\n    # 2) Prior buy-rate\n    g = comb.groupby(\"user_id\", sort=False)\n    pos = g.cumcount() + 1\n    prev_cnt = pos - 1\n    cum_sum_buy = g[\"has_buy\"].cumsum()\n    prev_sum_buy = cum_sum_buy - comb[\"has_buy\"]\n    comb[\"user_prev_buy_rate\"] = np.divide(\n        prev_sum_buy.astype(float), prev_cnt,\n        out=np.zeros_like(prev_sum_buy, dtype=float), where=prev_cnt > 0\n    )\n\n    # 3) Prior mean(session_value) with global backoff\n    comb[\"sv_notna\"]  = comb[TARGET_COL].notna().astype(int)\n    comb[\"sv_filled\"] = comb[TARGET_COL].fillna(0.0)\n    comb[\"cum_sum_sv\"] = g[\"sv_filled\"].cumsum()\n    comb[\"cum_cnt_sv\"] = g[\"sv_notna\"].cumsum()\n    prev_sum_sv = comb[\"cum_sum_sv\"] - comb[\"sv_filled\"]\n    prev_cnt_sv = comb[\"cum_cnt_sv\"] - comb[\"sv_notna\"]\n    prev_mean_sv = np.divide(\n        prev_sum_sv, prev_cnt_sv,\n        out=np.full(len(prev_sum_sv), np.nan, dtype=float), where=prev_cnt_sv > 0\n    )\n    global_mean_sv = float(train_sess[TARGET_COL].mean())\n    comb[\"user_prev_mean_sv\"] = np.where(np.isnan(prev_mean_sv), global_mean_sv, prev_mean_sv)\n\n    comb = comb.drop(columns=[\"sv_notna\",\"sv_filled\",\"cum_sum_sv\",\"cum_cnt_sv\"])\n\n    train_hist = comb[comb[\"_is_train\"] == 1].drop(columns=[\"_is_train\"])\n    test_hist  = comb[comb[\"_is_train\"] == 0].drop(columns=[\"_is_train\"])\n\n    for df in (train_hist, test_hist):\n        df[\"user_prev_n_sessions\"] = df[\"user_prev_n_sessions\"].astype(int)\n        df[\"user_prev_buy_rate\"]   = df[\"user_prev_buy_rate\"].astype(float)\n        df[\"user_prev_mean_sv\"]    = df[\"user_prev_mean_sv\"].astype(float)\n\n    return train_hist, test_hist\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:53:27.180674Z","iopub.execute_input":"2025-08-29T06:53:27.180989Z","iopub.status.idle":"2025-08-29T06:53:27.193144Z","shell.execute_reply.started":"2025-08-29T06:53:27.180968Z","shell.execute_reply":"2025-08-29T06:53:27.191783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if CFG[\"add_user_history\"]:\n    train_sess, test_sess = add_user_history(train_sess, test_sess)\n    print(\"Added user history features.\")\n    display(train_sess.filter(like=\"user_prev\").head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:53:34.114764Z","iopub.execute_input":"2025-08-29T06:53:34.115088Z","iopub.status.idle":"2025-08-29T06:53:34.597756Z","shell.execute_reply.started":"2025-08-29T06:53:34.115066Z","shell.execute_reply":"2025-08-29T06:53:34.595757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Low-risk numeric enrichments (log1p + simple rates)\ndef enrich_small_features(df: pd.DataFrame) -> pd.DataFrame:\n    eps = 1e-6\n    for c in [\"n_events\", \"n_products\", \"n_categories\", \"duration_sec\",\n              \"cnt_buy\", \"cnt_add\", \"cnt_rem\", \"cnt_view\"]:\n        if c in df:\n            df[f\"log1p_{c}\"] = np.log1p(df[c].astype(float))\n\n    if \"n_events\" in df:\n        denom_ev = df[\"n_events\"].clip(lower=1).astype(float)\n        if \"cnt_add\" in df: df[\"rate_add_per_event\"] = df[\"cnt_add\"] / denom_ev\n        if \"cnt_rem\" in df: df[\"rate_rem_per_event\"] = df[\"cnt_rem\"] / denom_ev\n        if \"cnt_buy\" in df: df[\"rate_buy_per_event\"] = df[\"cnt_buy\"] / denom_ev\n\n    if \"duration_sec\" in df and \"n_events\" in df:\n        df[\"events_per_min\"] = df[\"n_events\"] / (df[\"duration_sec\"] / 60.0 + eps)\n    return df\n\ntrain_sess = enrich_small_features(train_sess)\ntest_sess  = enrich_small_features(test_sess)\n\n# Feature columns\ncategorical_cols = [\"first_event_type\", \"last_event_type\"]\nnumeric_cols = [\n    \"n_events\", \"n_products\", \"n_categories\", \"n_event_types\",\n    \"cnt_buy\", \"cnt_add\", \"cnt_rem\", \"cnt_view\",\n    \"duration_sec\", \"has_buy\", \"idx_first_buy\", \"events_after_first_buy\",\n    \"cnt_add_before_buy\", \"cnt_rem_before_buy\", \"n_transitions\",\n    \"start_hour\", \"start_dow\", \"start_day\",\n]\nif CFG[\"add_user_history\"]:\n    numeric_cols += [\"user_prev_n_sessions\", \"user_prev_buy_rate\", \"user_prev_mean_sv\"]\n\n# add enrichments\nextra_feats = [\n    *(f\"log1p_{c}\" for c in [\"n_events\",\"n_products\",\"n_categories\",\"duration_sec\",\"cnt_buy\",\"cnt_add\",\"cnt_rem\",\"cnt_view\"]),\n    \"rate_add_per_event\",\"rate_rem_per_event\",\"rate_buy_per_event\",\"events_per_min\"\n]\nnumeric_cols += [c for c in extra_feats if c in train_sess.columns]\n\n# ensure existence\ncategorical_cols = [c for c in categorical_cols if c in train_sess.columns]\nnumeric_cols = [c for c in numeric_cols if c in train_sess.columns]\nFEATS = categorical_cols + numeric_cols\n\nprint(\"Num features:\", len(FEATS))\nprint(\"Categorical:\", categorical_cols)\nprint(\"Numeric:\", [c for c in FEATS if c not in categorical_cols])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:53:36.729692Z","iopub.execute_input":"2025-08-29T06:53:36.730009Z","iopub.status.idle":"2025-08-29T06:53:36.768362Z","shell.execute_reply.started":"2025-08-29T06:53:36.729988Z","shell.execute_reply":"2025-08-29T06:53:36.766991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After defining extra_feats and numeric_cols\nuse_enrichments = False  # <<< toggle OFF for this last run\nif not use_enrichments:\n    numeric_cols = [c for c in numeric_cols if c not in set(extra_feats)]\n\n# Rebuild FEATS\ncategorical_cols = [c for c in categorical_cols if c in train_sess.columns]\nnumeric_cols     = [c for c in numeric_cols if c in train_sess.columns]\nFEATS = categorical_cols + numeric_cols\n\nprint(\"Num features (final):\", len(FEATS))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:53:40.109483Z","iopub.execute_input":"2025-08-29T06:53:40.110117Z","iopub.status.idle":"2025-08-29T06:53:40.119801Z","shell.execute_reply.started":"2025-08-29T06:53:40.110068Z","shell.execute_reply":"2025-08-29T06:53:40.118046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Target\nif CFG[\"use_log_target\"]:\n    train_sess[\"target\"] = np.log1p(train_sess[TARGET_COL].clip(lower=0))\nelse:\n    train_sess[\"target\"] = train_sess[TARGET_COL].astype(float)\n\n# Categorical dtype for LightGBM\nfor c in categorical_cols:\n    train_sess[c] = train_sess[c].astype(\"category\")\n    test_sess[c]  = test_sess[c].astype(\"category\")\n\nX = train_sess[FEATS].copy()\ny = train_sess[\"target\"].values\nX_test = test_sess[FEATS].copy()\n\n# label-aligned series\ny_s = pd.Series(train_sess[\"target\"].values, index=X.index)\n\nX.shape, X_test.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:53:44.796412Z","iopub.execute_input":"2025-08-29T06:53:44.796738Z","iopub.status.idle":"2025-08-29T06:53:44.826889Z","shell.execute_reply.started":"2025-08-29T06:53:44.796709Z","shell.execute_reply":"2025-08-29T06:53:44.825410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_time_folds(df: pd.DataFrame, n_splits=3, date_col=\"session_start\"):\n    df_sorted = df.sort_values(date_col).reset_index()\n    n = len(df_sorted)\n    fold_sizes = [n // n_splits] * n_splits\n    for i in range(n % n_splits):\n        fold_sizes[i] += 1\n    idxs, start = [], 0\n    for fs in fold_sizes:\n        end = start + fs\n        idxs.append(df_sorted.loc[start:end-1, \"index\"].values)\n        start = end\n    folds = []\n    for i in range(n_splits):\n        val_idx = idxs[i]\n        tr_idx = np.concatenate([idxs[j] for j in range(n_splits) if j != i])\n        folds.append((tr_idx, val_idx))\n    return folds\n\ndef make_group_folds(df: pd.DataFrame, n_splits=5, group_col=\"user_id\"):\n    idx = np.arange(len(df))\n    gkf = GroupKFold(n_splits=n_splits)\n    return [(tr, va) for tr, va in gkf.split(idx, groups=df[group_col].values)]\n\n# quick view of time-fold windows (unchanged)\nfolds_preview = make_time_folds(train_sess, n_splits=CFG[\"n_splits\"], date_col=\"session_start\")\nfor i, (_, va) in enumerate(folds_preview):\n    d1 = train_sess.loc[va, \"session_start\"].min()\n    d2 = train_sess.loc[va, \"session_start\"].max()\n    print(f\"Time Fold {i}: val window {d1} â†’ {d2}, size={len(va)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:53:50.122522Z","iopub.execute_input":"2025-08-29T06:53:50.123530Z","iopub.status.idle":"2025-08-29T06:53:50.185825Z","shell.execute_reply.started":"2025-08-29T06:53:50.123369Z","shell.execute_reply":"2025-08-29T06:53:50.184914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== FIXED Cell 11: multi-seed, CV realism, with index â†’ position mapping ====\noof_all = np.zeros(len(X), dtype=float)\ntest_preds_seeds = []\nfeat_imps = []\n\nglobal_mean_sv = float(train_sess[TARGET_COL].mean())\nX_index = pd.Index(X.index)  # to map label indices to positions\n\nfor seed in CFG[\"seeds\"]:\n    lgb_params = CFG[\"lgb_params\"].copy()\n    lgb_params.update({\"seed\": seed, \"bagging_seed\": seed, \"feature_fraction_seed\": seed})\n    set_seed(seed)\n\n    if CFG[\"cv_type\"] == \"time\":\n        folds = make_time_folds(train_sess, n_splits=CFG[\"n_splits\"], date_col=\"session_start\")\n    else:\n        folds = make_group_folds(train_sess, n_splits=CFG[\"n_splits_group\"], group_col=\"user_id\")\n\n    oof_seed = np.zeros(len(X), dtype=float)\n    best_iters = []\n\n    for i, (tr_idx, va_idx) in enumerate(folds):\n        # Map label indices -> positional indices for any ndarray assignments\n        tr_pos = X_index.get_indexer(tr_idx)\n        va_pos = X_index.get_indexer(va_idx)\n\n        # Guard: no missing labels (-1) and all within bounds\n        if (tr_pos < 0).any() or (va_pos < 0).any():\n            raise ValueError(f\"Fold {i}: found indices not present in X.index. \"\n                             f\"Try resetting indices before building folds.\")\n        if (va_pos >= len(X)).any() or (tr_pos >= len(X)).any():\n            raise ValueError(f\"Fold {i}: positional indices out of bounds.\")\n\n        # Use .iloc with positions for safety; .loc with labels would also work\n        X_tr, y_tr = X.iloc[tr_pos], y_s.iloc[tr_pos].values\n        X_va, y_va = X.iloc[va_pos], y_s.iloc[va_pos].values\n\n        # Group CV realism: neutralize user history for val users (simulate unseen)\n        if CFG[\"cv_type\"] != \"time\":\n            X_va = X_va.copy()\n            if \"user_prev_mean_sv\" in X_va: X_va[\"user_prev_mean_sv\"] = global_mean_sv\n            if \"user_prev_buy_rate\" in X_va: X_va[\"user_prev_buy_rate\"] = 0.0\n            if \"user_prev_n_sessions\" in X_va: X_va[\"user_prev_n_sessions\"] = 0\n\n        lgb_train = lgb.Dataset(X_tr, label=y_tr, categorical_feature=categorical_cols, free_raw_data=False)\n        lgb_valid = lgb.Dataset(X_va, label=y_va, categorical_feature=categorical_cols, free_raw_data=False)\n\n        model = lgb.train(\n            lgb_params, lgb_train,\n            num_boost_round=CFG[\"num_boost_round\"],\n            valid_sets=[lgb_train, lgb_valid],\n            valid_names=[\"train\", \"valid\"],\n            callbacks=[lgb.early_stopping(CFG[\"early_stopping_rounds\"], verbose=False)]\n        )\n        best_iters.append(model.best_iteration)\n\n        pred_va = model.predict(X_va, num_iteration=model.best_iteration)\n        if CFG[\"use_log_target\"]:\n            pred_va = np.expm1(pred_va).clip(min=0)\n\n        # Apply the same post-process as test-time\n        pred_va = np.clip(pred_va, CFG[\"clip\"][\"floor\"], CFG[\"clip\"][\"cap\"])\n\n        # <<< KEY FIX: assign via positional indices >>>\n        oof_seed[va_pos] = pred_va\n\n        fi = pd.DataFrame({\n            \"feature\": FEATS,\n            \"gain\": model.feature_importance(importance_type=\"gain\"),\n            \"split\": model.feature_importance(importance_type=\"split\"),\n            \"fold\": i,\n            \"seed\": seed,\n        })\n        feat_imps.append(fi)\n\n        print(f\"[seed {seed}] Fold {i}: best_iter={model.best_iteration}, val_size={len(va_pos)}\")\n\n    oof_all += oof_seed / len(CFG[\"seeds\"])\n\n    full_iters = int(np.mean(best_iters))\n    full_ds = lgb.Dataset(X, label=y_s.values, categorical_feature=categorical_cols, free_raw_data=False)\n    full_model = lgb.train(lgb_params, full_ds, num_boost_round=full_iters)\n\n    pred_test = full_model.predict(X_test)\n    if CFG[\"use_log_target\"]:\n        pred_test = np.expm1(pred_test).clip(min=0)\n    pred_test = np.clip(pred_test, CFG[\"clip\"][\"floor\"], CFG[\"clip\"][\"cap\"])\n    test_preds_seeds.append(pred_test)\n\n# OOF MSE on raw scale\nif CFG[\"use_log_target\"]:\n    y_raw = np.expm1(y_s.values)\n    oof_mse = mean_squared_error(y_raw, oof_all)\nelse:\n    oof_mse = mean_squared_error(y_s.values, oof_all)\nprint(f\"OOF MSE (post-processed): {oof_mse:,.4f}\")\n\nfeat_importance = (\n    pd.concat(feat_imps, ignore_index=True)\n      .groupby(\"feature\")[[\"gain\",\"split\"]].mean()\n      .sort_values(\"gain\", ascending=False).reset_index()\n)\ndisplay(feat_importance.head(30))\n\ntest_pred = np.mean(test_preds_seeds, axis=0)\nprint(\"Pred summary (post-processed):\")\ndisplay(pd.Series(test_pred).describe())\n\noof = oof_all\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:54:01.841112Z","iopub.execute_input":"2025-08-29T06:54:01.841430Z","iopub.status.idle":"2025-08-29T06:55:28.885010Z","shell.execute_reply.started":"2025-08-29T06:54:01.841409Z","shell.execute_reply":"2025-08-29T06:55:28.883758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expect submission format: [\"user_session\", \"session_value\"]\nsub_out = sub.copy()\nkey = \"user_session\"\n\n# Map predictions by user_session (test_sess has one row per session)\npred_map = dict(zip(test_sess[key], test_pred))\nsub_out[TARGET_COL] = sub_out[key].map(pred_map).fillna(CFG[\"clip\"][\"floor\"])\n\nsave_name = \"submission_baseline_v1_0_3.csv\"\nsub_out.to_csv(save_name, index=False)\nprint(\"Saved:\", save_name)\ndisplay(sub_out.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:56:12.623098Z","iopub.execute_input":"2025-08-29T06:56:12.623474Z","iopub.status.idle":"2025-08-29T06:56:12.736337Z","shell.execute_reply.started":"2025-08-29T06:56:12.623448Z","shell.execute_reply":"2025-08-29T06:56:12.735465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# OOF vs target summary (raw scale)\nif CFG[\"use_log_target\"]:\n    y_raw = np.expm1(y_s.values).clip(min=0)\n    print(\"OOF MSE (raw):\", mean_squared_error(y_raw, oof))\n    display(pd.DataFrame({\"y\": y_raw, \"oof\": oof}).describe())\nelse:\n    print(\"OOF MSE (raw):\", mean_squared_error(y_s.values, oof))\n    display(pd.DataFrame({\"y\": y_s.values, \"oof\": oof}).describe())\n\n# Per-day MSE to spot drift\ntmp = train_sess.assign(y=y_s.values, oof=oof)\nday_mse = tmp.groupby(tmp[\"session_start\"].dt.date).apply(lambda d: mean_squared_error(d[\"y\"], d[\"oof\"]))\ndisplay(day_mse.to_frame(\"mse\").reset_index().rename(columns={\"session_start\":\"date\"}).head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:56:17.333009Z","iopub.execute_input":"2025-08-29T06:56:17.333296Z","iopub.status.idle":"2025-08-29T06:56:17.435743Z","shell.execute_reply.started":"2025-08-29T06:56:17.333275Z","shell.execute_reply":"2025-08-29T06:56:17.434936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}