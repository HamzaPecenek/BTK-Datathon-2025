{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112016,"databundleVersionId":13341508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================\n# BTK Datathon 2025 — Baseline v1\n# =============================\n\nimport os, sys, gc, math, json, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nwarnings.filterwarnings(\"ignore\")\n\nCFG = {\n    \"seed\": 42,\n    \"use_log_target\": False,      # set True to train on log1p(target) and back-transform\n    \"add_user_history\": True,     # toggle user expanding features\n    \"add_sequence_extras\": True,  # toggle sequence-derived features (pre/post BUY etc.)\n    \"lgb_params\": {\n        \"objective\": \"regression\",\n        \"metric\": \"mse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 63,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"min_data_in_leaf\": 50,\n        \"max_depth\": -1,\n        \"verbosity\": -1,\n        \"seed\": 42,\n        \"bagging_seed\": 42,\n        \"feature_fraction_seed\": 42,\n        \"force_row_wise\": True,   # safer on Kaggle runtimes\n    },\n    # Time-based CV windows (inclusive ranges of day-of-month or absolute dates; we’ll derive from event_time)\n    # We’ll auto-derive by date, but you can override with explicit cut points later.\n    \"n_splits\": 3,\n    \"early_stopping_rounds\": 300,\n    \"num_boost_round\": 5000,\n}\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n\nset_seed(CFG[\"seed\"])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:38:23.081144Z","iopub.execute_input":"2025-08-27T05:38:23.082272Z","iopub.status.idle":"2025-08-27T05:38:23.089666Z","shell.execute_reply.started":"2025-08-27T05:38:23.082220Z","shell.execute_reply":"2025-08-27T05:38:23.088733Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Try common Kaggle paths first, then fallback to cwd.\nCANDIDATE_DIRS = [\n    Path(\"/kaggle/input/datathon-2025\"),]\n\ndef find_csv(filename: str) -> Path:\n    for d in CANDIDATE_DIRS:\n        p = d / filename\n        if p.exists():\n            return p\n    raise FileNotFoundError(f\"Could not find {filename} in {CANDIDATE_DIRS}\")\n\ntrain_path = find_csv(\"train.csv\")\ntest_path  = find_csv(\"test.csv\")\nsub_path   = find_csv(\"sample_submission.csv\")\n\ntrain_path, test_path, sub_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:38:52.392180Z","iopub.execute_input":"2025-08-27T05:38:52.392489Z","iopub.status.idle":"2025-08-27T05:38:52.404815Z","shell.execute_reply.started":"2025-08-27T05:38:52.392470Z","shell.execute_reply":"2025-08-27T05:38:52.404058Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(PosixPath('/kaggle/input/datathon-2025/train.csv'),\n PosixPath('/kaggle/input/datathon-2025/test.csv'),\n PosixPath('/kaggle/input/datathon-2025/sample_submission.csv'))"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Auto-detect the time column name (common variants)\ndef detect_time_col(cols):\n    cand = [c for c in cols if c.lower() in (\"event_time\", \"event_timestamp\", \"timestamp\", \"time\", \"event_datetime\")]\n    if cand:\n        return cand[0]\n    # Best-effort: look for a column with \"time\" in its name\n    cand = [c for c in cols if \"time\" in c.lower() or \"date\" in c.lower()]\n    return cand[0] if cand else None\n\ndef read_df(path):\n    df = pd.read_csv(path)\n    tcol = detect_time_col(df.columns)\n    if tcol is None:\n        raise ValueError(\"Couldn't detect a time column. Please update detect_time_col().\")\n    df[tcol] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True)\n    # Make a normalized name for downstream code\n    if tcol != \"event_time\":\n        df = df.rename(columns={tcol: \"event_time\"})\n    return df\n\ntrain = read_df(train_path)\ntest  = read_df(test_path)\nsub   = pd.read_csv(sub_path)\n\nprint(\"Shapes:\", train.shape, test.shape, sub.shape)\nprint(\"\\nTrain columns:\\n\", list(train.columns))\nprint(\"\\nTest columns:\\n\", list(test.columns))\nprint(\"\\nSubmission columns:\\n\", list(sub.columns))\n\n# Sanity: expected target column must exist only in train\nTARGET_COL = \"session_value\"\nassert TARGET_COL in train.columns and TARGET_COL not in test.columns, \"Target column check failed.\"\n\n# Light touch peek (head only)\ndisplay(train.head(3))\ndisplay(test.head(3))\ndisplay(sub.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:39:02.903837Z","iopub.execute_input":"2025-08-27T05:39:02.904181Z","iopub.status.idle":"2025-08-27T05:39:03.992016Z","shell.execute_reply.started":"2025-08-27T05:39:02.904155Z","shell.execute_reply":"2025-08-27T05:39:03.991186Z"}},"outputs":[{"name":"stdout","text":"Shapes: (141219, 7) (62951, 6) (30789, 2)\n\nTrain columns:\n ['event_time', 'event_type', 'product_id', 'category_id', 'user_id', 'user_session', 'session_value']\n\nTest columns:\n ['event_time', 'event_type', 'product_id', 'category_id', 'user_id', 'user_session']\n\nSubmission columns:\n ['user_session', 'session_value']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                 event_time event_type   product_id category_id      user_id  \\\n0 2025-06-19 10:23:07+00:00   ADD_CART  PROD_011223   CAT_00054  USER_097562   \n1 2025-06-07 21:34:45+00:00   ADD_CART  PROD_005519   CAT_00144  USER_006535   \n2 2025-06-21 21:29:09+00:00   ADD_CART  PROD_000577   CAT_00273  USER_047199   \n\n     user_session  session_value  \n0  SESSION_158779          90.29  \n1  SESSION_029987          16.39  \n2  SESSION_022134          64.27  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_time</th>\n      <th>event_type</th>\n      <th>product_id</th>\n      <th>category_id</th>\n      <th>user_id</th>\n      <th>user_session</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-06-19 10:23:07+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_011223</td>\n      <td>CAT_00054</td>\n      <td>USER_097562</td>\n      <td>SESSION_158779</td>\n      <td>90.29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-06-07 21:34:45+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_005519</td>\n      <td>CAT_00144</td>\n      <td>USER_006535</td>\n      <td>SESSION_029987</td>\n      <td>16.39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-06-21 21:29:09+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_000577</td>\n      <td>CAT_00273</td>\n      <td>USER_047199</td>\n      <td>SESSION_022134</td>\n      <td>64.27</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                 event_time event_type   product_id category_id      user_id  \\\n0 2025-06-28 10:09:58+00:00   ADD_CART  PROD_015000   CAT_00019  USER_109759   \n1 2025-06-25 11:57:50+00:00   ADD_CART  PROD_023887   CAT_00010  USER_010614   \n2 2025-06-30 14:34:20+00:00   ADD_CART  PROD_022673   CAT_00090  USER_041338   \n\n     user_session  \n0  SESSION_164059  \n1  SESSION_109583  \n2  SESSION_171382  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_time</th>\n      <th>event_type</th>\n      <th>product_id</th>\n      <th>category_id</th>\n      <th>user_id</th>\n      <th>user_session</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-06-28 10:09:58+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_015000</td>\n      <td>CAT_00019</td>\n      <td>USER_109759</td>\n      <td>SESSION_164059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-06-25 11:57:50+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_023887</td>\n      <td>CAT_00010</td>\n      <td>USER_010614</td>\n      <td>SESSION_109583</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-06-30 14:34:20+00:00</td>\n      <td>ADD_CART</td>\n      <td>PROD_022673</td>\n      <td>CAT_00090</td>\n      <td>USER_041338</td>\n      <td>SESSION_171382</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     user_session  session_value\n0  SESSION_164059            0.0\n1  SESSION_109583            0.0\n2  SESSION_171382            0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_session</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SESSION_164059</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SESSION_109583</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SESSION_171382</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"before = len(train)\ntrain = train.drop_duplicates().reset_index(drop=True)\nafter = len(train)\nprint(f\"Dropped {before - after} duplicate rows from train.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:39:19.691777Z","iopub.execute_input":"2025-08-27T05:39:19.692103Z","iopub.status.idle":"2025-08-27T05:39:19.842830Z","shell.execute_reply.started":"2025-08-27T05:39:19.692077Z","shell.execute_reply":"2025-08-27T05:39:19.841804Z"}},"outputs":[{"name":"stdout","text":"Dropped 670 duplicate rows from train.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Normalize frequent columns; adapt these names if needed\nID_USER = \"user_id\"\nID_SESSION = \"user_session\"\nPRODUCT_COL = \"product_id\"\nCATEGORY_COL = \"category_id\"\nEVENT_COL = \"event_type\"  # expected values like VIEW/ADD_CART/REMOVE_CART/BUY\n\nexpected_cols = [ID_USER, ID_SESSION, PRODUCT_COL, CATEGORY_COL, EVENT_COL, \"event_time\"]\nfor c in expected_cols:\n    if c not in train.columns:\n        print(f\"Warning: expected column '{c}' not found in train.\")\n\n# Coerce to appropriate dtypes (safe casts)\nfor df in (train, test):\n    for c in [ID_USER, ID_SESSION]:\n        if c in df.columns:\n            df[c] = df[c].astype(str)\n    for c in [PRODUCT_COL, CATEGORY_COL, EVENT_COL]:\n        if c in df.columns:\n            df[c] = df[c].astype(\"category\")\n\n# Event type ordering (helps some models and features)\nif EVENT_COL in train.columns:\n    all_types = sorted(list(set(train[EVENT_COL].dropna().unique()).union(set(test[EVENT_COL].dropna().unique()))))\n    train[EVENT_COL] = train[EVENT_COL].cat.set_categories(all_types)\n    test[EVENT_COL]  = test[EVENT_COL].cat.set_categories(all_types)\n\nprint(\"Event types:\", train[EVENT_COL].cat.categories.tolist() if EVENT_COL in train.columns else \"N/A\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:40:21.767754Z","iopub.execute_input":"2025-08-27T05:40:21.768105Z","iopub.status.idle":"2025-08-27T05:40:21.889873Z","shell.execute_reply.started":"2025-08-27T05:40:21.768071Z","shell.execute_reply":"2025-08-27T05:40:21.889206Z"}},"outputs":[{"name":"stdout","text":"Event types: ['ADD_CART', 'BUY', 'REMOVE_CART', 'VIEW']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 6 — Session-level feature builder (updated to include `user_id`)\ndef build_session_table(events: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n    df = events.copy()\n    df = df.sort_values([\"user_session\", \"event_time\"]).reset_index(drop=True)\n\n    # Basic per-event helpers\n    df[\"is_buy\"] = (df[\"event_type\"] == \"BUY\").astype(int)\n    df[\"is_add\"] = (df[\"event_type\"] == \"ADD_CART\").astype(int)\n    df[\"is_rem\"] = (df[\"event_type\"] == \"REMOVE_CART\").astype(int)\n    df[\"is_view\"] = (df[\"event_type\"] == \"VIEW\").astype(int)\n\n    # Rank events within a session\n    df[\"ev_idx\"] = df.groupby(\"user_session\").cumcount()\n\n    # First/last event type\n    first_event = df.groupby(\"user_session\")[\"event_type\"].first().rename(\"first_event_type\")\n    last_event  = df.groupby(\"user_session\")[\"event_type\"].last().rename(\"last_event_type\")\n\n    # Session start/end time\n    t_start = df.groupby(\"user_session\")[\"event_time\"].min().rename(\"session_start\")\n    t_end   = df.groupby(\"user_session\")[\"event_time\"].max().rename(\"session_end\")\n    duration = (t_end - t_start).dt.total_seconds().rename(\"duration_sec\")\n\n    # Counts & uniques\n    agg_counts = df.groupby(\"user_session\").agg(\n        n_events = (\"event_type\", \"size\"),\n        n_products = (\"product_id\", pd.Series.nunique),\n        n_categories = (\"category_id\", pd.Series.nunique),\n        n_event_types = (\"event_type\", pd.Series.nunique),\n        cnt_buy = (\"is_buy\", \"sum\"),\n        cnt_add = (\"is_add\", \"sum\"),\n        cnt_rem = (\"is_rem\", \"sum\"),\n        cnt_view = (\"is_view\", \"sum\"),\n    )\n\n    # Has buy flag\n    has_buy = (agg_counts[\"cnt_buy\"] > 0).astype(int).rename(\"has_buy\")\n\n    # Index of first BUY (or -1)\n    first_buy_idx = (\n        df[df[\"is_buy\"] == 1]\n        .groupby(\"user_session\")[\"ev_idx\"]\n        .min()\n        .reindex(agg_counts.index)\n        .fillna(-1)\n        .astype(int)\n        .rename(\"idx_first_buy\")\n    )\n\n    # Events after first BUY (0 if none)\n    events_after_buy = (agg_counts[\"n_events\"] - (first_buy_idx + 1)).clip(lower=0).rename(\"events_after_first_buy\")\n\n    # Adds/removes before first BUY\n    tmp = df.merge(first_buy_idx.rename(\"fb\"), left_on=\"user_session\", right_index=True, how=\"left\")\n    before_fb = tmp[\"ev_idx\"] <= tmp[\"fb\"]\n    cnt_add_before_buy = tmp.loc[before_fb, \"is_add\"].groupby(tmp[\"user_session\"]).sum().reindex(agg_counts.index).fillna(0).astype(int).rename(\"cnt_add_before_buy\")\n    cnt_rem_before_buy = tmp.loc[before_fb, \"is_rem\"].groupby(tmp[\"user_session\"]).sum().reindex(agg_counts.index).fillna(0).astype(int).rename(\"cnt_rem_before_buy\")\n\n    # Transitions count\n    def count_transitions(g):\n        x = g[\"event_type\"].astype(str).values\n        if len(x) <= 1:\n            return 0\n        return int((x[1:] != x[:-1]).sum())\n    n_transitions = df.groupby(\"user_session\").apply(count_transitions).rename(\"n_transitions\")\n\n    # Time-of-day features\n    start_hour = t_start.dt.hour.rename(\"start_hour\")\n    start_dow  = t_start.dt.dayofweek.rename(\"start_dow\")\n    start_day  = t_start.dt.day.rename(\"start_day\")\n\n    # >>> NEW: carry user_id per session\n    user_map = df.groupby(\"user_session\")[\"user_id\"].first().rename(\"user_id\")\n\n    # Assemble\n    sess = pd.concat(\n        [\n            t_start, t_end, duration, agg_counts,\n            has_buy, first_buy_idx, events_after_buy,\n            cnt_add_before_buy, cnt_rem_before_buy,\n            n_transitions, first_event, last_event,\n            start_hour, start_dow, start_day,\n            user_map,\n        ],\n        axis=1\n    ).reset_index()\n\n    if is_train:\n        t = df.groupby(\"user_session\")[\"session_value\"].first().reset_index()\n        sess = sess.merge(t, on=\"user_session\", how=\"left\")\n        chk = df.groupby(\"user_session\")[\"session_value\"].nunique().max()\n        if chk != 1:\n            print(\"WARNING: session_value is not constant within sessions.\")\n\n    # Cast categoricals\n    for c in [\"first_event_type\", \"last_event_type\"]:\n        if c in sess.columns:\n            sess[c] = sess[c].astype(\"category\")\n    sess[\"user_id\"] = sess[\"user_id\"].astype(str)\n\n    return sess\n\n# Rebuild with updated function\ntrain_sess = build_session_table(train, is_train=True)\ntest_sess  = build_session_table(test,  is_train=False)\nprint(\"Session tables rebuilt; `user_id` included.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:43:38.701595Z","iopub.execute_input":"2025-08-27T05:43:38.701929Z","iopub.status.idle":"2025-08-27T05:44:19.517844Z","shell.execute_reply.started":"2025-08-27T05:43:38.701902Z","shell.execute_reply":"2025-08-27T05:44:19.517003Z"}},"outputs":[{"name":"stdout","text":"Session tables rebuilt; `user_id` included.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 7 — (Fixed) User expanding history — index-safe & fast\ndef add_user_history(train_sess: pd.DataFrame, test_sess: pd.DataFrame):\n    # Combine (train first, then test), keep chronological order within each user\n    comb = pd.concat(\n        [\n            train_sess.assign(_is_train=1),\n            test_sess.assign(_is_train=0, **{TARGET_COL: np.nan}),\n        ],\n        axis=0, ignore_index=True\n    ).sort_values([\"user_id\", \"session_start\"]).reset_index(drop=True)\n\n    # --- 1) Prior # of sessions (strictly before current) ---\n    comb[\"user_prev_n_sessions\"] = comb.groupby(\"user_id\").cumcount()\n\n    # --- 2) Prior buy-rate (expanding mean of has_buy, shifted) ---\n    g = comb.groupby(\"user_id\", sort=False)\n    # cumulative count position (1-based) and previous count\n    pos = g.cumcount() + 1\n    prev_cnt = pos - 1\n\n    # cumulative sum of has_buy including current, then subtract current for \"previous\" sum\n    cum_sum_buy = g[\"has_buy\"].cumsum()\n    prev_sum_buy = cum_sum_buy - comb[\"has_buy\"]\n    # safe divide\n    comb[\"user_prev_buy_rate\"] = np.divide(\n        prev_sum_buy.astype(float),\n        prev_cnt,\n        out=np.zeros_like(prev_sum_buy, dtype=float),\n        where=prev_cnt > 0\n    )\n\n    # --- 3) Prior mean(session_value) (labels exist only in train) ---\n    # create helper columns to count only non-NaN labels\n    comb[\"sv_notna\"]  = comb[TARGET_COL].notna().astype(int)\n    comb[\"sv_filled\"] = comb[TARGET_COL].fillna(0.0)\n\n    comb[\"cum_sum_sv\"]  = g[\"sv_filled\"].cumsum()\n    comb[\"cum_cnt_sv\"]  = g[\"sv_notna\"].cumsum()\n\n    prev_sum_sv = comb[\"cum_sum_sv\"] - comb[\"sv_filled\"]\n    prev_cnt_sv = comb[\"cum_cnt_sv\"] - comb[\"sv_notna\"]\n\n    prev_mean_sv = np.divide(\n        prev_sum_sv,\n        prev_cnt_sv,\n        out=np.full(len(prev_sum_sv), np.nan, dtype=float),\n        where=prev_cnt_sv > 0\n    )\n\n    global_mean_sv = float(train_sess[TARGET_COL].mean())\n    comb[\"user_prev_mean_sv\"] = np.where(np.isnan(prev_mean_sv), global_mean_sv, prev_mean_sv)\n\n    # cleanup helpers\n    comb = comb.drop(columns=[\"sv_notna\",\"sv_filled\",\"cum_sum_sv\",\"cum_cnt_sv\"])\n\n    # --- Split back ---\n    train_hist = comb[comb[\"_is_train\"] == 1].drop(columns=[\"_is_train\"])\n    test_hist  = comb[comb[\"_is_train\"] == 0].drop(columns=[\"_is_train\"])\n\n    # types\n    for df in (train_hist, test_hist):\n        df[\"user_prev_n_sessions\"] = df[\"user_prev_n_sessions\"].astype(int)\n        df[\"user_prev_buy_rate\"]   = df[\"user_prev_buy_rate\"].astype(float)\n        df[\"user_prev_mean_sv\"]    = df[\"user_prev_mean_sv\"].astype(float)\n\n    return train_hist, test_hist\n\n# Run it\nif CFG[\"add_user_history\"]:\n    train_sess, test_sess = add_user_history(train_sess, test_sess)\n    print(\"Added user history features.\")\n    display(train_sess.filter(like=\"user_prev\").head(3))\n# Feature columns\ncategorical_cols = [\"first_event_type\", \"last_event_type\"]\nnumeric_cols = [\n    \"n_events\", \"n_products\", \"n_categories\", \"n_event_types\",\n    \"cnt_buy\", \"cnt_add\", \"cnt_rem\", \"cnt_view\",\n    \"duration_sec\", \"has_buy\", \"idx_first_buy\", \"events_after_first_buy\",\n    \"cnt_add_before_buy\", \"cnt_rem_before_buy\", \"n_transitions\",\n    \"start_hour\", \"start_dow\", \"start_day\",\n]\n\nif CFG[\"add_user_history\"]:\n    numeric_cols += [\"user_prev_n_sessions\", \"user_prev_buy_rate\", \"user_prev_mean_sv\"]\n\n# Ensure columns exist (robustness)\ncategorical_cols = [c for c in categorical_cols if c in train_sess.columns]\nnumeric_cols = [c for c in numeric_cols if c in train_sess.columns]\n\nFEATS = categorical_cols + numeric_cols\nprint(\"Num features:\", len(FEATS))\nprint(\"Categorical:\", categorical_cols)\nprint(\"Numeric:\", [c for c in FEATS if c not in categorical_cols])\n\n# Target\nif CFG[\"use_log_target\"]:\n    train_sess[\"target\"] = np.log1p(train_sess[TARGET_COL].clip(lower=0))\nelse:\n    train_sess[\"target\"] = train_sess[TARGET_COL].astype(float)\n\n# LightGBM requires category dtype for cat features\nfor c in categorical_cols:\n    train_sess[c] = train_sess[c].astype(\"category\")\n    test_sess[c]  = test_sess[c].astype(\"category\")\n\nX = train_sess[FEATS].copy()\ny = train_sess[\"target\"].values\nX_test = test_sess[FEATS].copy()\n\nX.shape, X_test.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:00:59.828412Z","iopub.execute_input":"2025-08-27T06:00:59.828757Z","iopub.status.idle":"2025-08-27T06:01:00.368130Z","shell.execute_reply.started":"2025-08-27T06:00:59.828731Z","shell.execute_reply":"2025-08-27T06:01:00.367349Z"}},"outputs":[{"name":"stdout","text":"Added user history features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   user_prev_n_sessions  user_prev_buy_rate  user_prev_mean_sv\n0                     0                 0.0           42.19813\n1                     0                 0.0           42.19813\n2                     0                 0.0           42.19813","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_prev_n_sessions</th>\n      <th>user_prev_buy_rate</th>\n      <th>user_prev_mean_sv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>42.19813</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>42.19813</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>42.19813</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Feature columns\ncategorical_cols = [\"first_event_type\", \"last_event_type\"]\nnumeric_cols = [\n    \"n_events\", \"n_products\", \"n_categories\", \"n_event_types\",\n    \"cnt_buy\", \"cnt_add\", \"cnt_rem\", \"cnt_view\",\n    \"duration_sec\", \"has_buy\", \"idx_first_buy\", \"events_after_first_buy\",\n    \"cnt_add_before_buy\", \"cnt_rem_before_buy\", \"n_transitions\",\n    \"start_hour\", \"start_dow\", \"start_day\",\n]\n\nif CFG[\"add_user_history\"]:\n    numeric_cols += [\"user_prev_n_sessions\", \"user_prev_buy_rate\", \"user_prev_mean_sv\"]\n\n# Ensure columns exist (robustness)\ncategorical_cols = [c for c in categorical_cols if c in train_sess.columns]\nnumeric_cols = [c for c in numeric_cols if c in train_sess.columns]\n\nFEATS = categorical_cols + numeric_cols\nprint(\"Num features:\", len(FEATS))\nprint(\"Categorical:\", categorical_cols)\nprint(\"Numeric:\", [c for c in FEATS if c not in categorical_cols])\n\n# Target\nif CFG[\"use_log_target\"]:\n    train_sess[\"target\"] = np.log1p(train_sess[TARGET_COL].clip(lower=0))\nelse:\n    train_sess[\"target\"] = train_sess[TARGET_COL].astype(float)\n\n# LightGBM requires category dtype for cat features\nfor c in categorical_cols:\n    train_sess[c] = train_sess[c].astype(\"category\")\n    test_sess[c]  = test_sess[c].astype(\"category\")\n\nX = train_sess[FEATS].copy()\ny = train_sess[\"target\"].values\nX_test = test_sess[FEATS].copy()\n\nX.shape, X_test.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:01:37.537355Z","iopub.execute_input":"2025-08-27T06:01:37.537906Z","iopub.status.idle":"2025-08-27T06:01:37.568886Z","shell.execute_reply.started":"2025-08-27T06:01:37.537880Z","shell.execute_reply":"2025-08-27T06:01:37.567961Z"}},"outputs":[{"name":"stdout","text":"Num features: 23\nCategorical: ['first_event_type', 'last_event_type']\nNumeric: ['n_events', 'n_products', 'n_categories', 'n_event_types', 'cnt_buy', 'cnt_add', 'cnt_rem', 'cnt_view', 'duration_sec', 'has_buy', 'idx_first_buy', 'events_after_first_buy', 'cnt_add_before_buy', 'cnt_rem_before_buy', 'n_transitions', 'start_hour', 'start_dow', 'start_day', 'user_prev_n_sessions', 'user_prev_buy_rate', 'user_prev_mean_sv']\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"((70736, 23), (30789, 23))"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# We create 3 chronological folds that mimic the competition's temporal split.\n# You can inspect date distribution and adjust cut points if desired.\n\ndef make_time_folds(df: pd.DataFrame, n_splits=3, date_col=\"session_start\"):\n    # Sort sessions by start time and split by contiguous chunks\n    df_sorted = df.sort_values(date_col).reset_index()\n    n = len(df_sorted)\n    fold_sizes = [n // n_splits] * n_splits\n    for i in range(n % n_splits):\n        fold_sizes[i] += 1\n    idxs = []\n    start = 0\n    for fs in fold_sizes:\n        end = start + fs\n        idxs.append(df_sorted.loc[start:end-1, \"index\"].values)\n        start = end\n    folds = []\n    for i in range(n_splits):\n        val_idx = idxs[i]\n        tr_idx = np.concatenate([idxs[j] for j in range(n_splits) if j != i])\n        folds.append((tr_idx, val_idx))\n    return folds\n\nfolds = make_time_folds(train_sess, n_splits=CFG[\"n_splits\"], date_col=\"session_start\")\nfor i, (_, va) in enumerate(folds):\n    d1 = train_sess.loc[va, \"session_start\"].min()\n    d2 = train_sess.loc[va, \"session_start\"].max()\n    print(f\"Fold {i}: val window {d1} → {d2}, size={len(va)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:01:47.700303Z","iopub.execute_input":"2025-08-27T06:01:47.700597Z","iopub.status.idle":"2025-08-27T06:01:47.763608Z","shell.execute_reply.started":"2025-08-27T06:01:47.700577Z","shell.execute_reply":"2025-08-27T06:01:47.762706Z"}},"outputs":[{"name":"stdout","text":"Fold 0: val window 2025-06-01 00:00:24+00:00 → 2025-06-07 03:06:40+00:00, size=23579\nFold 1: val window 2025-06-07 03:06:51+00:00 → 2025-06-14 09:07:02+00:00, size=23579\nFold 2: val window 2025-06-14 09:07:35+00:00 → 2025-06-21 23:58:05+00:00, size=23578\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 10 — Train LightGBM with time-based CV (index-safe)\n\noof_s = pd.Series(0.0, index=X.index)  # OOF predictions aligned by label index\nmodels = []\nfi_frames = []\n\n# Align y to X's index for label-based indexing\ny_s = pd.Series(train_sess[\"target\"].values, index=X.index)\n\n# Optional guard: ensure fold indices line up with X\nbad = [i for i, (tr, va) in enumerate(folds)\n       if not set(tr).issubset(set(X.index)) or not set(va).issubset(set(X.index))]\nif bad:\n    raise ValueError(f\"Fold indices not aligned with X.index on folds {bad}. \"\n                     f\"Rebuild folds after any reindexing.\")\n\nfor i, (tr_idx, va_idx) in enumerate(folds):\n    # Use .loc (label-based), not .iloc\n    X_tr, y_tr = X.loc[tr_idx], y_s.loc[tr_idx].values\n    X_va, y_va = X.loc[va_idx], y_s.loc[va_idx].values\n\n    lgb_train = lgb.Dataset(\n        X_tr, label=y_tr, categorical_feature=categorical_cols, free_raw_data=False\n    )\n    lgb_valid = lgb.Dataset(\n        X_va, label=y_va, categorical_feature=categorical_cols, free_raw_data=False\n    )\n\n    model = lgb.train(\n        CFG[\"lgb_params\"],\n        lgb_train,\n        num_boost_round=CFG[\"num_boost_round\"],\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        callbacks=[lgb.early_stopping(stopping_rounds=CFG[\"early_stopping_rounds\"], verbose=False)],\n    )\n\n    pred = model.predict(X_va, num_iteration=model.best_iteration)\n    oof_s.loc[va_idx] = pred\n    models.append(model)\n\n    fi = pd.DataFrame({\n        \"feature\": FEATS,\n        \"gain\": model.feature_importance(importance_type=\"gain\"),\n        \"split\": model.feature_importance(importance_type=\"split\"),\n    })\n    fi[\"fold\"] = i\n    fi_frames.append(fi)\n    print(f\"Fold {i}: best_iter={model.best_iteration}, val_size={len(va_idx)}\")\n\n# OOF score on the competition (raw) scale\nif CFG[\"use_log_target\"]:\n    y_raw   = np.expm1(y_s.values)\n    oof_raw = np.expm1(oof_s.values).clip(min=0)\n    oof_mse = mean_squared_error(y_raw, oof_raw)\nelse:\n    oof_mse = mean_squared_error(y_s.values, oof_s.values)\n\nprint(f\"OOF MSE: {oof_mse:,.4f}\")\n\n# Aggregate feature importance across folds\nfeat_importance = (\n    pd.concat(fi_frames, ignore_index=True)\n      .groupby(\"feature\")[[\"gain\", \"split\"]]\n      .mean()\n      .sort_values(\"gain\", ascending=False)\n      .reset_index()\n)\ndisplay(feat_importance.head(30))\n\n# Keep 'oof' for Cell 13 compatibility\noof = oof_s.values\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:19:36.381263Z","iopub.execute_input":"2025-08-27T06:19:36.381594Z","iopub.status.idle":"2025-08-27T06:20:15.035814Z","shell.execute_reply.started":"2025-08-27T06:19:36.381568Z","shell.execute_reply":"2025-08-27T06:20:15.034913Z"}},"outputs":[{"name":"stdout","text":"Fold 0: best_iter=1561, val_size=23579\nFold 1: best_iter=230, val_size=23579\nFold 2: best_iter=2130, val_size=23578\nOOF MSE: 377.9613\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                   feature          gain         split\n0                  cnt_buy  4.666818e+08   3134.666667\n1   events_after_first_buy  9.625955e+07   3153.666667\n2                  has_buy  6.237483e+07    136.333333\n3               n_products  2.786652e+07   3738.333333\n4                  cnt_add  2.753348e+07   2804.333333\n5             duration_sec  2.629003e+07  13621.333333\n6                 n_events  2.510476e+07   3641.333333\n7             n_categories  1.876486e+07   3653.666667\n8               start_hour  1.466718e+07   9887.333333\n9            idx_first_buy  1.263281e+07   2042.666667\n10                 cnt_rem  7.778498e+06   2194.333333\n11         last_event_type  7.119482e+06    924.000000\n12    user_prev_n_sessions  6.668376e+06   3252.666667\n13       user_prev_mean_sv  5.900164e+06   8471.333333\n14               start_day  5.725390e+06   6122.333333\n15               start_dow  5.614038e+06   4862.333333\n16      cnt_add_before_buy  5.199344e+06    991.000000\n17           n_transitions  5.019368e+06   1917.666667\n18        first_event_type  4.233558e+06   1060.333333\n19                cnt_view  4.200294e+06   2368.000000\n20           n_event_types  4.169795e+06   1242.000000\n21      cnt_rem_before_buy  2.794949e+06   1007.000000\n22      user_prev_buy_rate  8.268641e+05    807.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>gain</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cnt_buy</td>\n      <td>4.666818e+08</td>\n      <td>3134.666667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>events_after_first_buy</td>\n      <td>9.625955e+07</td>\n      <td>3153.666667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>has_buy</td>\n      <td>6.237483e+07</td>\n      <td>136.333333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n_products</td>\n      <td>2.786652e+07</td>\n      <td>3738.333333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cnt_add</td>\n      <td>2.753348e+07</td>\n      <td>2804.333333</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>duration_sec</td>\n      <td>2.629003e+07</td>\n      <td>13621.333333</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>n_events</td>\n      <td>2.510476e+07</td>\n      <td>3641.333333</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>n_categories</td>\n      <td>1.876486e+07</td>\n      <td>3653.666667</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>start_hour</td>\n      <td>1.466718e+07</td>\n      <td>9887.333333</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>idx_first_buy</td>\n      <td>1.263281e+07</td>\n      <td>2042.666667</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>cnt_rem</td>\n      <td>7.778498e+06</td>\n      <td>2194.333333</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>last_event_type</td>\n      <td>7.119482e+06</td>\n      <td>924.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>user_prev_n_sessions</td>\n      <td>6.668376e+06</td>\n      <td>3252.666667</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>user_prev_mean_sv</td>\n      <td>5.900164e+06</td>\n      <td>8471.333333</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>start_day</td>\n      <td>5.725390e+06</td>\n      <td>6122.333333</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>start_dow</td>\n      <td>5.614038e+06</td>\n      <td>4862.333333</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>cnt_add_before_buy</td>\n      <td>5.199344e+06</td>\n      <td>991.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>n_transitions</td>\n      <td>5.019368e+06</td>\n      <td>1917.666667</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>first_event_type</td>\n      <td>4.233558e+06</td>\n      <td>1060.333333</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>cnt_view</td>\n      <td>4.200294e+06</td>\n      <td>2368.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>n_event_types</td>\n      <td>4.169795e+06</td>\n      <td>1242.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>cnt_rem_before_buy</td>\n      <td>2.794949e+06</td>\n      <td>1007.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>user_prev_buy_rate</td>\n      <td>8.268641e+05</td>\n      <td>807.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"full_train = lgb.Dataset(X, label=y, categorical_feature=categorical_cols, free_raw_data=False)\nfull_model = lgb.train(\n    CFG[\"lgb_params\"],\n    full_train,\n    num_boost_round=int(np.mean([m.best_iteration for m in models]))  # a common heuristic\n)\n\ntest_pred = full_model.predict(X_test)\n\n# Back-transform if using log target\nif CFG[\"use_log_target\"]:\n    test_pred = np.expm1(test_pred).clip(min=0)\n\n# Non-negative clamp (session_value cannot be negative)\ntest_pred = np.clip(test_pred, 0, None)\n\n# Optionally cap extreme outliers if helpful (tune later)\n# cap = np.percentile(test_pred, 99.8)\n# test_pred = np.clip(test_pred, 0, cap)\n\nprint(\"Pred summary:\", pd.Series(test_pred).describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:20:36.873591Z","iopub.execute_input":"2025-08-27T06:20:36.873867Z","iopub.status.idle":"2025-08-27T06:20:48.132875Z","shell.execute_reply.started":"2025-08-27T06:20:36.873848Z","shell.execute_reply":"2025-08-27T06:20:48.131737Z"}},"outputs":[{"name":"stdout","text":"Pred summary: count    30789.000000\nmean        43.576563\nstd         44.333535\nmin          0.000000\n25%         23.441041\n50%         29.609827\n75%         42.562781\nmax        962.948791\ndtype: float64\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Expect submission format: [\"user_session\", \"session_value\"]\nsub_out = sub.copy()\n# The submission's user_session order must match sample_submission (common in Kaggle)\n# Ensure we have predictions per session present in test_sess with the same key\nkey = \"user_session\"\n\n# Map predictions by user_session\npred_map = dict(zip(test_sess[key], test_pred))\n\nsub_out[TARGET_COL] = sub_out[key].map(pred_map).fillna(0.0)  # fill if any missing\nprint(sub_out.head(3))\n\nsave_name = \"submission_baseline_v1.csv\"\nsub_out.to_csv(save_name, index=False)\nprint(\"Saved:\", save_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:21:00.046183Z","iopub.execute_input":"2025-08-27T06:21:00.046519Z","iopub.status.idle":"2025-08-27T06:21:00.174364Z","shell.execute_reply.started":"2025-08-27T06:21:00.046496Z","shell.execute_reply":"2025-08-27T06:21:00.173481Z"}},"outputs":[{"name":"stdout","text":"     user_session  session_value\n0  SESSION_164059     153.601283\n1  SESSION_109583      40.205484\n2  SESSION_171382      40.285850\nSaved: submission_baseline_v1.csv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# OOF vs target (raw scale) distribution check\nif CFG[\"use_log_target\"]:\n    oof_raw = np.expm1(oof).clip(min=0)\n    y_raw   = np.expm1(y)\n    print(\"OOF MSE (raw):\", mean_squared_error(y_raw, oof_raw))\n    display(pd.DataFrame({\"y\": y_raw, \"oof\": oof_raw}).describe())\nelse:\n    print(\"OOF MSE (raw):\", mean_squared_error(y, oof))\n    display(pd.DataFrame({\"y\": y, \"oof\": oof}).describe())\n\n# Per-day stability (helps spot drift)\ntmp = train_sess.assign(y=y, oof=oof)\nday_mse = tmp.groupby(tmp[\"session_start\"].dt.date).apply(lambda d: mean_squared_error(d[\"y\"], d[\"oof\"]))\ndisplay(day_mse.to_frame(\"mse\").reset_index().rename(columns={\"session_start\":\"date\"}).head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T06:21:11.024163Z","iopub.execute_input":"2025-08-27T06:21:11.024473Z","iopub.status.idle":"2025-08-27T06:21:11.113205Z","shell.execute_reply.started":"2025-08-27T06:21:11.024450Z","shell.execute_reply":"2025-08-27T06:21:11.112441Z"}},"outputs":[{"name":"stdout","text":"OOF MSE (raw): 377.9613004443865\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                  y           oof\ncount  70736.000000  70736.000000\nmean      42.198130     41.860379\nstd       47.552369     43.764326\nmin        5.380000    -25.878172\n25%       18.530000     23.209348\n50%       30.750000     28.452416\n75%       46.620000     39.783453\nmax     2328.660000   1101.523628","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y</th>\n      <th>oof</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>70736.000000</td>\n      <td>70736.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>42.198130</td>\n      <td>41.860379</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47.552369</td>\n      <td>43.764326</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.380000</td>\n      <td>-25.878172</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>18.530000</td>\n      <td>23.209348</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>30.750000</td>\n      <td>28.452416</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>46.620000</td>\n      <td>39.783453</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2328.660000</td>\n      <td>1101.523628</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"          date          mse\n0   2025-06-01  1362.032424\n1   2025-06-02   562.847681\n2   2025-06-03   293.033534\n3   2025-06-04   305.120471\n4   2025-06-05   441.413325\n5   2025-06-06   236.473579\n6   2025-06-07   418.980382\n7   2025-06-08   342.092817\n8   2025-06-09   221.698942\n9   2025-06-10   235.670354\n10  2025-06-11   325.393566\n11  2025-06-12   250.201836\n12  2025-06-13   224.136479\n13  2025-06-14   311.627348\n14  2025-06-15   547.108301\n15  2025-06-16   250.245728\n16  2025-06-17   310.848275\n17  2025-06-18   302.627716\n18  2025-06-19   207.390202\n19  2025-06-20   282.659451","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-06-01</td>\n      <td>1362.032424</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-06-02</td>\n      <td>562.847681</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-06-03</td>\n      <td>293.033534</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-06-04</td>\n      <td>305.120471</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-06-05</td>\n      <td>441.413325</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2025-06-06</td>\n      <td>236.473579</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2025-06-07</td>\n      <td>418.980382</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2025-06-08</td>\n      <td>342.092817</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2025-06-09</td>\n      <td>221.698942</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2025-06-10</td>\n      <td>235.670354</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2025-06-11</td>\n      <td>325.393566</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2025-06-12</td>\n      <td>250.201836</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2025-06-13</td>\n      <td>224.136479</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2025-06-14</td>\n      <td>311.627348</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2025-06-15</td>\n      <td>547.108301</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2025-06-16</td>\n      <td>250.245728</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2025-06-17</td>\n      <td>310.848275</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2025-06-18</td>\n      <td>302.627716</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2025-06-19</td>\n      <td>207.390202</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2025-06-20</td>\n      <td>282.659451</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}