{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112016,"databundleVersionId":13341508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# BTK Datathon 2025 — Baseline v2 (sub-1000 push)\n# Cell 1: Setup & imports\n\nimport os, sys, gc, math, json, random, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import Ridge\n\nimport lightgbm as lgb\n\nSEED = 2025\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\nseed_everything()\n\npd.set_option(\"display.max_columns\", 200)\nprint(\"Versions:\",\n      \"pandas\", pd.__version__,\n      \"| numpy\", np.__version__,\n      \"| lightgbm\", lgb.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T07:59:28.917717Z","iopub.execute_input":"2025-08-27T07:59:28.918299Z","iopub.status.idle":"2025-08-27T07:59:28.925196Z","shell.execute_reply.started":"2025-08-27T07:59:28.918273Z","shell.execute_reply":"2025-08-27T07:59:28.923940Z"}},"outputs":[{"name":"stdout","text":"Versions: pandas 2.2.3 | numpy 1.26.4 | lightgbm 4.5.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 2: Config & helper for column inference\n\nCFG = dict(\n    train_path = Path(\"/kaggle/input/datathon-2025/train.csv\"),       # change if needed\n    test_path  = Path(\"/kaggle/input/datathon-2025/test.csv\"),        # change if needed\n    submission_path = Path(\"/kaggle/input/datathon-2025/submission.csv\"),\n    id_candidates = [\"session_id\",\"user_session\",\"id\"],\n    time_candidates = [\"event_time\",\"timestamp\",\"time\",\"ts\",\"date\"],\n    session_candidates = [\"user_session\",\"session_id\",\"session\",\"sid\"],\n    user_candidates = [\"user_id\",\"uid\",\"user\"],\n    event_candidates = [\"event\",\"type\",\"action\",\"event_type\"],\n    product_candidates = [\"product_id\",\"item_id\",\"sku\",\"product\"],\n    category_candidates = [\"category_id\",\"category\",\"cat\"],\n    price_candidates = [\"price\",\"value\",\"amount\",\"revenue\",\"purchase_value\",\"event_value\"],\n    target_candidates = [\"session_value\",\"purchase_amount\",\"order_value\",\"target\",\"label\",\"y\"],\n\n    # modeling\n    n_folds = 5,\n    purge_days = 1,                 # temporal purge between train/val windows\n    use_log_target = True,          # log1p target to tame heavy tails\n    label_clip_p = 99.5,            # clip top tail before log (reduces MSE)\n    has_buy_threshold = 0.0,        # when deriving has_buy from counts\n\n    # LightGBM regression defaults\n    lgb_params = dict(\n        objective = \"rmse\",\n        metric    = \"rmse\",\n        learning_rate = 0.05,\n        num_leaves = 256,\n        min_data_in_leaf = 128,\n        feature_fraction = 0.85,\n        bagging_fraction = 0.8,\n        bagging_freq = 1,\n        lambda_l2 = 2.0,\n        lambda_l1 = 0.1,\n        verbose = -1,\n        seed = SEED\n    ),\n\n    # seed bagging — train multiple seeds (averaged for test)\n    bagging_seeds = [2025, 2026, 2027],\n)\n\ndef first_existing(df, cand):\n    for c in cand:\n        if c in df.columns:\n            return c\n    return None\n\ndef infer_columns(train_df):\n    \"\"\"\n    Infer key columns; fallback gracefully if something is missing.\n    \"\"\"\n    cols = dict()\n    cols[\"ID_COL\"]       = first_existing(train_df, CFG[\"id_candidates\"])\n    cols[\"TIME_COL\"]     = first_existing(train_df, CFG[\"time_candidates\"])\n    cols[\"SESSION_COL\"]  = first_existing(train_df, CFG[\"session_candidates\"]) or cols[\"ID_COL\"]\n    cols[\"USER_COL\"]     = first_existing(train_df, CFG[\"user_candidates\"])\n    cols[\"EVENT_COL\"]    = first_existing(train_df, CFG[\"event_candidates\"])\n    cols[\"PRODUCT_COL\"]  = first_existing(train_df, CFG[\"product_candidates\"])\n    cols[\"CATEGORY_COL\"] = first_existing(train_df, CFG[\"category_candidates\"])\n    cols[\"PRICE_COL\"]    = first_existing(train_df, CFG[\"price_candidates\"])\n    cols[\"TARGET_COL\"]   = first_existing(train_df, CFG[\"target_candidates\"])\n    return cols\n\ndef ensure_datetime(df, col):\n    if col and col in df.columns:\n        if not np.issubdtype(df[col].dtype, np.datetime64):\n            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T07:59:32.327962Z","iopub.execute_input":"2025-08-27T07:59:32.328640Z","iopub.status.idle":"2025-08-27T07:59:32.339049Z","shell.execute_reply.started":"2025-08-27T07:59:32.328613Z","shell.execute_reply":"2025-08-27T07:59:32.338194Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Cell 3: Load CSVs\ntrain = pd.read_csv(CFG[\"train_path\"])\ntest  = pd.read_csv(CFG[\"test_path\"])\n\nprint(\"train:\", train.shape, \"test:\", test.shape)\nprint(\"train cols:\", list(train.columns)[:40])\n\nCOLS = infer_columns(train)\nglobals().update(COLS)  # expose names like ID_COL, TIME_COL, etc.\n\nprint(\"Inferred columns:\", COLS)\n\n# If TIME_COL exists, convert to datetime\nfor df in (train, test):\n    ensure_datetime(df, TIME_COL)\n\n# Basic sanity\nassert SESSION_COL in train.columns, f\"Couldn't infer session id (candidates: {CFG['session_candidates']})\"\nif TIME_COL: \n    assert TIME_COL in train.columns, f\"Couldn't infer time column (candidates: {CFG['time_candidates']})\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T07:59:51.417434Z","iopub.execute_input":"2025-08-27T07:59:51.417739Z","iopub.status.idle":"2025-08-27T07:59:52.132967Z","shell.execute_reply.started":"2025-08-27T07:59:51.417716Z","shell.execute_reply":"2025-08-27T07:59:52.132073Z"}},"outputs":[{"name":"stdout","text":"train: (141219, 7) test: (62951, 6)\ntrain cols: ['event_time', 'event_type', 'product_id', 'category_id', 'user_id', 'user_session', 'session_value']\nInferred columns: {'ID_COL': 'user_session', 'TIME_COL': 'event_time', 'SESSION_COL': 'user_session', 'USER_COL': 'user_id', 'EVENT_COL': 'event_type', 'PRODUCT_COL': 'product_id', 'CATEGORY_COL': 'category_id', 'PRICE_COL': None, 'TARGET_COL': 'session_value'}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 4: Build session-level labels and basic aggregates\n\ndef guess_session_value(df):\n    # Priority 1: if TARGET_COL exists at row-level, aggregate by session sum\n    if TARGET_COL and TARGET_COL in df.columns:\n        sv = df.groupby(SESSION_COL)[TARGET_COL].sum()\n        return sv.rename(\"session_value\")\n\n    # Priority 2: sum of price/value on purchase/buy events if EVENT_COL present\n    ev_col = EVENT_COL\n    price_col = PRICE_COL\n    if ev_col in df.columns and price_col in df.columns:\n        # mark purchases\n        purchase_mask = df[ev_col].astype(str).str.lower().str.contains(\"buy|purchase|order|checkout\")\n        sv = df.loc[purchase_mask].groupby(SESSION_COL)[price_col].sum()\n        return sv.rename(\"session_value\")\n\n    # Priority 3: sum of any 'price/value' column across session\n    if price_col in df.columns:\n        sv = df.groupby(SESSION_COL)[price_col].sum()\n        return sv.rename(\"session_value\")\n\n    # If nothing matched, fallback: count events (not ideal, but consistent)\n    sv = df.groupby(SESSION_COL).size()\n    return sv.rename(\"session_value\")\n\n# session_value for train\nsess_val = guess_session_value(train)\nprint(\"Built session_value for train. Sessions with value:\", (sess_val>0).sum(), \"/\", len(sess_val))\n\n# For test, label unknown: keep placeholder\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T07:58:23.924114Z","iopub.execute_input":"2025-08-27T07:58:23.924775Z","iopub.status.idle":"2025-08-27T07:58:24.027120Z","shell.execute_reply.started":"2025-08-27T07:58:23.924746Z","shell.execute_reply":"2025-08-27T07:58:24.026186Z"}},"outputs":[{"name":"stdout","text":"Built session_value for train. Sessions with value: 70736 / 70736\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 5: Create session-level frames (train_sess, test_sess) with core aggregations — DUP-SAFE\n\ndef build_session_frame(df, is_train=True):\n    g = df.groupby(SESSION_COL, as_index=False)\n\n    # session start/end if time exists\n    if TIME_COL and TIME_COL in df.columns:\n        agg = g.agg(\n            session_start=(TIME_COL, \"min\"),\n            session_end=(TIME_COL, \"max\"),\n            n_events=(TIME_COL, \"count\")\n        )\n        agg[\"session_dur_s\"] = (agg[\"session_end\"] - agg[\"session_start\"]).dt.total_seconds().fillna(0)\n    else:\n        agg = g.size().rename(columns={\"size\":\"n_events\"}).reset_index()\n        agg[\"session_start\"] = pd.NaT\n        agg[\"session_end\"]   = pd.NaT\n        agg[\"session_dur_s\"] = 0.0\n\n    # unique counts\n    if PRODUCT_COL and PRODUCT_COL in df.columns:\n        uu = df.groupby(SESSION_COL)[PRODUCT_COL].nunique().rename(\"n_product_unique\")\n        agg = agg.merge(uu, on=SESSION_COL, how=\"left\")\n    if CATEGORY_COL and CATEGORY_COL in df.columns:\n        uu = df.groupby(SESSION_COL)[CATEGORY_COL].nunique().rename(\"n_category_unique\")\n        agg = agg.merge(uu, on=SESSION_COL, how=\"left\")\n\n    # event-type counts  -> collapse duplicate semantic columns (e.g., multiple \"view\" variants)\n    if EVENT_COL and EVENT_COL in df.columns:\n        ev = df[[SESSION_COL, EVENT_COL]].copy()\n        ev[EVENT_COL] = ev[EVENT_COL].astype(str).str.lower()\n        piv = (ev\n               .groupby([SESSION_COL, EVENT_COL]).size()\n               .unstack(fill_value=0))\n\n        # friendly names: map raw tokens to standard columns\n        rename_map = {}\n        for c in piv.columns:\n            if \"view\" in c: rename_map[c] = \"cnt_view\"\n            elif \"add\" in c or \"cart\" in c: rename_map[c] = \"cnt_add\"\n            elif \"buy\" in c or \"purchase\" in c or \"order\" in c or \"checkout\" in c: rename_map[c] = \"cnt_buy\"\n\n        piv2 = piv.rename(columns=rename_map)\n\n        # *** CRITICAL: collapse duplicate columns by summing (e.g., multiple tokens -> 'cnt_view') ***\n        piv2 = piv2.T.groupby(level=0).sum().T  # sums duplicates with same column name\n\n        # now safe to merge\n        agg = agg.merge(piv2.reset_index(), on=SESSION_COL, how=\"left\")\n\n    # ---- last N event tokens per session (no duplicate SESSION_COL on reset) ----\n    def lastN_events(local_df, N=3):\n        if EVENT_COL not in local_df.columns:\n            return pd.DataFrame({SESSION_COL: local_df[SESSION_COL].unique()})\n        srt = local_df.copy()\n        srt[EVENT_COL] = srt[EVENT_COL].astype(str)\n        if TIME_COL and TIME_COL in srt.columns:\n            srt = srt.sort_values([SESSION_COL, TIME_COL])\n        else:\n            srt = srt.sort_values(SESSION_COL)\n\n        grp = srt.groupby(SESSION_COL)\n        for n in range(1, N+1):\n            srt[f\"last_evt_{n}\"] = grp[EVENT_COL].shift(n)\n\n        cols = [f\"last_evt_{n}\" for n in range(1, N+1)]\n        res = srt.groupby(SESSION_COL, as_index=False)[cols].last()\n        return res\n\n    last3 = lastN_events(df, N=3)\n    agg = agg.merge(last3, on=SESSION_COL, how=\"left\")\n\n    # main product/category heuristic: last product/category in session\n    if PRODUCT_COL and PRODUCT_COL in df.columns:\n        srt = df.sort_values([SESSION_COL, TIME_COL]) if TIME_COL in df.columns else df.sort_values(SESSION_COL)\n        main_prod = srt.groupby(SESSION_COL)[PRODUCT_COL].last().rename(\"main_product\")\n        agg = agg.merge(main_prod, on=SESSION_COL, how=\"left\")\n    if CATEGORY_COL and CATEGORY_COL in df.columns:\n        srt = df.sort_values([SESSION_COL, TIME_COL]) if TIME_COL in df.columns else df.sort_values(SESSION_COL)\n        main_cat = srt.groupby(SESSION_COL)[CATEGORY_COL].last().rename(\"main_category\")\n        agg = agg.merge(main_cat, on=SESSION_COL, how=\"left\")\n\n    # ---------- robust ratios even if duplicate labels sneak in ----------\n    def get_series_sum(df_local, colname):\n        \"\"\"Return a 1D float Series for the (possibly duplicate-named) column by summing duplicates.\"\"\"\n        if colname not in df_local.columns:\n            return None\n        sub = df_local.loc[:, df_local.columns == colname]\n        if isinstance(sub, pd.Series):  # single match\n            return sub.astype(float)\n        # DataFrame with one or more columns (duplicates) -> sum row-wise\n        return sub.astype(float).sum(axis=1)\n\n    for a, b, name in [(\"cnt_view\",\"cnt_add\",\"view_to_add\"),\n                       (\"cnt_add\",\"cnt_buy\",\"add_to_buy\")]:\n        num = get_series_sum(agg, a)\n        den = get_series_sum(agg, b)\n        if num is not None and den is not None:\n            den_safe = den.replace(0, np.nan)\n            ratio = (num / den_safe).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n            # assign by values to avoid alignment complaints\n            agg[name] = ratio.values\n\n    # has_buy\n    if \"cnt_buy\" in agg.columns:\n        # ensure it's a 1D series even if duplicates existed\n        cnt_buy = get_series_sum(agg, \"cnt_buy\")\n        agg[\"has_buy\"] = (cnt_buy.fillna(0) > CFG[\"has_buy_threshold\"]).astype(int)\n\n    # attach label for train\n    if is_train:\n        lbl = guess_session_value(df)\n        lbl = lbl.reindex(agg[SESSION_COL]).fillna(0.0).reset_index(drop=False)\n        agg = agg.merge(lbl, on=SESSION_COL, how=\"left\")\n\n    return agg\n\ntrain_sess = build_session_frame(train, is_train=True)\ntest_sess  = build_session_frame(test,  is_train=False)\n\nprint(\"train_sess:\", train_sess.shape, \"test_sess:\", test_sess.shape)\ntrain_sess.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:00:19.021079Z","iopub.execute_input":"2025-08-27T08:00:19.021424Z","iopub.status.idle":"2025-08-27T08:00:21.449645Z","shell.execute_reply.started":"2025-08-27T08:00:19.021403Z","shell.execute_reply":"2025-08-27T08:00:21.448864Z"}},"outputs":[{"name":"stdout","text":"train_sess: (70736, 19) test_sess: (30789, 18)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"     user_session             session_start               session_end  \\\n0  SESSION_000000 2025-06-02 12:14:45+00:00 2025-06-20 17:17:32+00:00   \n1  SESSION_000001 2025-06-02 03:55:52+00:00 2025-06-02 06:06:10+00:00   \n2  SESSION_000004 2025-06-04 10:27:59+00:00 2025-06-04 10:27:59+00:00   \n\n   n_events  session_dur_s  n_product_unique  n_category_unique  cnt_add  \\\n0        28      1573367.0                24                 20       28   \n1         6         7818.0                 5                  5        4   \n2         1            0.0                 1                  1        0   \n\n   cnt_buy  cnt_view last_evt_1   last_evt_2 last_evt_3 main_product  \\\n0        0         0   ADD_CART     ADD_CART   ADD_CART  PROD_004203   \n1        1         1        BUY  REMOVE_CART   ADD_CART  PROD_021185   \n2        0         1       None         None       None  PROD_013881   \n\n  main_category  view_to_add  add_to_buy  has_buy  session_value  \n0     CAT_00280         0.00         0.0        0        9962.40  \n1     CAT_00280         0.25         4.0        1         579.60  \n2     CAT_00267         0.00         0.0        0          30.92  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_session</th>\n      <th>session_start</th>\n      <th>session_end</th>\n      <th>n_events</th>\n      <th>session_dur_s</th>\n      <th>n_product_unique</th>\n      <th>n_category_unique</th>\n      <th>cnt_add</th>\n      <th>cnt_buy</th>\n      <th>cnt_view</th>\n      <th>last_evt_1</th>\n      <th>last_evt_2</th>\n      <th>last_evt_3</th>\n      <th>main_product</th>\n      <th>main_category</th>\n      <th>view_to_add</th>\n      <th>add_to_buy</th>\n      <th>has_buy</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SESSION_000000</td>\n      <td>2025-06-02 12:14:45+00:00</td>\n      <td>2025-06-20 17:17:32+00:00</td>\n      <td>28</td>\n      <td>1573367.0</td>\n      <td>24</td>\n      <td>20</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ADD_CART</td>\n      <td>ADD_CART</td>\n      <td>ADD_CART</td>\n      <td>PROD_004203</td>\n      <td>CAT_00280</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>9962.40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SESSION_000001</td>\n      <td>2025-06-02 03:55:52+00:00</td>\n      <td>2025-06-02 06:06:10+00:00</td>\n      <td>6</td>\n      <td>7818.0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>BUY</td>\n      <td>REMOVE_CART</td>\n      <td>ADD_CART</td>\n      <td>PROD_021185</td>\n      <td>CAT_00280</td>\n      <td>0.25</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>579.60</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SESSION_000004</td>\n      <td>2025-06-04 10:27:59+00:00</td>\n      <td>2025-06-04 10:27:59+00:00</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>PROD_013881</td>\n      <td>CAT_00267</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>30.92</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Cell 6: Expanding/shifted histories for main_product / main_category\n\ndef add_entity_history(train_sess, test_sess, key_col, target_col=\"session_value\"):\n    if key_col not in train_sess.columns: \n        return train_sess, test_sess\n\n    cols_needed = [SESSION_COL, key_col, \"session_start\", target_col]\n    for c in [\"session_start\"]:\n        if c not in train_sess.columns:\n            train_sess[c] = pd.to_datetime(\"1970-01-01\")\n        if c not in test_sess.columns:\n            test_sess[c] = pd.to_datetime(\"1970-01-01\")\n\n    comb = pd.concat(\n        [\n            train_sess[cols_needed].assign(_is_train=1),\n            test_sess[[SESSION_COL, key_col, \"session_start\"]].assign(**{target_col: np.nan, \"_is_train\":0})\n        ],\n        axis=0, ignore_index=True\n    ).sort_values([\"session_start\"]).reset_index(drop=True)\n\n    comb[key_col] = comb[key_col].astype(str)\n\n    g = comb.groupby(key_col, sort=False)\n    # expanding mean/count of past session_value (shifted via cumcount)\n    sv = comb[target_col].fillna(0.0).values\n    cum_sum = g[target_col].transform(lambda x: x.fillna(0.0).cumsum())\n    cum_cnt = g[target_col].transform(lambda x: np.arange(len(x)))\n    prev_sum = cum_sum - sv\n    prev_cnt = cum_cnt\n    gmean = float(train_sess[target_col].mean())\n\n    comb[f\"{key_col}_prev_mean_sv\"] = np.where(prev_cnt>0, prev_sum/np.maximum(prev_cnt,1), np.nan)\n    comb[f\"{key_col}_prev_n\"] = prev_cnt.astype(int)\n\n    comb[f\"{key_col}_prev_mean_sv\"] = comb[f\"{key_col}_prev_mean_sv\"].fillna(gmean)\n\n    tr = comb[comb[\"_is_train\"]==1][[SESSION_COL, f\"{key_col}_prev_mean_sv\", f\"{key_col}_prev_n\"]]\n    te = comb[comb[\"_is_train\"]==0][[SESSION_COL, f\"{key_col}_prev_mean_sv\", f\"{key_col}_prev_n\"]]\n\n    train_sess = train_sess.merge(tr, on=SESSION_COL, how=\"left\")\n    test_sess  = test_sess.merge(te, on=SESSION_COL, how=\"left\")\n\n    # fill gaps\n    for c in [f\"{key_col}_prev_mean_sv\", f\"{key_col}_prev_n\"]:\n        if c in train_sess.columns:\n            train_sess[c] = train_sess[c].fillna(gmean if \"mean\" in c else 0)\n        if c in test_sess.columns:\n            test_sess[c] = test_sess[c].fillna(gmean if \"mean\" in c else 0)\n\n    return train_sess, test_sess\n\nfor key in [\"main_product\",\"main_category\"]:\n    if key in train_sess.columns:\n        train_sess, test_sess = add_entity_history(train_sess, test_sess, key_col=key, target_col=\"session_value\")\n\nprint(\"Added entity histories.\")\ntrain_sess.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:00:42.562818Z","iopub.execute_input":"2025-08-27T08:00:42.563171Z","iopub.status.idle":"2025-08-27T08:00:49.424565Z","shell.execute_reply.started":"2025-08-27T08:00:42.563127Z","shell.execute_reply":"2025-08-27T08:00:49.423757Z"}},"outputs":[{"name":"stdout","text":"Added entity histories.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"     user_session             session_start               session_end  \\\n0  SESSION_000000 2025-06-02 12:14:45+00:00 2025-06-20 17:17:32+00:00   \n1  SESSION_000001 2025-06-02 03:55:52+00:00 2025-06-02 06:06:10+00:00   \n2  SESSION_000004 2025-06-04 10:27:59+00:00 2025-06-04 10:27:59+00:00   \n\n   n_events  session_dur_s  n_product_unique  n_category_unique  cnt_add  \\\n0        28      1573367.0                24                 20       28   \n1         6         7818.0                 5                  5        4   \n2         1            0.0                 1                  1        0   \n\n   cnt_buy  cnt_view last_evt_1   last_evt_2 last_evt_3 main_product  \\\n0        0         0   ADD_CART     ADD_CART   ADD_CART  PROD_004203   \n1        1         1        BUY  REMOVE_CART   ADD_CART  PROD_021185   \n2        0         1       None         None       None  PROD_013881   \n\n  main_category  view_to_add  add_to_buy  has_buy  session_value  \\\n0     CAT_00280         0.00         0.0        0        9962.40   \n1     CAT_00280         0.25         4.0        1         579.60   \n2     CAT_00267         0.00         0.0        0          30.92   \n\n   main_product_prev_mean_sv  main_product_prev_n  main_category_prev_mean_sv  \\\n0                  42.360000                    1                   47.808713   \n1                 131.550000                    1                   58.768310   \n2                 150.427581                    0                   35.535238   \n\n   main_category_prev_n  \n0                   303  \n1                    71  \n2                    21  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_session</th>\n      <th>session_start</th>\n      <th>session_end</th>\n      <th>n_events</th>\n      <th>session_dur_s</th>\n      <th>n_product_unique</th>\n      <th>n_category_unique</th>\n      <th>cnt_add</th>\n      <th>cnt_buy</th>\n      <th>cnt_view</th>\n      <th>last_evt_1</th>\n      <th>last_evt_2</th>\n      <th>last_evt_3</th>\n      <th>main_product</th>\n      <th>main_category</th>\n      <th>view_to_add</th>\n      <th>add_to_buy</th>\n      <th>has_buy</th>\n      <th>session_value</th>\n      <th>main_product_prev_mean_sv</th>\n      <th>main_product_prev_n</th>\n      <th>main_category_prev_mean_sv</th>\n      <th>main_category_prev_n</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SESSION_000000</td>\n      <td>2025-06-02 12:14:45+00:00</td>\n      <td>2025-06-20 17:17:32+00:00</td>\n      <td>28</td>\n      <td>1573367.0</td>\n      <td>24</td>\n      <td>20</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ADD_CART</td>\n      <td>ADD_CART</td>\n      <td>ADD_CART</td>\n      <td>PROD_004203</td>\n      <td>CAT_00280</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>9962.40</td>\n      <td>42.360000</td>\n      <td>1</td>\n      <td>47.808713</td>\n      <td>303</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SESSION_000001</td>\n      <td>2025-06-02 03:55:52+00:00</td>\n      <td>2025-06-02 06:06:10+00:00</td>\n      <td>6</td>\n      <td>7818.0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>BUY</td>\n      <td>REMOVE_CART</td>\n      <td>ADD_CART</td>\n      <td>PROD_021185</td>\n      <td>CAT_00280</td>\n      <td>0.25</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>579.60</td>\n      <td>131.550000</td>\n      <td>1</td>\n      <td>58.768310</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SESSION_000004</td>\n      <td>2025-06-04 10:27:59+00:00</td>\n      <td>2025-06-04 10:27:59+00:00</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>PROD_013881</td>\n      <td>CAT_00267</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>30.92</td>\n      <td>150.427581</td>\n      <td>0</td>\n      <td>35.535238</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Cell 7: Build feature lists\n\ncategorical_cols = []\nfor c in [f\"last_evt_{i}\" for i in range(1,4)] + [\"main_product\",\"main_category\"]:\n    if c in train_sess.columns:\n        if train_sess[c].dtype == \"O\" or str(train_sess[c].dtype).startswith(\"category\"):\n            categorical_cols.append(c)\n\nnumeric_cols = []\nfor c in [\n    \"n_events\",\"session_dur_s\",\"n_product_unique\",\"n_category_unique\",\n    \"cnt_view\",\"cnt_add\",\"cnt_buy\",\"view_to_add\",\"add_to_buy\",\n    \"main_product_prev_mean_sv\",\"main_product_prev_n\",\n    \"main_category_prev_mean_sv\",\"main_category_prev_n\"\n]:\n    if c in train_sess.columns:\n        numeric_cols.append(c)\n\n# time features from session_start\nif \"session_start\" in train_sess.columns and pd.api.types.is_datetime64_any_dtype(train_sess[\"session_start\"]):\n    for df in (train_sess, test_sess):\n        df[\"ss_hour\"] = df[\"session_start\"].dt.hour.fillna(0).astype(int)\n        df[\"ss_dow\"]  = df[\"session_start\"].dt.dayofweek.fillna(0).astype(int)\n    categorical_cols += [\"ss_dow\",\"ss_hour\"]\n\nFEATS = categorical_cols + numeric_cols\nprint(\"categorical_cols:\", categorical_cols)\nprint(\"numeric_cols:\", numeric_cols)\nprint(\"Total features:\", len(FEATS))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:00:58.389095Z","iopub.execute_input":"2025-08-27T08:00:58.389450Z","iopub.status.idle":"2025-08-27T08:00:58.408446Z","shell.execute_reply.started":"2025-08-27T08:00:58.389429Z","shell.execute_reply":"2025-08-27T08:00:58.407441Z"}},"outputs":[{"name":"stdout","text":"categorical_cols: ['last_evt_1', 'last_evt_2', 'last_evt_3', 'main_product', 'main_category', 'ss_dow', 'ss_hour']\nnumeric_cols: ['n_events', 'session_dur_s', 'n_product_unique', 'n_category_unique', 'cnt_view', 'cnt_add', 'cnt_buy', 'view_to_add', 'add_to_buy', 'main_product_prev_mean_sv', 'main_product_prev_n', 'main_category_prev_mean_sv', 'main_category_prev_n']\nTotal features: 20\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Cell 8: Target transformation\nassert \"session_value\" in train_sess.columns, \"No session_value in train_sess.\"\n\ny_raw = train_sess[\"session_value\"].values.astype(float)\nclip_val = np.percentile(y_raw, CFG[\"label_clip_p\"])\ny_clipped = np.clip(y_raw, 0, clip_val)\n\nif CFG[\"use_log_target\"]:\n    y = np.log1p(y_clipped)\nelse:\n    y = y_clipped\n\ntrain_sess[\"target\"] = y\nprint(\"Label clip @p=\", CFG[\"label_clip_p\"], \"=>\", clip_val, \"| log-target:\", CFG[\"use_log_target\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:01:05.876817Z","iopub.execute_input":"2025-08-27T08:01:05.877714Z","iopub.status.idle":"2025-08-27T08:01:05.895277Z","shell.execute_reply.started":"2025-08-27T08:01:05.877679Z","shell.execute_reply":"2025-08-27T08:01:05.894584Z"}},"outputs":[{"name":"stdout","text":"Label clip @p= 99.5 => 3119.3924999999977 | log-target: True\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Cell 9: Label-encode categoricals to keep LightGBM happy\nencoders = {}\nfor c in categorical_cols:\n    le = LabelEncoder()\n    all_vals = pd.concat([train_sess[c].astype(str), test_sess[c].astype(str)], axis=0).fillna(\"NA\")\n    le.fit(all_vals)\n    train_sess[c] = le.transform(train_sess[c].astype(str).fillna(\"NA\"))\n    test_sess[c]  = le.transform(test_sess[c].astype(str).fillna(\"NA\"))\n    encoders[c] = le\n\nprint(\"Encoded categoricals:\", len(encoders))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:01:11.473273Z","iopub.execute_input":"2025-08-27T08:01:11.474381Z","iopub.status.idle":"2025-08-27T08:01:11.975707Z","shell.execute_reply.started":"2025-08-27T08:01:11.474350Z","shell.execute_reply":"2025-08-27T08:01:11.974830Z"}},"outputs":[{"name":"stdout","text":"Encoded categoricals: 7\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 10: Optional auxiliary classifier -> add p_has_buy as numeric feature\n\ndef add_has_buy_proba(train_sess, test_sess):\n    # derive label if missing\n    if \"has_buy\" not in train_sess.columns:\n        if \"cnt_buy\" in train_sess.columns:\n            train_sess[\"has_buy\"] = (train_sess[\"cnt_buy\"] > CFG[\"has_buy_threshold\"]).astype(int)\n        else:\n            train_sess[\"has_buy\"] = (train_sess[\"session_value\"] > 0).astype(int)\n\n    clf_feats = [f for f in FEATS if f != \"has_buy\" and f in train_sess.columns]\n    cat_in_clf = [c for c in categorical_cols if c in clf_feats]\n\n    d_clf = lgb.Dataset(train_sess[clf_feats], label=train_sess[\"has_buy\"],\n                        categorical_feature=cat_in_clf, free_raw_data=False)\n    clf_params = dict(\n        objective=\"binary\", metric=\"auc\", learning_rate=0.05,\n        num_leaves=64, feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=1,\n        min_data_in_leaf=64, seed=SEED, verbose=-1\n    )\n    clf = lgb.train(clf_params, d_clf, num_boost_round=600)\n\n    train_sess[\"p_has_buy\"] = clf.predict(train_sess[clf_feats])\n    test_sess[\"p_has_buy\"]  = clf.predict(test_sess[clf_feats])\n\n    if \"p_has_buy\" not in FEATS:\n        FEATS.append(\"p_has_buy\")\n    if \"p_has_buy\" not in numeric_cols:\n        numeric_cols.append(\"p_has_buy\")\n    return train_sess, test_sess\n\ntrain_sess, test_sess = add_has_buy_proba(train_sess, test_sess)\nprint(\"Added p_has_buy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:01:17.067639Z","iopub.execute_input":"2025-08-27T08:01:17.068241Z","iopub.status.idle":"2025-08-27T08:01:20.740527Z","shell.execute_reply.started":"2025-08-27T08:01:17.068214Z","shell.execute_reply":"2025-08-27T08:01:20.737923Z"}},"outputs":[{"name":"stdout","text":"Added p_has_buy\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 11: Time-sorted CV with purge\n\ndef make_time_sorted_folds(df, n_folds=5, purge_days=1):\n    if \"session_start\" not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[\"session_start\"]):\n        # fallback to simple KFold if no time\n        kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n        idx = np.arange(len(df))\n        folds = []\n        for tr, va in kf.split(idx):\n            folds.append((tr, va))\n        return folds\n\n    srt_idx = np.argsort(df[\"session_start\"].values)\n    fold_sizes = np.full(n_folds, len(df)//n_folds, dtype=int)\n    fold_sizes[:len(df)%n_folds] += 1\n\n    current = 0\n    fold_indices = []\n    for fs in fold_sizes:\n        fold_indices.append(srt_idx[current:current+fs])\n        current += fs\n\n    folds = []\n    for k in range(n_folds):\n        va_idx = fold_indices[k]\n        va_min_time = df[\"session_start\"].iloc[va_idx].min()\n        va_max_time = df[\"session_start\"].iloc[va_idx].max()\n        purge_start = va_min_time - timedelta(days=purge_days)\n        purge_end   = va_max_time + timedelta(days=purge_days)\n\n        tr_mask = ~((df[\"session_start\"]>=purge_start) & (df[\"session_start\"]<=purge_end))\n        tr_idx = np.where(tr_mask)[0]\n        folds.append((tr_idx, va_idx))\n    return folds\n\nfolds = make_time_sorted_folds(train_sess, n_folds=CFG[\"n_folds\"], purge_days=CFG[\"purge_days\"])\nprint(\"Built\", len(folds), \"folds.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:01:26.001415Z","iopub.execute_input":"2025-08-27T08:01:26.002236Z","iopub.status.idle":"2025-08-27T08:01:26.030341Z","shell.execute_reply.started":"2025-08-27T08:01:26.002209Z","shell.execute_reply":"2025-08-27T08:01:26.029411Z"}},"outputs":[{"name":"stdout","text":"Built 5 folds.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Cell 12: LGBM CV — OOF training (uses callbacks + numpy clipping)\n\ndef train_lgb_cv(train_df, features, target, folds, params, seeds=[SEED]):\n    oof = np.zeros(len(train_df), dtype=float)\n    models_by_seed = []\n    fold_metrics = []\n\n    cat_feats = [c for c in features if c in categorical_cols]\n\n    for sd in seeds:\n        params_seeded = params.copy()\n        params_seeded[\"seed\"] = sd\n        models = []\n\n        for fold_i, (tr_idx, va_idx) in enumerate(folds):\n            X_tr = train_df.iloc[tr_idx][features]\n            y_tr = target[tr_idx]\n            X_va = train_df.iloc[va_idx][features]\n            y_va = target[va_idx]\n\n            d_tr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_feats, free_raw_data=False)\n            d_va = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_feats, free_raw_data=False)\n\n            callbacks = [\n                lgb.early_stopping(stopping_rounds=200),\n                lgb.log_evaluation(period=250),\n            ]\n\n            model = lgb.train(\n                params_seeded,\n                d_tr,\n                valid_sets=[d_tr, d_va],\n                valid_names=[\"train\",\"valid\"],\n                num_boost_round=5000,\n                callbacks=callbacks\n            )\n\n            best_iter = getattr(model, \"best_iteration\", None)\n            if best_iter is None or best_iter <= 0:\n                best_iter = model.current_iteration()\n\n            pred_va = model.predict(X_va, num_iteration=best_iter)\n            oof[va_idx] += pred_va / len(seeds)\n            models.append(model)\n\n            if CFG[\"use_log_target\"]:\n                y_va_raw    = np.clip(np.expm1(y_va),   0, None)\n                pred_va_raw = np.clip(np.expm1(pred_va),0, None)\n            else:\n                y_va_raw    = y_va\n                pred_va_raw = pred_va\n\n            mse = mean_squared_error(y_va_raw, pred_va_raw)\n            fold_metrics.append((sd, fold_i, mse))\n\n            del X_tr, y_tr, X_va, y_va, d_tr, d_va\n            gc.collect()\n\n        models_by_seed.append(models)\n\n    # OOF raw metric\n    if CFG[\"use_log_target\"]:\n        y_raw   = np.clip(np.expm1(target), 0, None)\n        oof_raw = np.clip(np.expm1(oof),    0, None)\n    else:\n        y_raw   = target\n        oof_raw = oof\n\n    oof_mse = mean_squared_error(y_raw, oof_raw)\n    print(f\"OOF MSE (raw scale): {oof_mse:.4f}\")\n    return oof, models_by_seed, oof_mse, pd.DataFrame(fold_metrics, columns=[\"seed\",\"fold\",\"mse\"])\n\noof, models_by_seed, oof_mse, fold_df = train_lgb_cv(\n    train_sess, FEATS, train_sess[\"target\"].values, folds, CFG[\"lgb_params\"], seeds=CFG[\"bagging_seeds\"]\n)\nfold_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:12:41.241411Z","iopub.execute_input":"2025-08-27T08:12:41.241704Z","iopub.status.idle":"2025-08-27T08:13:59.430230Z","shell.execute_reply.started":"2025-08-27T08:12:41.241684Z","shell.execute_reply":"2025-08-27T08:13:59.429324Z"}},"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.384601\tvalid's rmse: 0.493121\nEarly stopping, best iteration is:\n[70]\ttrain's rmse: 0.463712\tvalid's rmse: 0.479188\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.3745\tvalid's rmse: 0.525698\nEarly stopping, best iteration is:\n[68]\ttrain's rmse: 0.453338\tvalid's rmse: 0.5143\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.375631\tvalid's rmse: 0.532323\nEarly stopping, best iteration is:\n[78]\ttrain's rmse: 0.445349\tvalid's rmse: 0.522025\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.379419\tvalid's rmse: 0.513422\nEarly stopping, best iteration is:\n[70]\ttrain's rmse: 0.454122\tvalid's rmse: 0.502207\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.374637\tvalid's rmse: 0.539418\nEarly stopping, best iteration is:\n[85]\ttrain's rmse: 0.439081\tvalid's rmse: 0.530868\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.385528\tvalid's rmse: 0.493061\nEarly stopping, best iteration is:\n[66]\ttrain's rmse: 0.467555\tvalid's rmse: 0.479059\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.375003\tvalid's rmse: 0.525392\nEarly stopping, best iteration is:\n[64]\ttrain's rmse: 0.457449\tvalid's rmse: 0.514967\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.376087\tvalid's rmse: 0.53221\nEarly stopping, best iteration is:\n[75]\ttrain's rmse: 0.447731\tvalid's rmse: 0.521566\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.379973\tvalid's rmse: 0.513466\nEarly stopping, best iteration is:\n[74]\ttrain's rmse: 0.451945\tvalid's rmse: 0.501879\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.375094\tvalid's rmse: 0.538073\nEarly stopping, best iteration is:\n[71]\ttrain's rmse: 0.448747\tvalid's rmse: 0.530043\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.383803\tvalid's rmse: 0.493784\nEarly stopping, best iteration is:\n[71]\ttrain's rmse: 0.46249\tvalid's rmse: 0.480451\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.373611\tvalid's rmse: 0.527196\nEarly stopping, best iteration is:\n[67]\ttrain's rmse: 0.453506\tvalid's rmse: 0.514841\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.374817\tvalid's rmse: 0.532841\nEarly stopping, best iteration is:\n[73]\ttrain's rmse: 0.447954\tvalid's rmse: 0.522273\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.378689\tvalid's rmse: 0.513652\nEarly stopping, best iteration is:\n[72]\ttrain's rmse: 0.452516\tvalid's rmse: 0.501678\nTraining until validation scores don't improve for 200 rounds\n[250]\ttrain's rmse: 0.372923\tvalid's rmse: 0.538963\nEarly stopping, best iteration is:\n[75]\ttrain's rmse: 0.444265\tvalid's rmse: 0.530667\nOOF MSE (raw scale): 9300.0521\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"    seed  fold           mse\n0   2025     0  10581.376414\n1   2025     1   9168.786527\n2   2025     2   9297.541884\n3   2025     3   9885.118867\n4   2025     4   6386.312916\n5   2026     0  11813.170409\n6   2026     1  10144.548109\n7   2026     2   9713.114064\n8   2026     3   9216.950303\n9   2026     4   7629.337326\n10  2027     0  10380.713317\n11  2027     1   9184.488564\n12  2027     2  10268.534303\n13  2027     3   9297.894492\n14  2027     4   7184.857510","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>fold</th>\n      <th>mse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025</td>\n      <td>0</td>\n      <td>10581.376414</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025</td>\n      <td>1</td>\n      <td>9168.786527</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025</td>\n      <td>2</td>\n      <td>9297.541884</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025</td>\n      <td>3</td>\n      <td>9885.118867</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025</td>\n      <td>4</td>\n      <td>6386.312916</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2026</td>\n      <td>0</td>\n      <td>11813.170409</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2026</td>\n      <td>1</td>\n      <td>10144.548109</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2026</td>\n      <td>2</td>\n      <td>9713.114064</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2026</td>\n      <td>3</td>\n      <td>9216.950303</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2026</td>\n      <td>4</td>\n      <td>7629.337326</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2027</td>\n      <td>0</td>\n      <td>10380.713317</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2027</td>\n      <td>1</td>\n      <td>9184.488564</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2027</td>\n      <td>2</td>\n      <td>10268.534303</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2027</td>\n      <td>3</td>\n      <td>9297.894492</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2027</td>\n      <td>4</td>\n      <td>7184.857510</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Cell 13: Fit final models on ALL training data (per seed), predict test — CALLBACKS + SAFE num_boost_round\n\ndef fit_full_and_predict(train_df, test_df, features, target, params, seeds=[SEED]):\n    cat_feats = [c for c in features if c in categorical_cols]\n    X_full = train_df[features]\n    y_full = target\n    X_test = test_df[features]\n\n    # derive a robust num_boost_round from CV models\n    bests = []\n    for ms in models_by_seed:\n        for m in ms:\n            bi = getattr(m, \"best_iteration\", None)\n            if bi is None or bi <= 0:\n                bi = getattr(m, \"current_iteration\", lambda: 1000)()\n            bests.append(int(bi))\n    avg_best = int(np.mean(bests)) if len(bests) else 1000\n    num_boost = max(300, int(1.1 * avg_best))  # small safety margin\n\n    test_pred = np.zeros(len(test_df), dtype=float)\n    models = []\n\n    for sd in seeds:\n        params_seeded = params.copy()\n        params_seeded[\"seed\"] = sd\n\n        d_tr = lgb.Dataset(\n            X_full, label=y_full,\n            categorical_feature=cat_feats,\n            free_raw_data=False\n        )\n\n        # Use callbacks; NO verbose_eval kw\n        callbacks = [\n            lgb.log_evaluation(period=250)  # optional; remove if you want silent training\n        ]\n\n        model = lgb.train(\n            params_seeded,\n            d_tr,\n            num_boost_round=num_boost,\n            valid_sets=[d_tr],              # just to enable log_evaluation on train\n            valid_names=[\"train\"],\n            callbacks=callbacks\n        )\n        models.append(model)\n        test_pred += model.predict(X_test, num_iteration=model.best_iteration or num_boost) / len(seeds)\n\n    return test_pred, models\n\ntest_pred, full_models = fit_full_and_predict(\n    train_sess, test_sess, FEATS, train_sess[\"target\"].values, CFG[\"lgb_params\"], seeds=CFG[\"bagging_seeds\"]\n)\nprint(\"Got raw test predictions. num_boost_round used:\", full_models[0].current_iteration())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:14:12.431942Z","iopub.execute_input":"2025-08-27T08:14:12.432268Z","iopub.status.idle":"2025-08-27T08:14:40.233771Z","shell.execute_reply.started":"2025-08-27T08:14:12.432245Z","shell.execute_reply":"2025-08-27T08:14:40.232462Z"}},"outputs":[{"name":"stdout","text":"[250]\ttrain's rmse: 0.386174\n[250]\ttrain's rmse: 0.38656\n[250]\ttrain's rmse: 0.385226\nGot raw test predictions. num_boost_round used: 300\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Cell 14: Calibrate predictions with a 1-D Ridge on OOF\n\nif CFG[\"use_log_target\"]:\n    y_raw    = np.clip(np.expm1(train_sess[\"target\"].values), 0, None)\n    oof_raw  = np.clip(np.expm1(oof),                         0, None)\n    test_raw = np.clip(np.expm1(test_pred),                   0, None)\nelse:\n    y_raw    = train_sess[\"target\"].values\n    oof_raw  = oof\n    test_raw = test_pred\n\nridge = Ridge(alpha=1.0, fit_intercept=True, positive=True)\nridge.fit(oof_raw.reshape(-1,1), y_raw)\noof_cal  = ridge.predict(oof_raw.reshape(-1,1))\ntest_cal = ridge.predict(test_raw.reshape(-1,1))\n\ncal_mse = mean_squared_error(y_raw, oof_cal)\nprint(f\"Calibrated OOF MSE (raw): {cal_mse:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:15:14.018500Z","iopub.execute_input":"2025-08-27T08:15:14.019232Z","iopub.status.idle":"2025-08-27T08:15:14.032177Z","shell.execute_reply.started":"2025-08-27T08:15:14.019210Z","shell.execute_reply":"2025-08-27T08:15:14.031362Z"}},"outputs":[{"name":"stdout","text":"Calibrated OOF MSE (raw): 6674.6220\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Force a writeable path in Kaggle\nfrom pathlib import Path\nCFG[\"submission_path\"] = Path(\"/kaggle/working/submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:18:07.192308Z","iopub.execute_input":"2025-08-27T08:18:07.193008Z","iopub.status.idle":"2025-08-27T08:18:07.197602Z","shell.execute_reply.started":"2025-08-27T08:18:07.192985Z","shell.execute_reply":"2025-08-27T08:18:07.196589Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Cell 15: Build submission.csv (aligns to sample_submission if present) + writable path safety\n\nimport os\nfrom pathlib import Path\n\nsample_path = Path(\"./sample_submission.csv\")\nuse_sample = sample_path.exists()\n\ndef raw_train_labels():\n    if CFG[\"use_log_target\"]:\n        base = np.expm1(train_sess[\"target\"].values)\n    else:\n        base = train_sess[\"target\"].values\n    return np.clip(base, 0, None)\n\nif use_sample:\n    sample = pd.read_csv(sample_path)\n    ID_OUT, TARGET_OUT = sample.columns[:2].tolist()\n    print(f\"Using sample_submission format: ID={ID_OUT}, TARGET={TARGET_OUT}\")\n\n    pred_df = pd.DataFrame({\n        SESSION_COL: test_sess[SESSION_COL].astype(str).values,\n        \"_pred\": test_cal.astype(float)  # calibrated predictions on raw scale\n    })\n\n    tmp = sample[[ID_OUT]].copy()\n    if SESSION_COL != ID_OUT:\n        pred_df = pred_df.rename(columns={SESSION_COL: ID_OUT})\n    pred_df[ID_OUT] = pred_df[ID_OUT].astype(str)\n    tmp[ID_OUT] = tmp[ID_OUT].astype(str)\n\n    sub = tmp.merge(pred_df[[ID_OUT, \"_pred\"]], on=ID_OUT, how=\"left\")\n    sub[TARGET_OUT] = sub[\"_pred\"].fillna(0.0)\n\n    p998 = np.percentile(raw_train_labels(), 99.8)\n    sub[TARGET_OUT] = np.clip(sub[TARGET_OUT].values, 0, float(p998))\n    sub = sub[[ID_OUT, TARGET_OUT]]\nelse:\n    ID_OUT = SESSION_COL\n    TARGET_OUT = \"session_value\"\n\n    preds_cal = test_cal.astype(float)\n    p998 = np.percentile(raw_train_labels(), 99.8)\n    preds_cal = np.clip(preds_cal, 0, float(p998))\n\n    sub = pd.DataFrame({\n        ID_OUT: test_sess[SESSION_COL].astype(str).values,\n        TARGET_OUT: preds_cal\n    })\n\n# ---- Writable path fix ----\nsave_path = Path(CFG[\"submission_path\"])\n# If pointing into read-only /kaggle/input, redirect to /kaggle/working\nif str(save_path).startswith(\"/kaggle/input\"):\n    save_path = Path(\"/kaggle/working\") / save_path.name\n# If relative, make sure it lands in working by default\nif not save_path.is_absolute():\n    save_path = Path(\"/kaggle/working\") / save_path\n\nsub.to_csv(save_path, index=False)\nprint(\"Saved:\", save_path, \"shape:\", sub.shape)\ndisplay(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T08:18:18.910325Z","iopub.execute_input":"2025-08-27T08:18:18.910647Z","iopub.status.idle":"2025-08-27T08:18:19.012666Z","shell.execute_reply.started":"2025-08-27T08:18:18.910624Z","shell.execute_reply":"2025-08-27T08:18:19.011596Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/submission.csv shape: (30789, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     user_session  session_value\n0  SESSION_000000    3119.392500\n1  SESSION_000013      30.961481\n2  SESSION_000022      28.223525\n3  SESSION_000024      24.912189\n4  SESSION_000025      90.828167","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_session</th>\n      <th>session_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SESSION_000000</td>\n      <td>3119.392500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SESSION_000013</td>\n      <td>30.961481</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SESSION_000022</td>\n      <td>28.223525</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SESSION_000024</td>\n      <td>24.912189</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SESSION_000025</td>\n      <td>90.828167</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}