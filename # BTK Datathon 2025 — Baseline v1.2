{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb8b735",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:14.458525Z",
     "iopub.status.busy": "2025-08-27T14:13:14.458053Z",
     "iopub.status.idle": "2025-08-27T14:13:27.453673Z",
     "shell.execute_reply": "2025-08-27T14:13:27.452475Z"
    },
    "papermill": {
     "duration": 13.005155,
     "end_time": "2025-08-27T14:13:27.456409",
     "exception": false,
     "start_time": "2025-08-27T14:13:14.451254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions: pandas 2.2.3 | numpy 1.26.4 | lightgbm 4.5.0\n"
     ]
    }
   ],
   "source": [
    "# BTK Datathon 2025 — Baseline v2 (sub-1000 push)\n",
    "# Cell 1: Setup & imports\n",
    "\n",
    "import os, sys, gc, math, json, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "SEED = 2025\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "print(\"Versions:\",\n",
    "      \"pandas\", pd.__version__,\n",
    "      \"| numpy\", np.__version__,\n",
    "      \"| lightgbm\", lgb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3e1111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:27.467220Z",
     "iopub.status.busy": "2025-08-27T14:13:27.466517Z",
     "iopub.status.idle": "2025-08-27T14:13:27.479425Z",
     "shell.execute_reply": "2025-08-27T14:13:27.478600Z"
    },
    "papermill": {
     "duration": 0.020078,
     "end_time": "2025-08-27T14:13:27.481194",
     "exception": false,
     "start_time": "2025-08-27T14:13:27.461116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2: Config & helper for column inference\n",
    "\n",
    "CFG = dict(\n",
    "    train_path = Path(\"/kaggle/input/datathon-2025/train.csv\"),       # change if needed\n",
    "    test_path  = Path(\"/kaggle/input/datathon-2025/test.csv\"),        # change if needed\n",
    "    submission_path = Path(\"/kaggle/input/datathon-2025/submission.csv\"),\n",
    "    id_candidates = [\"session_id\",\"user_session\",\"id\"],\n",
    "    time_candidates = [\"event_time\",\"timestamp\",\"time\",\"ts\",\"date\"],\n",
    "    session_candidates = [\"user_session\",\"session_id\",\"session\",\"sid\"],\n",
    "    user_candidates = [\"user_id\",\"uid\",\"user\"],\n",
    "    event_candidates = [\"event\",\"type\",\"action\",\"event_type\"],\n",
    "    product_candidates = [\"product_id\",\"item_id\",\"sku\",\"product\"],\n",
    "    category_candidates = [\"category_id\",\"category\",\"cat\"],\n",
    "    price_candidates = [\"price\",\"value\",\"amount\",\"revenue\",\"purchase_value\",\"event_value\"],\n",
    "    target_candidates = [\"session_value\",\"purchase_amount\",\"order_value\",\"target\",\"label\",\"y\"],\n",
    "\n",
    "    # modeling\n",
    "    n_folds = 5,\n",
    "    purge_days = 1,                 # temporal purge between train/val windows\n",
    "    use_log_target = True,          # log1p target to tame heavy tails\n",
    "    label_clip_p = 99.5,            # clip top tail before log (reduces MSE)\n",
    "    has_buy_threshold = 0.0,        # when deriving has_buy from counts\n",
    "\n",
    "    # LightGBM regression defaults\n",
    "    lgb_params = dict(\n",
    "        objective = \"rmse\",\n",
    "        metric    = \"rmse\",\n",
    "        learning_rate = 0.05,\n",
    "        num_leaves = 256,\n",
    "        min_data_in_leaf = 128,\n",
    "        feature_fraction = 0.85,\n",
    "        bagging_fraction = 0.8,\n",
    "        bagging_freq = 1,\n",
    "        lambda_l2 = 2.0,\n",
    "        lambda_l1 = 0.1,\n",
    "        verbose = -1,\n",
    "        seed = SEED\n",
    "    ),\n",
    "\n",
    "    # seed bagging — train multiple seeds (averaged for test)\n",
    "    bagging_seeds = [2025, 2026, 2027],\n",
    ")\n",
    "\n",
    "def first_existing(df, cand):\n",
    "    for c in cand:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def infer_columns(train_df):\n",
    "    \"\"\"\n",
    "    Infer key columns; fallback gracefully if something is missing.\n",
    "    \"\"\"\n",
    "    cols = dict()\n",
    "    cols[\"ID_COL\"]       = first_existing(train_df, CFG[\"id_candidates\"])\n",
    "    cols[\"TIME_COL\"]     = first_existing(train_df, CFG[\"time_candidates\"])\n",
    "    cols[\"SESSION_COL\"]  = first_existing(train_df, CFG[\"session_candidates\"]) or cols[\"ID_COL\"]\n",
    "    cols[\"USER_COL\"]     = first_existing(train_df, CFG[\"user_candidates\"])\n",
    "    cols[\"EVENT_COL\"]    = first_existing(train_df, CFG[\"event_candidates\"])\n",
    "    cols[\"PRODUCT_COL\"]  = first_existing(train_df, CFG[\"product_candidates\"])\n",
    "    cols[\"CATEGORY_COL\"] = first_existing(train_df, CFG[\"category_candidates\"])\n",
    "    cols[\"PRICE_COL\"]    = first_existing(train_df, CFG[\"price_candidates\"])\n",
    "    cols[\"TARGET_COL\"]   = first_existing(train_df, CFG[\"target_candidates\"])\n",
    "    return cols\n",
    "\n",
    "def ensure_datetime(df, col):\n",
    "    if col and col in df.columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c0c248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:27.491520Z",
     "iopub.status.busy": "2025-08-27T14:13:27.491215Z",
     "iopub.status.idle": "2025-08-27T14:13:28.518204Z",
     "shell.execute_reply": "2025-08-27T14:13:28.516873Z"
    },
    "papermill": {
     "duration": 1.03444,
     "end_time": "2025-08-27T14:13:28.520020",
     "exception": false,
     "start_time": "2025-08-27T14:13:27.485580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (141219, 7) test: (62951, 6)\n",
      "train cols: ['event_time', 'event_type', 'product_id', 'category_id', 'user_id', 'user_session', 'session_value']\n",
      "Inferred columns: {'ID_COL': 'user_session', 'TIME_COL': 'event_time', 'SESSION_COL': 'user_session', 'USER_COL': 'user_id', 'EVENT_COL': 'event_type', 'PRODUCT_COL': 'product_id', 'CATEGORY_COL': 'category_id', 'PRICE_COL': None, 'TARGET_COL': 'session_value'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load CSVs\n",
    "train = pd.read_csv(CFG[\"train_path\"])\n",
    "test  = pd.read_csv(CFG[\"test_path\"])\n",
    "\n",
    "print(\"train:\", train.shape, \"test:\", test.shape)\n",
    "print(\"train cols:\", list(train.columns)[:40])\n",
    "\n",
    "COLS = infer_columns(train)\n",
    "globals().update(COLS)  # expose names like ID_COL, TIME_COL, etc.\n",
    "\n",
    "print(\"Inferred columns:\", COLS)\n",
    "\n",
    "# If TIME_COL exists, convert to datetime\n",
    "for df in (train, test):\n",
    "    ensure_datetime(df, TIME_COL)\n",
    "\n",
    "# Basic sanity\n",
    "assert SESSION_COL in train.columns, f\"Couldn't infer session id (candidates: {CFG['session_candidates']})\"\n",
    "if TIME_COL: \n",
    "    assert TIME_COL in train.columns, f\"Couldn't infer time column (candidates: {CFG['time_candidates']})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f332ae5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:28.531029Z",
     "iopub.status.busy": "2025-08-27T14:13:28.530099Z",
     "iopub.status.idle": "2025-08-27T14:13:28.690525Z",
     "shell.execute_reply": "2025-08-27T14:13:28.688826Z"
    },
    "papermill": {
     "duration": 0.168518,
     "end_time": "2025-08-27T14:13:28.693198",
     "exception": false,
     "start_time": "2025-08-27T14:13:28.524680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built session_value for train. Sessions with value: 70736 / 70736\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build session-level labels and basic aggregates\n",
    "\n",
    "def guess_session_value(df):\n",
    "    # Priority 1: if TARGET_COL exists at row-level, aggregate by session sum\n",
    "    if TARGET_COL and TARGET_COL in df.columns:\n",
    "        sv = df.groupby(SESSION_COL)[TARGET_COL].sum()\n",
    "        return sv.rename(\"session_value\")\n",
    "\n",
    "    # Priority 2: sum of price/value on purchase/buy events if EVENT_COL present\n",
    "    ev_col = EVENT_COL\n",
    "    price_col = PRICE_COL\n",
    "    if ev_col in df.columns and price_col in df.columns:\n",
    "        # mark purchases\n",
    "        purchase_mask = df[ev_col].astype(str).str.lower().str.contains(\"buy|purchase|order|checkout\")\n",
    "        sv = df.loc[purchase_mask].groupby(SESSION_COL)[price_col].sum()\n",
    "        return sv.rename(\"session_value\")\n",
    "\n",
    "    # Priority 3: sum of any 'price/value' column across session\n",
    "    if price_col in df.columns:\n",
    "        sv = df.groupby(SESSION_COL)[price_col].sum()\n",
    "        return sv.rename(\"session_value\")\n",
    "\n",
    "    # If nothing matched, fallback: count events (not ideal, but consistent)\n",
    "    sv = df.groupby(SESSION_COL).size()\n",
    "    return sv.rename(\"session_value\")\n",
    "\n",
    "# session_value for train\n",
    "sess_val = guess_session_value(train)\n",
    "print(\"Built session_value for train. Sessions with value:\", (sess_val>0).sum(), \"/\", len(sess_val))\n",
    "\n",
    "# For test, label unknown: keep placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c43d0ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:28.707054Z",
     "iopub.status.busy": "2025-08-27T14:13:28.706682Z",
     "iopub.status.idle": "2025-08-27T14:13:32.048037Z",
     "shell.execute_reply": "2025-08-27T14:13:32.046640Z"
    },
    "papermill": {
     "duration": 3.35061,
     "end_time": "2025-08-27T14:13:32.050263",
     "exception": false,
     "start_time": "2025-08-27T14:13:28.699653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sess: (70736, 19) test_sess: (30789, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_session</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_end</th>\n",
       "      <th>n_events</th>\n",
       "      <th>session_dur_s</th>\n",
       "      <th>n_product_unique</th>\n",
       "      <th>n_category_unique</th>\n",
       "      <th>cnt_add</th>\n",
       "      <th>cnt_buy</th>\n",
       "      <th>cnt_view</th>\n",
       "      <th>last_evt_1</th>\n",
       "      <th>last_evt_2</th>\n",
       "      <th>last_evt_3</th>\n",
       "      <th>main_product</th>\n",
       "      <th>main_category</th>\n",
       "      <th>view_to_add</th>\n",
       "      <th>add_to_buy</th>\n",
       "      <th>has_buy</th>\n",
       "      <th>session_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SESSION_000000</td>\n",
       "      <td>2025-06-02 12:14:45+00:00</td>\n",
       "      <td>2025-06-20 17:17:32+00:00</td>\n",
       "      <td>28</td>\n",
       "      <td>1573367.0</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>PROD_004203</td>\n",
       "      <td>CAT_00280</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9962.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SESSION_000001</td>\n",
       "      <td>2025-06-02 03:55:52+00:00</td>\n",
       "      <td>2025-06-02 06:06:10+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>7818.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>BUY</td>\n",
       "      <td>REMOVE_CART</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>PROD_021185</td>\n",
       "      <td>CAT_00280</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>579.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SESSION_000004</td>\n",
       "      <td>2025-06-04 10:27:59+00:00</td>\n",
       "      <td>2025-06-04 10:27:59+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PROD_013881</td>\n",
       "      <td>CAT_00267</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_session             session_start               session_end  \\\n",
       "0  SESSION_000000 2025-06-02 12:14:45+00:00 2025-06-20 17:17:32+00:00   \n",
       "1  SESSION_000001 2025-06-02 03:55:52+00:00 2025-06-02 06:06:10+00:00   \n",
       "2  SESSION_000004 2025-06-04 10:27:59+00:00 2025-06-04 10:27:59+00:00   \n",
       "\n",
       "   n_events  session_dur_s  n_product_unique  n_category_unique  cnt_add  \\\n",
       "0        28      1573367.0                24                 20       28   \n",
       "1         6         7818.0                 5                  5        4   \n",
       "2         1            0.0                 1                  1        0   \n",
       "\n",
       "   cnt_buy  cnt_view last_evt_1   last_evt_2 last_evt_3 main_product  \\\n",
       "0        0         0   ADD_CART     ADD_CART   ADD_CART  PROD_004203   \n",
       "1        1         1        BUY  REMOVE_CART   ADD_CART  PROD_021185   \n",
       "2        0         1       None         None       None  PROD_013881   \n",
       "\n",
       "  main_category  view_to_add  add_to_buy  has_buy  session_value  \n",
       "0     CAT_00280         0.00         0.0        0        9962.40  \n",
       "1     CAT_00280         0.25         4.0        1         579.60  \n",
       "2     CAT_00267         0.00         0.0        0          30.92  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Create session-level frames (train_sess, test_sess) with core aggregations — DUP-SAFE\n",
    "\n",
    "def build_session_frame(df, is_train=True):\n",
    "    g = df.groupby(SESSION_COL, as_index=False)\n",
    "\n",
    "    # session start/end if time exists\n",
    "    if TIME_COL and TIME_COL in df.columns:\n",
    "        agg = g.agg(\n",
    "            session_start=(TIME_COL, \"min\"),\n",
    "            session_end=(TIME_COL, \"max\"),\n",
    "            n_events=(TIME_COL, \"count\")\n",
    "        )\n",
    "        agg[\"session_dur_s\"] = (agg[\"session_end\"] - agg[\"session_start\"]).dt.total_seconds().fillna(0)\n",
    "    else:\n",
    "        agg = g.size().rename(columns={\"size\":\"n_events\"}).reset_index()\n",
    "        agg[\"session_start\"] = pd.NaT\n",
    "        agg[\"session_end\"]   = pd.NaT\n",
    "        agg[\"session_dur_s\"] = 0.0\n",
    "\n",
    "    # unique counts\n",
    "    if PRODUCT_COL and PRODUCT_COL in df.columns:\n",
    "        uu = df.groupby(SESSION_COL)[PRODUCT_COL].nunique().rename(\"n_product_unique\")\n",
    "        agg = agg.merge(uu, on=SESSION_COL, how=\"left\")\n",
    "    if CATEGORY_COL and CATEGORY_COL in df.columns:\n",
    "        uu = df.groupby(SESSION_COL)[CATEGORY_COL].nunique().rename(\"n_category_unique\")\n",
    "        agg = agg.merge(uu, on=SESSION_COL, how=\"left\")\n",
    "\n",
    "    # event-type counts  -> collapse duplicate semantic columns (e.g., multiple \"view\" variants)\n",
    "    if EVENT_COL and EVENT_COL in df.columns:\n",
    "        ev = df[[SESSION_COL, EVENT_COL]].copy()\n",
    "        ev[EVENT_COL] = ev[EVENT_COL].astype(str).str.lower()\n",
    "        piv = (ev\n",
    "               .groupby([SESSION_COL, EVENT_COL]).size()\n",
    "               .unstack(fill_value=0))\n",
    "\n",
    "        # friendly names: map raw tokens to standard columns\n",
    "        rename_map = {}\n",
    "        for c in piv.columns:\n",
    "            if \"view\" in c: rename_map[c] = \"cnt_view\"\n",
    "            elif \"add\" in c or \"cart\" in c: rename_map[c] = \"cnt_add\"\n",
    "            elif \"buy\" in c or \"purchase\" in c or \"order\" in c or \"checkout\" in c: rename_map[c] = \"cnt_buy\"\n",
    "\n",
    "        piv2 = piv.rename(columns=rename_map)\n",
    "\n",
    "        # *** CRITICAL: collapse duplicate columns by summing (e.g., multiple tokens -> 'cnt_view') ***\n",
    "        piv2 = piv2.T.groupby(level=0).sum().T  # sums duplicates with same column name\n",
    "\n",
    "        # now safe to merge\n",
    "        agg = agg.merge(piv2.reset_index(), on=SESSION_COL, how=\"left\")\n",
    "\n",
    "    # ---- last N event tokens per session (no duplicate SESSION_COL on reset) ----\n",
    "    def lastN_events(local_df, N=3):\n",
    "        if EVENT_COL not in local_df.columns:\n",
    "            return pd.DataFrame({SESSION_COL: local_df[SESSION_COL].unique()})\n",
    "        srt = local_df.copy()\n",
    "        srt[EVENT_COL] = srt[EVENT_COL].astype(str)\n",
    "        if TIME_COL and TIME_COL in srt.columns:\n",
    "            srt = srt.sort_values([SESSION_COL, TIME_COL])\n",
    "        else:\n",
    "            srt = srt.sort_values(SESSION_COL)\n",
    "\n",
    "        grp = srt.groupby(SESSION_COL)\n",
    "        for n in range(1, N+1):\n",
    "            srt[f\"last_evt_{n}\"] = grp[EVENT_COL].shift(n)\n",
    "\n",
    "        cols = [f\"last_evt_{n}\" for n in range(1, N+1)]\n",
    "        res = srt.groupby(SESSION_COL, as_index=False)[cols].last()\n",
    "        return res\n",
    "\n",
    "    last3 = lastN_events(df, N=3)\n",
    "    agg = agg.merge(last3, on=SESSION_COL, how=\"left\")\n",
    "\n",
    "    # main product/category heuristic: last product/category in session\n",
    "    if PRODUCT_COL and PRODUCT_COL in df.columns:\n",
    "        srt = df.sort_values([SESSION_COL, TIME_COL]) if TIME_COL in df.columns else df.sort_values(SESSION_COL)\n",
    "        main_prod = srt.groupby(SESSION_COL)[PRODUCT_COL].last().rename(\"main_product\")\n",
    "        agg = agg.merge(main_prod, on=SESSION_COL, how=\"left\")\n",
    "    if CATEGORY_COL and CATEGORY_COL in df.columns:\n",
    "        srt = df.sort_values([SESSION_COL, TIME_COL]) if TIME_COL in df.columns else df.sort_values(SESSION_COL)\n",
    "        main_cat = srt.groupby(SESSION_COL)[CATEGORY_COL].last().rename(\"main_category\")\n",
    "        agg = agg.merge(main_cat, on=SESSION_COL, how=\"left\")\n",
    "\n",
    "    # ---------- robust ratios even if duplicate labels sneak in ----------\n",
    "    def get_series_sum(df_local, colname):\n",
    "        \"\"\"Return a 1D float Series for the (possibly duplicate-named) column by summing duplicates.\"\"\"\n",
    "        if colname not in df_local.columns:\n",
    "            return None\n",
    "        sub = df_local.loc[:, df_local.columns == colname]\n",
    "        if isinstance(sub, pd.Series):  # single match\n",
    "            return sub.astype(float)\n",
    "        # DataFrame with one or more columns (duplicates) -> sum row-wise\n",
    "        return sub.astype(float).sum(axis=1)\n",
    "\n",
    "    for a, b, name in [(\"cnt_view\",\"cnt_add\",\"view_to_add\"),\n",
    "                       (\"cnt_add\",\"cnt_buy\",\"add_to_buy\")]:\n",
    "        num = get_series_sum(agg, a)\n",
    "        den = get_series_sum(agg, b)\n",
    "        if num is not None and den is not None:\n",
    "            den_safe = den.replace(0, np.nan)\n",
    "            ratio = (num / den_safe).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "            # assign by values to avoid alignment complaints\n",
    "            agg[name] = ratio.values\n",
    "\n",
    "    # has_buy\n",
    "    if \"cnt_buy\" in agg.columns:\n",
    "        # ensure it's a 1D series even if duplicates existed\n",
    "        cnt_buy = get_series_sum(agg, \"cnt_buy\")\n",
    "        agg[\"has_buy\"] = (cnt_buy.fillna(0) > CFG[\"has_buy_threshold\"]).astype(int)\n",
    "\n",
    "    # attach label for train\n",
    "    if is_train:\n",
    "        lbl = guess_session_value(df)\n",
    "        lbl = lbl.reindex(agg[SESSION_COL]).fillna(0.0).reset_index(drop=False)\n",
    "        agg = agg.merge(lbl, on=SESSION_COL, how=\"left\")\n",
    "\n",
    "    return agg\n",
    "\n",
    "train_sess = build_session_frame(train, is_train=True)\n",
    "test_sess  = build_session_frame(test,  is_train=False)\n",
    "\n",
    "print(\"train_sess:\", train_sess.shape, \"test_sess:\", test_sess.shape)\n",
    "train_sess.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c85a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:32.062926Z",
     "iopub.status.busy": "2025-08-27T14:13:32.062536Z",
     "iopub.status.idle": "2025-08-27T14:13:39.699492Z",
     "shell.execute_reply": "2025-08-27T14:13:39.697877Z"
    },
    "papermill": {
     "duration": 7.645852,
     "end_time": "2025-08-27T14:13:39.701498",
     "exception": false,
     "start_time": "2025-08-27T14:13:32.055646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added entity histories.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_session</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_end</th>\n",
       "      <th>n_events</th>\n",
       "      <th>session_dur_s</th>\n",
       "      <th>n_product_unique</th>\n",
       "      <th>n_category_unique</th>\n",
       "      <th>cnt_add</th>\n",
       "      <th>cnt_buy</th>\n",
       "      <th>cnt_view</th>\n",
       "      <th>last_evt_1</th>\n",
       "      <th>last_evt_2</th>\n",
       "      <th>last_evt_3</th>\n",
       "      <th>main_product</th>\n",
       "      <th>main_category</th>\n",
       "      <th>view_to_add</th>\n",
       "      <th>add_to_buy</th>\n",
       "      <th>has_buy</th>\n",
       "      <th>session_value</th>\n",
       "      <th>main_product_prev_mean_sv</th>\n",
       "      <th>main_product_prev_n</th>\n",
       "      <th>main_category_prev_mean_sv</th>\n",
       "      <th>main_category_prev_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SESSION_000000</td>\n",
       "      <td>2025-06-02 12:14:45+00:00</td>\n",
       "      <td>2025-06-20 17:17:32+00:00</td>\n",
       "      <td>28</td>\n",
       "      <td>1573367.0</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>PROD_004203</td>\n",
       "      <td>CAT_00280</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9962.40</td>\n",
       "      <td>42.360000</td>\n",
       "      <td>1</td>\n",
       "      <td>47.808713</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SESSION_000001</td>\n",
       "      <td>2025-06-02 03:55:52+00:00</td>\n",
       "      <td>2025-06-02 06:06:10+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>7818.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>BUY</td>\n",
       "      <td>REMOVE_CART</td>\n",
       "      <td>ADD_CART</td>\n",
       "      <td>PROD_021185</td>\n",
       "      <td>CAT_00280</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>579.60</td>\n",
       "      <td>131.550000</td>\n",
       "      <td>1</td>\n",
       "      <td>58.768310</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SESSION_000004</td>\n",
       "      <td>2025-06-04 10:27:59+00:00</td>\n",
       "      <td>2025-06-04 10:27:59+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PROD_013881</td>\n",
       "      <td>CAT_00267</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.92</td>\n",
       "      <td>150.427581</td>\n",
       "      <td>0</td>\n",
       "      <td>35.535238</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_session             session_start               session_end  \\\n",
       "0  SESSION_000000 2025-06-02 12:14:45+00:00 2025-06-20 17:17:32+00:00   \n",
       "1  SESSION_000001 2025-06-02 03:55:52+00:00 2025-06-02 06:06:10+00:00   \n",
       "2  SESSION_000004 2025-06-04 10:27:59+00:00 2025-06-04 10:27:59+00:00   \n",
       "\n",
       "   n_events  session_dur_s  n_product_unique  n_category_unique  cnt_add  \\\n",
       "0        28      1573367.0                24                 20       28   \n",
       "1         6         7818.0                 5                  5        4   \n",
       "2         1            0.0                 1                  1        0   \n",
       "\n",
       "   cnt_buy  cnt_view last_evt_1   last_evt_2 last_evt_3 main_product  \\\n",
       "0        0         0   ADD_CART     ADD_CART   ADD_CART  PROD_004203   \n",
       "1        1         1        BUY  REMOVE_CART   ADD_CART  PROD_021185   \n",
       "2        0         1       None         None       None  PROD_013881   \n",
       "\n",
       "  main_category  view_to_add  add_to_buy  has_buy  session_value  \\\n",
       "0     CAT_00280         0.00         0.0        0        9962.40   \n",
       "1     CAT_00280         0.25         4.0        1         579.60   \n",
       "2     CAT_00267         0.00         0.0        0          30.92   \n",
       "\n",
       "   main_product_prev_mean_sv  main_product_prev_n  main_category_prev_mean_sv  \\\n",
       "0                  42.360000                    1                   47.808713   \n",
       "1                 131.550000                    1                   58.768310   \n",
       "2                 150.427581                    0                   35.535238   \n",
       "\n",
       "   main_category_prev_n  \n",
       "0                   303  \n",
       "1                    71  \n",
       "2                    21  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6: Expanding/shifted histories for main_product / main_category\n",
    "\n",
    "def add_entity_history(train_sess, test_sess, key_col, target_col=\"session_value\"):\n",
    "    if key_col not in train_sess.columns: \n",
    "        return train_sess, test_sess\n",
    "\n",
    "    cols_needed = [SESSION_COL, key_col, \"session_start\", target_col]\n",
    "    for c in [\"session_start\"]:\n",
    "        if c not in train_sess.columns:\n",
    "            train_sess[c] = pd.to_datetime(\"1970-01-01\")\n",
    "        if c not in test_sess.columns:\n",
    "            test_sess[c] = pd.to_datetime(\"1970-01-01\")\n",
    "\n",
    "    comb = pd.concat(\n",
    "        [\n",
    "            train_sess[cols_needed].assign(_is_train=1),\n",
    "            test_sess[[SESSION_COL, key_col, \"session_start\"]].assign(**{target_col: np.nan, \"_is_train\":0})\n",
    "        ],\n",
    "        axis=0, ignore_index=True\n",
    "    ).sort_values([\"session_start\"]).reset_index(drop=True)\n",
    "\n",
    "    comb[key_col] = comb[key_col].astype(str)\n",
    "\n",
    "    g = comb.groupby(key_col, sort=False)\n",
    "    # expanding mean/count of past session_value (shifted via cumcount)\n",
    "    sv = comb[target_col].fillna(0.0).values\n",
    "    cum_sum = g[target_col].transform(lambda x: x.fillna(0.0).cumsum())\n",
    "    cum_cnt = g[target_col].transform(lambda x: np.arange(len(x)))\n",
    "    prev_sum = cum_sum - sv\n",
    "    prev_cnt = cum_cnt\n",
    "    gmean = float(train_sess[target_col].mean())\n",
    "\n",
    "    comb[f\"{key_col}_prev_mean_sv\"] = np.where(prev_cnt>0, prev_sum/np.maximum(prev_cnt,1), np.nan)\n",
    "    comb[f\"{key_col}_prev_n\"] = prev_cnt.astype(int)\n",
    "\n",
    "    comb[f\"{key_col}_prev_mean_sv\"] = comb[f\"{key_col}_prev_mean_sv\"].fillna(gmean)\n",
    "\n",
    "    tr = comb[comb[\"_is_train\"]==1][[SESSION_COL, f\"{key_col}_prev_mean_sv\", f\"{key_col}_prev_n\"]]\n",
    "    te = comb[comb[\"_is_train\"]==0][[SESSION_COL, f\"{key_col}_prev_mean_sv\", f\"{key_col}_prev_n\"]]\n",
    "\n",
    "    train_sess = train_sess.merge(tr, on=SESSION_COL, how=\"left\")\n",
    "    test_sess  = test_sess.merge(te, on=SESSION_COL, how=\"left\")\n",
    "\n",
    "    # fill gaps\n",
    "    for c in [f\"{key_col}_prev_mean_sv\", f\"{key_col}_prev_n\"]:\n",
    "        if c in train_sess.columns:\n",
    "            train_sess[c] = train_sess[c].fillna(gmean if \"mean\" in c else 0)\n",
    "        if c in test_sess.columns:\n",
    "            test_sess[c] = test_sess[c].fillna(gmean if \"mean\" in c else 0)\n",
    "\n",
    "    return train_sess, test_sess\n",
    "\n",
    "for key in [\"main_product\",\"main_category\"]:\n",
    "    if key in train_sess.columns:\n",
    "        train_sess, test_sess = add_entity_history(train_sess, test_sess, key_col=key, target_col=\"session_value\")\n",
    "\n",
    "print(\"Added entity histories.\")\n",
    "train_sess.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a937aa4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:39.713741Z",
     "iopub.status.busy": "2025-08-27T14:13:39.713425Z",
     "iopub.status.idle": "2025-08-27T14:13:39.733527Z",
     "shell.execute_reply": "2025-08-27T14:13:39.732586Z"
    },
    "papermill": {
     "duration": 0.0288,
     "end_time": "2025-08-27T14:13:39.735359",
     "exception": false,
     "start_time": "2025-08-27T14:13:39.706559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cols: ['last_evt_1', 'last_evt_2', 'last_evt_3', 'main_product', 'main_category', 'ss_dow', 'ss_hour']\n",
      "numeric_cols: ['n_events', 'session_dur_s', 'n_product_unique', 'n_category_unique', 'cnt_view', 'cnt_add', 'cnt_buy', 'view_to_add', 'add_to_buy', 'main_product_prev_mean_sv', 'main_product_prev_n', 'main_category_prev_mean_sv', 'main_category_prev_n']\n",
      "Total features: 20\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Build feature lists\n",
    "\n",
    "categorical_cols = []\n",
    "for c in [f\"last_evt_{i}\" for i in range(1,4)] + [\"main_product\",\"main_category\"]:\n",
    "    if c in train_sess.columns:\n",
    "        if train_sess[c].dtype == \"O\" or str(train_sess[c].dtype).startswith(\"category\"):\n",
    "            categorical_cols.append(c)\n",
    "\n",
    "numeric_cols = []\n",
    "for c in [\n",
    "    \"n_events\",\"session_dur_s\",\"n_product_unique\",\"n_category_unique\",\n",
    "    \"cnt_view\",\"cnt_add\",\"cnt_buy\",\"view_to_add\",\"add_to_buy\",\n",
    "    \"main_product_prev_mean_sv\",\"main_product_prev_n\",\n",
    "    \"main_category_prev_mean_sv\",\"main_category_prev_n\"\n",
    "]:\n",
    "    if c in train_sess.columns:\n",
    "        numeric_cols.append(c)\n",
    "\n",
    "# time features from session_start\n",
    "if \"session_start\" in train_sess.columns and pd.api.types.is_datetime64_any_dtype(train_sess[\"session_start\"]):\n",
    "    for df in (train_sess, test_sess):\n",
    "        df[\"ss_hour\"] = df[\"session_start\"].dt.hour.fillna(0).astype(int)\n",
    "        df[\"ss_dow\"]  = df[\"session_start\"].dt.dayofweek.fillna(0).astype(int)\n",
    "    categorical_cols += [\"ss_dow\",\"ss_hour\"]\n",
    "\n",
    "FEATS = categorical_cols + numeric_cols\n",
    "print(\"categorical_cols:\", categorical_cols)\n",
    "print(\"numeric_cols:\", numeric_cols)\n",
    "print(\"Total features:\", len(FEATS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1382772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:39.747079Z",
     "iopub.status.busy": "2025-08-27T14:13:39.746734Z",
     "iopub.status.idle": "2025-08-27T14:13:39.789056Z",
     "shell.execute_reply": "2025-08-27T14:13:39.788057Z"
    },
    "papermill": {
     "duration": 0.050029,
     "end_time": "2025-08-27T14:13:39.790964",
     "exception": false,
     "start_time": "2025-08-27T14:13:39.740935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label clip @p= 99.5 => 3119.3924999999977 | log-target: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Target transformation\n",
    "assert \"session_value\" in train_sess.columns, \"No session_value in train_sess.\"\n",
    "\n",
    "y_raw = train_sess[\"session_value\"].values.astype(float)\n",
    "clip_val = np.percentile(y_raw, CFG[\"label_clip_p\"])\n",
    "y_clipped = np.clip(y_raw, 0, clip_val)\n",
    "\n",
    "if CFG[\"use_log_target\"]:\n",
    "    y = np.log1p(y_clipped)\n",
    "else:\n",
    "    y = y_clipped\n",
    "\n",
    "train_sess[\"target\"] = y\n",
    "print(\"Label clip @p=\", CFG[\"label_clip_p\"], \"=>\", clip_val, \"| log-target:\", CFG[\"use_log_target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac64daad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:39.802844Z",
     "iopub.status.busy": "2025-08-27T14:13:39.802518Z",
     "iopub.status.idle": "2025-08-27T14:13:40.373959Z",
     "shell.execute_reply": "2025-08-27T14:13:40.372536Z"
    },
    "papermill": {
     "duration": 0.579498,
     "end_time": "2025-08-27T14:13:40.375592",
     "exception": false,
     "start_time": "2025-08-27T14:13:39.796094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded categoricals: 7\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Label-encode categoricals to keep LightGBM happy\n",
    "encoders = {}\n",
    "for c in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_vals = pd.concat([train_sess[c].astype(str), test_sess[c].astype(str)], axis=0).fillna(\"NA\")\n",
    "    le.fit(all_vals)\n",
    "    train_sess[c] = le.transform(train_sess[c].astype(str).fillna(\"NA\"))\n",
    "    test_sess[c]  = le.transform(test_sess[c].astype(str).fillna(\"NA\"))\n",
    "    encoders[c] = le\n",
    "\n",
    "print(\"Encoded categoricals:\", len(encoders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae22b25d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:40.386642Z",
     "iopub.status.busy": "2025-08-27T14:13:40.386258Z",
     "iopub.status.idle": "2025-08-27T14:13:44.003798Z",
     "shell.execute_reply": "2025-08-27T14:13:44.003115Z"
    },
    "papermill": {
     "duration": 3.6251,
     "end_time": "2025-08-27T14:13:44.005577",
     "exception": false,
     "start_time": "2025-08-27T14:13:40.380477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added p_has_buy\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Optional auxiliary classifier -> add p_has_buy as numeric feature\n",
    "\n",
    "def add_has_buy_proba(train_sess, test_sess):\n",
    "    # derive label if missing\n",
    "    if \"has_buy\" not in train_sess.columns:\n",
    "        if \"cnt_buy\" in train_sess.columns:\n",
    "            train_sess[\"has_buy\"] = (train_sess[\"cnt_buy\"] > CFG[\"has_buy_threshold\"]).astype(int)\n",
    "        else:\n",
    "            train_sess[\"has_buy\"] = (train_sess[\"session_value\"] > 0).astype(int)\n",
    "\n",
    "    clf_feats = [f for f in FEATS if f != \"has_buy\" and f in train_sess.columns]\n",
    "    cat_in_clf = [c for c in categorical_cols if c in clf_feats]\n",
    "\n",
    "    d_clf = lgb.Dataset(train_sess[clf_feats], label=train_sess[\"has_buy\"],\n",
    "                        categorical_feature=cat_in_clf, free_raw_data=False)\n",
    "    clf_params = dict(\n",
    "        objective=\"binary\", metric=\"auc\", learning_rate=0.05,\n",
    "        num_leaves=64, feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=1,\n",
    "        min_data_in_leaf=64, seed=SEED, verbose=-1\n",
    "    )\n",
    "    clf = lgb.train(clf_params, d_clf, num_boost_round=600)\n",
    "\n",
    "    train_sess[\"p_has_buy\"] = clf.predict(train_sess[clf_feats])\n",
    "    test_sess[\"p_has_buy\"]  = clf.predict(test_sess[clf_feats])\n",
    "\n",
    "    if \"p_has_buy\" not in FEATS:\n",
    "        FEATS.append(\"p_has_buy\")\n",
    "    if \"p_has_buy\" not in numeric_cols:\n",
    "        numeric_cols.append(\"p_has_buy\")\n",
    "    return train_sess, test_sess\n",
    "\n",
    "train_sess, test_sess = add_has_buy_proba(train_sess, test_sess)\n",
    "print(\"Added p_has_buy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e11bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:44.018896Z",
     "iopub.status.busy": "2025-08-27T14:13:44.018581Z",
     "iopub.status.idle": "2025-08-27T14:13:44.047505Z",
     "shell.execute_reply": "2025-08-27T14:13:44.046287Z"
    },
    "papermill": {
     "duration": 0.037412,
     "end_time": "2025-08-27T14:13:44.049434",
     "exception": false,
     "start_time": "2025-08-27T14:13:44.012022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 5 folds.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Time-sorted CV with purge\n",
    "\n",
    "def make_time_sorted_folds(df, n_folds=5, purge_days=1):\n",
    "    if \"session_start\" not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[\"session_start\"]):\n",
    "        # fallback to simple KFold if no time\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "        idx = np.arange(len(df))\n",
    "        folds = []\n",
    "        for tr, va in kf.split(idx):\n",
    "            folds.append((tr, va))\n",
    "        return folds\n",
    "\n",
    "    srt_idx = np.argsort(df[\"session_start\"].values)\n",
    "    fold_sizes = np.full(n_folds, len(df)//n_folds, dtype=int)\n",
    "    fold_sizes[:len(df)%n_folds] += 1\n",
    "\n",
    "    current = 0\n",
    "    fold_indices = []\n",
    "    for fs in fold_sizes:\n",
    "        fold_indices.append(srt_idx[current:current+fs])\n",
    "        current += fs\n",
    "\n",
    "    folds = []\n",
    "    for k in range(n_folds):\n",
    "        va_idx = fold_indices[k]\n",
    "        va_min_time = df[\"session_start\"].iloc[va_idx].min()\n",
    "        va_max_time = df[\"session_start\"].iloc[va_idx].max()\n",
    "        purge_start = va_min_time - timedelta(days=purge_days)\n",
    "        purge_end   = va_max_time + timedelta(days=purge_days)\n",
    "\n",
    "        tr_mask = ~((df[\"session_start\"]>=purge_start) & (df[\"session_start\"]<=purge_end))\n",
    "        tr_idx = np.where(tr_mask)[0]\n",
    "        folds.append((tr_idx, va_idx))\n",
    "    return folds\n",
    "\n",
    "folds = make_time_sorted_folds(train_sess, n_folds=CFG[\"n_folds\"], purge_days=CFG[\"purge_days\"])\n",
    "print(\"Built\", len(folds), \"folds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702a0e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:13:44.061717Z",
     "iopub.status.busy": "2025-08-27T14:13:44.061414Z",
     "iopub.status.idle": "2025-08-27T14:15:11.697230Z",
     "shell.execute_reply": "2025-08-27T14:15:11.696331Z"
    },
    "papermill": {
     "duration": 87.643842,
     "end_time": "2025-08-27T14:15:11.698936",
     "exception": false,
     "start_time": "2025-08-27T14:13:44.055094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.384601\tvalid's rmse: 0.493121\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's rmse: 0.463712\tvalid's rmse: 0.479188\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.3745\tvalid's rmse: 0.525698\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttrain's rmse: 0.453338\tvalid's rmse: 0.5143\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.375631\tvalid's rmse: 0.532323\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttrain's rmse: 0.445349\tvalid's rmse: 0.522025\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.379419\tvalid's rmse: 0.513422\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's rmse: 0.454122\tvalid's rmse: 0.502207\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.374637\tvalid's rmse: 0.539418\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttrain's rmse: 0.439081\tvalid's rmse: 0.530868\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.385528\tvalid's rmse: 0.493061\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 0.467555\tvalid's rmse: 0.479059\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.375003\tvalid's rmse: 0.525392\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttrain's rmse: 0.457449\tvalid's rmse: 0.514967\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.376087\tvalid's rmse: 0.53221\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttrain's rmse: 0.447731\tvalid's rmse: 0.521566\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.379973\tvalid's rmse: 0.513466\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttrain's rmse: 0.451945\tvalid's rmse: 0.501879\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.375094\tvalid's rmse: 0.538073\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's rmse: 0.448747\tvalid's rmse: 0.530043\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.383803\tvalid's rmse: 0.493784\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's rmse: 0.46249\tvalid's rmse: 0.480451\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.373611\tvalid's rmse: 0.527196\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttrain's rmse: 0.453506\tvalid's rmse: 0.514841\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.374817\tvalid's rmse: 0.532841\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttrain's rmse: 0.447954\tvalid's rmse: 0.522273\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.378689\tvalid's rmse: 0.513652\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttrain's rmse: 0.452516\tvalid's rmse: 0.501678\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[250]\ttrain's rmse: 0.372923\tvalid's rmse: 0.538963\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttrain's rmse: 0.444265\tvalid's rmse: 0.530667\n",
      "OOF MSE (raw scale): 9300.0521\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>10581.376414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>9168.786527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>9297.541884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>9885.118867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>6386.312916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026</td>\n",
       "      <td>0</td>\n",
       "      <td>11813.170409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>10144.548109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026</td>\n",
       "      <td>2</td>\n",
       "      <td>9713.114064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026</td>\n",
       "      <td>3</td>\n",
       "      <td>9216.950303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026</td>\n",
       "      <td>4</td>\n",
       "      <td>7629.337326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2027</td>\n",
       "      <td>0</td>\n",
       "      <td>10380.713317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2027</td>\n",
       "      <td>1</td>\n",
       "      <td>9184.488564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2027</td>\n",
       "      <td>2</td>\n",
       "      <td>10268.534303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2027</td>\n",
       "      <td>3</td>\n",
       "      <td>9297.894492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2027</td>\n",
       "      <td>4</td>\n",
       "      <td>7184.857510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed  fold           mse\n",
       "0   2025     0  10581.376414\n",
       "1   2025     1   9168.786527\n",
       "2   2025     2   9297.541884\n",
       "3   2025     3   9885.118867\n",
       "4   2025     4   6386.312916\n",
       "5   2026     0  11813.170409\n",
       "6   2026     1  10144.548109\n",
       "7   2026     2   9713.114064\n",
       "8   2026     3   9216.950303\n",
       "9   2026     4   7629.337326\n",
       "10  2027     0  10380.713317\n",
       "11  2027     1   9184.488564\n",
       "12  2027     2  10268.534303\n",
       "13  2027     3   9297.894492\n",
       "14  2027     4   7184.857510"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: LGBM CV — OOF training (uses callbacks + numpy clipping)\n",
    "\n",
    "def train_lgb_cv(train_df, features, target, folds, params, seeds=[SEED]):\n",
    "    oof = np.zeros(len(train_df), dtype=float)\n",
    "    models_by_seed = []\n",
    "    fold_metrics = []\n",
    "\n",
    "    cat_feats = [c for c in features if c in categorical_cols]\n",
    "\n",
    "    for sd in seeds:\n",
    "        params_seeded = params.copy()\n",
    "        params_seeded[\"seed\"] = sd\n",
    "        models = []\n",
    "\n",
    "        for fold_i, (tr_idx, va_idx) in enumerate(folds):\n",
    "            X_tr = train_df.iloc[tr_idx][features]\n",
    "            y_tr = target[tr_idx]\n",
    "            X_va = train_df.iloc[va_idx][features]\n",
    "            y_va = target[va_idx]\n",
    "\n",
    "            d_tr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_feats, free_raw_data=False)\n",
    "            d_va = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_feats, free_raw_data=False)\n",
    "\n",
    "            callbacks = [\n",
    "                lgb.early_stopping(stopping_rounds=200),\n",
    "                lgb.log_evaluation(period=250),\n",
    "            ]\n",
    "\n",
    "            model = lgb.train(\n",
    "                params_seeded,\n",
    "                d_tr,\n",
    "                valid_sets=[d_tr, d_va],\n",
    "                valid_names=[\"train\",\"valid\"],\n",
    "                num_boost_round=5000,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "\n",
    "            best_iter = getattr(model, \"best_iteration\", None)\n",
    "            if best_iter is None or best_iter <= 0:\n",
    "                best_iter = model.current_iteration()\n",
    "\n",
    "            pred_va = model.predict(X_va, num_iteration=best_iter)\n",
    "            oof[va_idx] += pred_va / len(seeds)\n",
    "            models.append(model)\n",
    "\n",
    "            if CFG[\"use_log_target\"]:\n",
    "                y_va_raw    = np.clip(np.expm1(y_va),   0, None)\n",
    "                pred_va_raw = np.clip(np.expm1(pred_va),0, None)\n",
    "            else:\n",
    "                y_va_raw    = y_va\n",
    "                pred_va_raw = pred_va\n",
    "\n",
    "            mse = mean_squared_error(y_va_raw, pred_va_raw)\n",
    "            fold_metrics.append((sd, fold_i, mse))\n",
    "\n",
    "            del X_tr, y_tr, X_va, y_va, d_tr, d_va\n",
    "            gc.collect()\n",
    "\n",
    "        models_by_seed.append(models)\n",
    "\n",
    "    # OOF raw metric\n",
    "    if CFG[\"use_log_target\"]:\n",
    "        y_raw   = np.clip(np.expm1(target), 0, None)\n",
    "        oof_raw = np.clip(np.expm1(oof),    0, None)\n",
    "    else:\n",
    "        y_raw   = target\n",
    "        oof_raw = oof\n",
    "\n",
    "    oof_mse = mean_squared_error(y_raw, oof_raw)\n",
    "    print(f\"OOF MSE (raw scale): {oof_mse:.4f}\")\n",
    "    return oof, models_by_seed, oof_mse, pd.DataFrame(fold_metrics, columns=[\"seed\",\"fold\",\"mse\"])\n",
    "\n",
    "oof, models_by_seed, oof_mse, fold_df = train_lgb_cv(\n",
    "    train_sess, FEATS, train_sess[\"target\"].values, folds, CFG[\"lgb_params\"], seeds=CFG[\"bagging_seeds\"]\n",
    ")\n",
    "fold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2dcf570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:15:11.715429Z",
     "iopub.status.busy": "2025-08-27T14:15:11.714841Z",
     "iopub.status.idle": "2025-08-27T14:15:42.571432Z",
     "shell.execute_reply": "2025-08-27T14:15:42.568914Z"
    },
    "papermill": {
     "duration": 30.871649,
     "end_time": "2025-08-27T14:15:42.578335",
     "exception": false,
     "start_time": "2025-08-27T14:15:11.706686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttrain's rmse: 0.386174\n",
      "[250]\ttrain's rmse: 0.38656\n",
      "[250]\ttrain's rmse: 0.385226\n",
      "Got raw test predictions. num_boost_round used: 300\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Fit final models on ALL training data (per seed), predict test — CALLBACKS + SAFE num_boost_round\n",
    "\n",
    "def fit_full_and_predict(train_df, test_df, features, target, params, seeds=[SEED]):\n",
    "    cat_feats = [c for c in features if c in categorical_cols]\n",
    "    X_full = train_df[features]\n",
    "    y_full = target\n",
    "    X_test = test_df[features]\n",
    "\n",
    "    # derive a robust num_boost_round from CV models\n",
    "    bests = []\n",
    "    for ms in models_by_seed:\n",
    "        for m in ms:\n",
    "            bi = getattr(m, \"best_iteration\", None)\n",
    "            if bi is None or bi <= 0:\n",
    "                bi = getattr(m, \"current_iteration\", lambda: 1000)()\n",
    "            bests.append(int(bi))\n",
    "    avg_best = int(np.mean(bests)) if len(bests) else 1000\n",
    "    num_boost = max(300, int(1.1 * avg_best))  # small safety margin\n",
    "\n",
    "    test_pred = np.zeros(len(test_df), dtype=float)\n",
    "    models = []\n",
    "\n",
    "    for sd in seeds:\n",
    "        params_seeded = params.copy()\n",
    "        params_seeded[\"seed\"] = sd\n",
    "\n",
    "        d_tr = lgb.Dataset(\n",
    "            X_full, label=y_full,\n",
    "            categorical_feature=cat_feats,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "\n",
    "        # Use callbacks; NO verbose_eval kw\n",
    "        callbacks = [\n",
    "            lgb.log_evaluation(period=250)  # optional; remove if you want silent training\n",
    "        ]\n",
    "\n",
    "        model = lgb.train(\n",
    "            params_seeded,\n",
    "            d_tr,\n",
    "            num_boost_round=num_boost,\n",
    "            valid_sets=[d_tr],              # just to enable log_evaluation on train\n",
    "            valid_names=[\"train\"],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        models.append(model)\n",
    "        test_pred += model.predict(X_test, num_iteration=model.best_iteration or num_boost) / len(seeds)\n",
    "\n",
    "    return test_pred, models\n",
    "\n",
    "test_pred, full_models = fit_full_and_predict(\n",
    "    train_sess, test_sess, FEATS, train_sess[\"target\"].values, CFG[\"lgb_params\"], seeds=CFG[\"bagging_seeds\"]\n",
    ")\n",
    "print(\"Got raw test predictions. num_boost_round used:\", full_models[0].current_iteration())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdefbce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:15:42.594398Z",
     "iopub.status.busy": "2025-08-27T14:15:42.594073Z",
     "iopub.status.idle": "2025-08-27T14:15:42.619203Z",
     "shell.execute_reply": "2025-08-27T14:15:42.618257Z"
    },
    "papermill": {
     "duration": 0.034817,
     "end_time": "2025-08-27T14:15:42.620633",
     "exception": false,
     "start_time": "2025-08-27T14:15:42.585816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated OOF MSE (raw): 6674.6220\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Calibrate predictions with a 1-D Ridge on OOF\n",
    "\n",
    "if CFG[\"use_log_target\"]:\n",
    "    y_raw    = np.clip(np.expm1(train_sess[\"target\"].values), 0, None)\n",
    "    oof_raw  = np.clip(np.expm1(oof),                         0, None)\n",
    "    test_raw = np.clip(np.expm1(test_pred),                   0, None)\n",
    "else:\n",
    "    y_raw    = train_sess[\"target\"].values\n",
    "    oof_raw  = oof\n",
    "    test_raw = test_pred\n",
    "\n",
    "ridge = Ridge(alpha=1.0, fit_intercept=True, positive=True)\n",
    "ridge.fit(oof_raw.reshape(-1,1), y_raw)\n",
    "oof_cal  = ridge.predict(oof_raw.reshape(-1,1))\n",
    "test_cal = ridge.predict(test_raw.reshape(-1,1))\n",
    "\n",
    "cal_mse = mean_squared_error(y_raw, oof_cal)\n",
    "print(f\"Calibrated OOF MSE (raw): {cal_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be28722f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:15:42.636456Z",
     "iopub.status.busy": "2025-08-27T14:15:42.636185Z",
     "iopub.status.idle": "2025-08-27T14:15:42.640442Z",
     "shell.execute_reply": "2025-08-27T14:15:42.639621Z"
    },
    "papermill": {
     "duration": 0.013726,
     "end_time": "2025-08-27T14:15:42.641853",
     "exception": false,
     "start_time": "2025-08-27T14:15:42.628127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Force a writeable path in Kaggle\n",
    "from pathlib import Path\n",
    "CFG[\"submission_path\"] = Path(\"/kaggle/working/submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5740eeb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T14:15:42.658291Z",
     "iopub.status.busy": "2025-08-27T14:15:42.657950Z",
     "iopub.status.idle": "2025-08-27T14:15:42.762744Z",
     "shell.execute_reply": "2025-08-27T14:15:42.761870Z"
    },
    "papermill": {
     "duration": 0.114935,
     "end_time": "2025-08-27T14:15:42.764253",
     "exception": false,
     "start_time": "2025-08-27T14:15:42.649318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /kaggle/working/submission.csv shape: (30789, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_session</th>\n",
       "      <th>session_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SESSION_000000</td>\n",
       "      <td>3119.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SESSION_000013</td>\n",
       "      <td>30.961481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SESSION_000022</td>\n",
       "      <td>28.223525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SESSION_000024</td>\n",
       "      <td>24.912189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SESSION_000025</td>\n",
       "      <td>90.828167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_session  session_value\n",
       "0  SESSION_000000    3119.392500\n",
       "1  SESSION_000013      30.961481\n",
       "2  SESSION_000022      28.223525\n",
       "3  SESSION_000024      24.912189\n",
       "4  SESSION_000025      90.828167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 15: Build submission.csv (aligns to sample_submission if present) + writable path safety\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sample_path = Path(\"./sample_submission.csv\")\n",
    "use_sample = sample_path.exists()\n",
    "\n",
    "def raw_train_labels():\n",
    "    if CFG[\"use_log_target\"]:\n",
    "        base = np.expm1(train_sess[\"target\"].values)\n",
    "    else:\n",
    "        base = train_sess[\"target\"].values\n",
    "    return np.clip(base, 0, None)\n",
    "\n",
    "if use_sample:\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    ID_OUT, TARGET_OUT = sample.columns[:2].tolist()\n",
    "    print(f\"Using sample_submission format: ID={ID_OUT}, TARGET={TARGET_OUT}\")\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        SESSION_COL: test_sess[SESSION_COL].astype(str).values,\n",
    "        \"_pred\": test_cal.astype(float)  # calibrated predictions on raw scale\n",
    "    })\n",
    "\n",
    "    tmp = sample[[ID_OUT]].copy()\n",
    "    if SESSION_COL != ID_OUT:\n",
    "        pred_df = pred_df.rename(columns={SESSION_COL: ID_OUT})\n",
    "    pred_df[ID_OUT] = pred_df[ID_OUT].astype(str)\n",
    "    tmp[ID_OUT] = tmp[ID_OUT].astype(str)\n",
    "\n",
    "    sub = tmp.merge(pred_df[[ID_OUT, \"_pred\"]], on=ID_OUT, how=\"left\")\n",
    "    sub[TARGET_OUT] = sub[\"_pred\"].fillna(0.0)\n",
    "\n",
    "    p998 = np.percentile(raw_train_labels(), 99.8)\n",
    "    sub[TARGET_OUT] = np.clip(sub[TARGET_OUT].values, 0, float(p998))\n",
    "    sub = sub[[ID_OUT, TARGET_OUT]]\n",
    "else:\n",
    "    ID_OUT = SESSION_COL\n",
    "    TARGET_OUT = \"session_value\"\n",
    "\n",
    "    preds_cal = test_cal.astype(float)\n",
    "    p998 = np.percentile(raw_train_labels(), 99.8)\n",
    "    preds_cal = np.clip(preds_cal, 0, float(p998))\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        ID_OUT: test_sess[SESSION_COL].astype(str).values,\n",
    "        TARGET_OUT: preds_cal\n",
    "    })\n",
    "\n",
    "# ---- Writable path fix ----\n",
    "save_path = Path(CFG[\"submission_path\"])\n",
    "# If pointing into read-only /kaggle/input, redirect to /kaggle/working\n",
    "if str(save_path).startswith(\"/kaggle/input\"):\n",
    "    save_path = Path(\"/kaggle/working\") / save_path.name\n",
    "# If relative, make sure it lands in working by default\n",
    "if not save_path.is_absolute():\n",
    "    save_path = Path(\"/kaggle/working\") / save_path\n",
    "\n",
    "sub.to_csv(save_path, index=False)\n",
    "print(\"Saved:\", save_path, \"shape:\", sub.shape)\n",
    "display(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361f2fb",
   "metadata": {
    "papermill": {
     "duration": 0.007439,
     "end_time": "2025-08-27T14:15:42.779282",
     "exception": false,
     "start_time": "2025-08-27T14:15:42.771843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13341508,
     "sourceId": 112016,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 156.579184,
   "end_time": "2025-08-27T14:15:43.808192",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-27T14:13:07.229008",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
